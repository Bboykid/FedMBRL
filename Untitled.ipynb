{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e306ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import TRPO\n",
    "from Client_diff_Emb import FRLClient\n",
    "from Agent import SB3Agent\n",
    "import copy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from H_Envs.pendulum import PendulumEnv\n",
    "from H_Envs.pendulum_emb import PendulumEnvEmb\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from MBEnvs.mb_pendulum_emb import MB_PendulumEnv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c30971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env_paras, device, save_dir, timesteps_real_per_round = 500, timesteps_fc_per_round = 20000, epoch_per_round = 100, rounds_num = 30, batch_size_env_model = 128):\n",
    "    \n",
    "    model_tmp_path = save_dir + \"/models/tmp\"\n",
    "    \n",
    "    CLIENTS_NUM = len(env_paras)\n",
    "    embs = [np.array([x]) for x in env_paras]\n",
    "    env_models = []\n",
    "    MB_env = TimeLimit(MB_PendulumEnv(env_models, embs,device), max_episode_steps = 200)\n",
    "    \n",
    "    # Global_RL = PPO(\"MlpPolicy\", MB_env, verbose=1)\n",
    "    Global_RL = TRPO(\"MlpPolicy\", MB_env, verbose=1)\n",
    "    \n",
    "    # env_theta = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    real_envs = []\n",
    "    eva_envs = []\n",
    "    Clients = []\n",
    "    for i in range(CLIENTS_NUM):\n",
    "        real_envs.append( TimeLimit(PendulumEnv(g=env_paras[i]), max_episode_steps=200) )\n",
    "        eva_envs.append( TimeLimit(PendulumEnvEmb(g=env_paras[i]), max_episode_steps=200) )\n",
    "        policy_net = Global_RL\n",
    "        agent = SB3Agent(policy_net)\n",
    "        client = FRLClient(real_envs[i], agent, lr = 3e-4, hidden_size = 256, device = device, emb=embs[i])\n",
    "        Clients.append(client)\n",
    "        env_model = copy.deepcopy(client.model)\n",
    "        env_models.append(env_model)\n",
    "        \n",
    "    \n",
    "        \n",
    "    Global_RL.env.models = env_models\n",
    "    \n",
    "    Global_RL.save(model_tmp_path)\n",
    "    \n",
    "    rewards_log = []\n",
    "    \n",
    "    env_models = []\n",
    "    for round_idx in range(rounds_num):\n",
    "        print('------------------------------')\n",
    "        print(\"round: \" + str(round_idx))\n",
    "        env_models = []\n",
    "        for client_idx in range(len(Clients)):\n",
    "            print('------------------------------')\n",
    "            print(\"client: \" + str(client_idx))\n",
    "            # update policy\n",
    "            Clients[client_idx].agent.policy_net = Global_RL\n",
    "            # train prediction models\n",
    "            Clients[client_idx].learn(timesteps_real_per_round, epoch_per_round, batch_size_env_model)\n",
    "            #\n",
    "            env_model = Clients[client_idx].get_prediction_model()\n",
    "            env_models.append(env_model)\n",
    "        \n",
    "    #     Server.update_env_models(env_models)\n",
    "    \n",
    "        MB_env = TimeLimit(MB_PendulumEnv(env_models,embs,device), max_episode_steps = 200)\n",
    "        \n",
    "        Global_RL = TRPO.load(model_tmp_path, env = MB_env)\n",
    "    #     Global_RL.env.models = env_models\n",
    "        #\n",
    "        Global_RL.learn(total_timesteps=timesteps_fc_per_round)\n",
    "        \n",
    "        Global_RL.save(model_tmp_path)\n",
    "    #     Server.learn(timesteps_real_per_round = 10000)\n",
    "        # test performance\n",
    "        \n",
    "        round_reward = []\n",
    "        \n",
    "        for client_idx in range(CLIENTS_NUM):\n",
    "            mean_reward, std_reward = evaluate_policy(Global_RL, eva_envs[client_idx], n_eval_episodes=20)\n",
    "            round_reward.append(mean_reward)\n",
    "        rewards_log.append(round_reward)\n",
    "        print(\"mean_reward in real env:\" + str(round_reward))\n",
    "        \n",
    "    return rewards_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0e1e6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # initialize the client and server\n",
    "timesteps_real_per_round = 1000\n",
    "timesteps_fc_per_round = timesteps_real_per_round * 30\n",
    "epoch_per_round = 10\n",
    "rounds_num = 50\n",
    "batch_size_env_model = 128\n",
    "test_dir= \"TestEmb\"\n",
    "env_paras = [7.0,7.0,7.0, 10.0,10.0,10.0, 13.0,13.0,13.0]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "save_dir = test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "064a9cd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:278: UserWarning: Path 'TestEmb\\models' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "round: 0\n",
      "------------------------------\n",
      "client: 0\n",
      "Avg loss: 0.059428532811192175!\n",
      "Avg loss: 0.03293476407258519!\n",
      "Avg loss: 0.023983481511628876!\n",
      "Avg loss: 0.018789916601684428!\n",
      "Avg loss: 0.018249193139757456!\n",
      "Avg loss: 0.017707207884929327!\n",
      "Avg loss: 0.017114631159972003!\n",
      "Avg loss: 0.016514584048236428!\n",
      "Avg loss: 0.01635221907910515!\n",
      "Avg loss: 0.01634298132273519!\n",
      "------------------------------\n",
      "client: 1\n",
      "Avg loss: 0.053227601621765645!\n",
      "Avg loss: 0.024563304519542726!\n",
      "Avg loss: 0.011105171824795737!\n",
      "Avg loss: 0.005387888080846703!\n",
      "Avg loss: 0.0024734056072702516!\n",
      "Avg loss: 0.002442291295725075!\n",
      "Avg loss: 0.002377772452573481!\n",
      "Avg loss: 0.001721375538521291!\n",
      "Avg loss: 0.001491951504874578!\n",
      "Avg loss: 0.0012957964165555798!\n",
      "------------------------------\n",
      "client: 2\n",
      "Avg loss: 0.05578530824499467!\n",
      "Avg loss: 0.028420251820546884!\n",
      "Avg loss: 0.01836653086715766!\n",
      "Avg loss: 0.014014349858007336!\n",
      "Avg loss: 0.013225900448912096!\n",
      "Avg loss: 0.013822014175117752!\n",
      "Avg loss: 0.01317952800389321!\n",
      "Avg loss: 0.012386881897036801!\n",
      "Avg loss: 0.011970279208186791!\n",
      "Avg loss: 0.01154617799950453!\n",
      "------------------------------\n",
      "client: 3\n",
      "Avg loss: 0.10768725343514234!\n",
      "Avg loss: 0.09560729787655874!\n",
      "Avg loss: 0.07810198616449876!\n",
      "Avg loss: 0.07200246192631311!\n",
      "Avg loss: 0.06689108498540008!\n",
      "Avg loss: 0.06664818881001944!\n",
      "Avg loss: 0.06722889111954525!\n",
      "Avg loss: 0.06757751624191011!\n",
      "Avg loss: 0.06604309626474181!\n",
      "Avg loss: 0.0655629002168295!\n",
      "------------------------------\n",
      "client: 4\n",
      "Avg loss: 0.09022394688043278!\n",
      "Avg loss: 0.05978116312649945!\n",
      "Avg loss: 0.044058407608038884!\n",
      "Avg loss: 0.03461423065811687!\n",
      "Avg loss: 0.03235754683531316!\n",
      "Avg loss: 0.031186534975640823!\n",
      "Avg loss: 0.030777309480836266!\n",
      "Avg loss: 0.030577909389503475!\n",
      "Avg loss: 0.03037792521538601!\n",
      "Avg loss: 0.030353978405661716!\n",
      "------------------------------\n",
      "client: 5\n",
      "Avg loss: 0.13807413986107955!\n",
      "Avg loss: 0.10597342208173359!\n",
      "Avg loss: 0.09257549290637447!\n",
      "Avg loss: 0.07994234032211049!\n",
      "Avg loss: 0.07630860693359864!\n",
      "Avg loss: 0.07581794906100792!\n",
      "Avg loss: 0.07456677992447415!\n",
      "Avg loss: 0.07498072438594439!\n",
      "Avg loss: 0.07310564226315061!\n",
      "Avg loss: 0.0731732435021877!\n",
      "------------------------------\n",
      "client: 6\n",
      "Avg loss: 0.16736067222280931!\n",
      "Avg loss: 0.12864355809860475!\n",
      "Avg loss: 0.0984999852044469!\n",
      "Avg loss: 0.07266956994753855!\n",
      "Avg loss: 0.0556760044022667!\n",
      "Avg loss: 0.04792967107536242!\n",
      "Avg loss: 0.044363726672199844!\n",
      "Avg loss: 0.04343464780145041!\n",
      "Avg loss: 0.042886563671742504!\n",
      "Avg loss: 0.04255220540702188!\n",
      "------------------------------\n",
      "client: 7\n",
      "Avg loss: 0.17641942387931825!\n",
      "Avg loss: 0.1295402854120281!\n",
      "Avg loss: 0.09932403286142896!\n",
      "Avg loss: 0.07760011359951022!\n",
      "Avg loss: 0.06934826684420842!\n",
      "Avg loss: 0.06910026672742485!\n",
      "Avg loss: 0.0684440212561655!\n",
      "Avg loss: 0.06826778664427062!\n",
      "Avg loss: 0.06780341499187367!\n",
      "Avg loss: 0.06643608935747276!\n",
      "------------------------------\n",
      "client: 8\n",
      "Avg loss: 0.16278752151449832!\n",
      "Avg loss: 0.1166343279754316!\n",
      "Avg loss: 0.07968767241599077!\n",
      "Avg loss: 0.05463364244923772!\n",
      "Avg loss: 0.03978695130661435!\n",
      "Avg loss: 0.026929103889900337!\n",
      "Avg loss: 0.022447810396261047!\n",
      "Avg loss: 0.02207458790340752!\n",
      "Avg loss: 0.021608074214327644!\n",
      "Avg loss: 0.021550835970811023!\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.17e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 327       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 6         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.25e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 314       |\n",
      "|    iterations             | 2         |\n",
      "|    time_elapsed           | 13        |\n",
      "|    total_timesteps        | 4096      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.00291   |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00818   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 1         |\n",
      "|    policy_objective       | 0.0112    |\n",
      "|    std                    | 0.986     |\n",
      "|    value_loss             | 7.75e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.21e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 311       |\n",
      "|    iterations             | 3         |\n",
      "|    time_elapsed           | 19        |\n",
      "|    total_timesteps        | 6144      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.000711  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00521   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 2         |\n",
      "|    policy_objective       | 0.00568   |\n",
      "|    std                    | 0.992     |\n",
      "|    value_loss             | 9.68e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.19e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 313       |\n",
      "|    iterations             | 4         |\n",
      "|    time_elapsed           | 26        |\n",
      "|    total_timesteps        | 8192      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.000283  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00368   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 3         |\n",
      "|    policy_objective       | 0.00684   |\n",
      "|    std                    | 0.986     |\n",
      "|    value_loss             | 6.81e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.19e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 310       |\n",
      "|    iterations             | 5         |\n",
      "|    time_elapsed           | 33        |\n",
      "|    total_timesteps        | 10240     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.000124  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00543   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 4         |\n",
      "|    policy_objective       | 0.0109    |\n",
      "|    std                    | 0.988     |\n",
      "|    value_loss             | 7.16e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.17e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 306       |\n",
      "|    iterations             | 6         |\n",
      "|    time_elapsed           | 40        |\n",
      "|    total_timesteps        | 12288     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 4.79e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00357   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 5         |\n",
      "|    policy_objective       | 0.0095    |\n",
      "|    std                    | 0.997     |\n",
      "|    value_loss             | 6.86e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.18e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 304       |\n",
      "|    iterations             | 7         |\n",
      "|    time_elapsed           | 47        |\n",
      "|    total_timesteps        | 14336     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 6.78e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.0074    |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 6         |\n",
      "|    policy_objective       | 0.00816   |\n",
      "|    std                    | 0.989     |\n",
      "|    value_loss             | 5.79e+03  |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.16e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 304       |\n",
      "|    iterations             | 8         |\n",
      "|    time_elapsed           | 53        |\n",
      "|    total_timesteps        | 16384     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.5e-05   |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00732   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 7         |\n",
      "|    policy_objective       | 0.00768   |\n",
      "|    std                    | 0.955     |\n",
      "|    value_loss             | 6.93e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.15e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 303       |\n",
      "|    iterations             | 9         |\n",
      "|    time_elapsed           | 60        |\n",
      "|    total_timesteps        | 18432     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 5.01e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00583   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 8         |\n",
      "|    policy_objective       | 0.00689   |\n",
      "|    std                    | 0.956     |\n",
      "|    value_loss             | 5.51e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.15e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 303       |\n",
      "|    iterations             | 10        |\n",
      "|    time_elapsed           | 67        |\n",
      "|    total_timesteps        | 20480     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 3.52e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.0063    |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 9         |\n",
      "|    policy_objective       | 0.00893   |\n",
      "|    std                    | 0.936     |\n",
      "|    value_loss             | 4.52e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.14e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 303       |\n",
      "|    iterations             | 11        |\n",
      "|    time_elapsed           | 74        |\n",
      "|    total_timesteps        | 22528     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.97e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00617   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 10        |\n",
      "|    policy_objective       | 0.00914   |\n",
      "|    std                    | 0.938     |\n",
      "|    value_loss             | 5.26e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.13e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 303       |\n",
      "|    iterations             | 12        |\n",
      "|    time_elapsed           | 80        |\n",
      "|    total_timesteps        | 24576     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.3e-05   |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00698   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 11        |\n",
      "|    policy_objective       | 0.00979   |\n",
      "|    std                    | 0.924     |\n",
      "|    value_loss             | 4.91e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.12e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 302       |\n",
      "|    iterations             | 13        |\n",
      "|    time_elapsed           | 87        |\n",
      "|    total_timesteps        | 26624     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 9.66e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00752   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 12        |\n",
      "|    policy_objective       | 0.00869   |\n",
      "|    std                    | 0.919     |\n",
      "|    value_loss             | 5.43e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.12e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 302       |\n",
      "|    iterations             | 14        |\n",
      "|    time_elapsed           | 94        |\n",
      "|    total_timesteps        | 28672     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 5.6e-06   |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00383   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 13        |\n",
      "|    policy_objective       | 0.00934   |\n",
      "|    std                    | 0.898     |\n",
      "|    value_loss             | 3.57e+03  |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -1.1e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 301      |\n",
      "|    iterations             | 15       |\n",
      "|    time_elapsed           | 101      |\n",
      "|    total_timesteps        | 30720    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 1.37e-05 |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00537  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 14       |\n",
      "|    policy_objective       | 0.00604  |\n",
      "|    std                    | 0.863    |\n",
      "|    value_loss             | 4.38e+03 |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:[-1219.391234071972, -1181.3146120980382, -1166.449192996323, -1174.0073819140903, -1159.3624025997008, -1157.299137605878, -1058.3642891133204, -1158.4174241259693, -1080.9330974409356]\n"
     ]
    }
   ],
   "source": [
    "model_tmp_path = save_dir + \"/models/tmp\"\n",
    "rounds_num = 1\n",
    "    \n",
    "CLIENTS_NUM = len(env_paras)\n",
    "embs = [np.array([x]) for x in env_paras]\n",
    "env_models = []\n",
    "MB_env = TimeLimit(MB_PendulumEnv(env_models, embs,device), max_episode_steps = 200)\n",
    "\n",
    "# Global_RL = PPO(\"MlpPolicy\", MB_env, verbose=1)\n",
    "Global_RL = TRPO(\"MlpPolicy\", MB_env, verbose=1)\n",
    "\n",
    "# env_theta = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "real_envs = []\n",
    "eva_envs = []\n",
    "Clients = []\n",
    "for i in range(CLIENTS_NUM):\n",
    "    real_envs.append( TimeLimit(PendulumEnv(g=env_paras[i]), max_episode_steps=200) )\n",
    "    eva_envs.append( TimeLimit(PendulumEnvEmb(g=env_paras[i]), max_episode_steps=200) )\n",
    "    policy_net = Global_RL\n",
    "    agent = SB3Agent(policy_net)\n",
    "    client = FRLClient(real_envs[i], agent, lr = 3e-4, hidden_size = 256, device = device, emb=embs[i])\n",
    "    Clients.append(client)\n",
    "    env_model = copy.deepcopy(client.model)\n",
    "    env_models.append(env_model)\n",
    "    \n",
    "\n",
    "    \n",
    "Global_RL.env.models = env_models\n",
    "\n",
    "Global_RL.save(model_tmp_path)\n",
    "\n",
    "rewards_log = []\n",
    "\n",
    "env_models = []\n",
    "for round_idx in range(rounds_num):\n",
    "    print('------------------------------')\n",
    "    print(\"round: \" + str(round_idx))\n",
    "    env_models = []\n",
    "    for client_idx in range(len(Clients)):\n",
    "        print('------------------------------')\n",
    "        print(\"client: \" + str(client_idx))\n",
    "        # update policy\n",
    "        Clients[client_idx].agent.policy_net = Global_RL\n",
    "        # train prediction models\n",
    "        Clients[client_idx].learn(timesteps_real_per_round, epoch_per_round, batch_size_env_model)\n",
    "        #\n",
    "        env_model = Clients[client_idx].get_prediction_model()\n",
    "        env_models.append(env_model)\n",
    "    \n",
    "#     Server.update_env_models(env_models)\n",
    "\n",
    "    MB_env = TimeLimit(MB_PendulumEnv(env_models,embs,device), max_episode_steps = 200)\n",
    "    \n",
    "    Global_RL = TRPO.load(model_tmp_path, env = MB_env)\n",
    "#     Global_RL.env.models = env_models\n",
    "    #\n",
    "    Global_RL.learn(total_timesteps=timesteps_fc_per_round)\n",
    "    \n",
    "    Global_RL.save(model_tmp_path)\n",
    "#     Server.learn(timesteps_real_per_round = 10000)\n",
    "    # test performance\n",
    "    \n",
    "    round_reward = []\n",
    "    \n",
    "    for client_idx in range(CLIENTS_NUM):\n",
    "        mean_reward, std_reward = evaluate_policy(Global_RL, eva_envs[client_idx], n_eval_episodes=20)\n",
    "        round_reward.append(mean_reward)\n",
    "    rewards_log.append(round_reward)\n",
    "    print(\"mean_reward in real env:\" + str(round_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "533e652c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.87111247, -0.49108353,  0.805978  , 13.        ], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MB_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2815675c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.30767384,  0.7691297 , -3.5322733 , 13.        ], dtype=float32),\n",
       " -3.186168909072876,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MB_env.step(MB_env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ec26a606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.12367555, -0.9923227 ,  0.9276148 , 10.        ], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eva_envs[3].reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f2e88a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Clients[0].emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "aa16d3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionModel(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Clients[0].model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6094184f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>actions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.994386</td>\n",
       "      <td>0.105812</td>\n",
       "      <td>-0.139859</td>\n",
       "      <td>[-1.4192818]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.995849</td>\n",
       "      <td>0.091024</td>\n",
       "      <td>-0.297200</td>\n",
       "      <td>[-0.8089152]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.997365</td>\n",
       "      <td>0.072549</td>\n",
       "      <td>-0.370749</td>\n",
       "      <td>[-0.8214741]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.998759</td>\n",
       "      <td>0.049798</td>\n",
       "      <td>-0.455882</td>\n",
       "      <td>[1.2252804]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.999296</td>\n",
       "      <td>0.037513</td>\n",
       "      <td>-0.245946</td>\n",
       "      <td>[1.1295936]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>-0.785173</td>\n",
       "      <td>0.619277</td>\n",
       "      <td>-3.779884</td>\n",
       "      <td>[1.5387541]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-0.675600</td>\n",
       "      <td>0.737269</td>\n",
       "      <td>-3.223950</td>\n",
       "      <td>[0.54706544]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-0.567969</td>\n",
       "      <td>0.823050</td>\n",
       "      <td>-2.754824</td>\n",
       "      <td>[-0.26217845]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-0.467035</td>\n",
       "      <td>0.884239</td>\n",
       "      <td>-2.362050</td>\n",
       "      <td>[1.6102653]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-0.392290</td>\n",
       "      <td>0.919842</td>\n",
       "      <td>-1.656285</td>\n",
       "      <td>[0.0047804425]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         actions\n",
       "0    0.994386  0.105812 -0.139859    [-1.4192818]\n",
       "1    0.995849  0.091024 -0.297200    [-0.8089152]\n",
       "2    0.997365  0.072549 -0.370749    [-0.8214741]\n",
       "3    0.998759  0.049798 -0.455882     [1.2252804]\n",
       "4    0.999296  0.037513 -0.245946     [1.1295936]\n",
       "..        ...       ...       ...             ...\n",
       "994 -0.785173  0.619277 -3.779884     [1.5387541]\n",
       "995 -0.675600  0.737269 -3.223950    [0.54706544]\n",
       "996 -0.567969  0.823050 -2.754824   [-0.26217845]\n",
       "997 -0.467035  0.884239 -2.362050     [1.6102653]\n",
       "998 -0.392290  0.919842 -1.656285  [0.0047804425]\n",
       "\n",
       "[999 rows x 4 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Clients[0].dataset_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2aec8cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Clients[0].env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8c6b42a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "feb2635b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2996944 , -0.9540353 ,  0.21686848], dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8563e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    " obs_tensor = np.concatenate((observation, Clients[0].emb)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c0f8d641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2996944 , -0.9540353 ,  0.21686848,  7.        ], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9523ab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = Clients[0].agent.act(obs_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ff734eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37384626], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b31245c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, done, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fb9e26bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.28880283, -0.9573886 , -0.2279231 ], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50155d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_tensor = np.concatenate((observation, self.emb)).astype(np.float32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
