{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2303b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Envs.pendulum import PendulumEnv\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TimeLimit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "494268cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PendulumEnv()\n",
    "env = TimeLimit(env, max_episode_steps=200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fba20c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.70250255,  0.71168125, -0.7742709 ], dtype=float32), {})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eec815b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2024f7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MBEnvs.mb_pendulum2 import MB_PendulumEnv\n",
    "env_models = []\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MB_env = MB_PendulumEnv(env_models, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07c32890",
   "metadata": {},
   "outputs": [],
   "source": [
    "Global_RL = PPO(\"MlpPolicy\", MB_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24c97b36",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SB3Agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m agent_sb3 \u001b[38;5;241m=\u001b[39m \u001b[43mSB3Agent\u001b[49m(policy_net\u001b[38;5;241m=\u001b[39mGlobal_RL)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SB3Agent' is not defined"
     ]
    }
   ],
   "source": [
    "agent_sb3 = SB3Agent(policy_net=Global_RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c62b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_client = FRLClient(env=env,agent=agent_sb3, lr=0.0001, hidden_size= 256, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a500b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_client.sample_data(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4875b2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_client.dataset_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a891705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_client.dataset_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfdeb4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aed97cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c69c26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import TRPO\n",
    "from Client_diff_Gaussian import FRLClient\n",
    "from Agent import SB3Agent\n",
    "import copy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d1f4b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Envs.pendulum import PendulumEnv\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from MBEnvs.mb_pendulum2_gaussian import MB_PendulumEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e29110c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbc0bbb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "------------------------------\n",
      "round: 0\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: -0.0012, Test Loss: -0.0040\n",
      "Epoch 2/100, Train Loss: -0.0026, Test Loss: -0.0059\n",
      "Epoch 3/100, Train Loss: -0.0040, Test Loss: -0.0079\n",
      "Epoch 4/100, Train Loss: -0.0054, Test Loss: -0.0102\n",
      "Epoch 5/100, Train Loss: -0.0070, Test Loss: -0.0127\n",
      "Epoch 6/100, Train Loss: -0.0088, Test Loss: -0.0156\n",
      "Epoch 7/100, Train Loss: -0.0108, Test Loss: -0.0187\n",
      "Epoch 8/100, Train Loss: -0.0130, Test Loss: -0.0223\n",
      "Epoch 9/100, Train Loss: -0.0155, Test Loss: -0.0263\n",
      "Epoch 10/100, Train Loss: -0.0182, Test Loss: -0.0305\n",
      "Epoch 11/100, Train Loss: -0.0207, Test Loss: -0.0343\n",
      "Epoch 12/100, Train Loss: -0.0230, Test Loss: -0.0375\n",
      "Epoch 13/100, Train Loss: -0.0245, Test Loss: -0.0406\n",
      "Epoch 14/100, Train Loss: -0.0262, Test Loss: -0.0418\n",
      "Epoch 15/100, Train Loss: -0.0276, Test Loss: -0.0452\n",
      "Epoch 16/100, Train Loss: -0.0287, Test Loss: -0.0465\n",
      "Epoch 17/100, Train Loss: -0.0286, Test Loss: -0.0452\n",
      "Epoch 18/100, Train Loss: -0.0275, Test Loss: -0.0487\n",
      "Epoch 19/100, Train Loss: -0.0285, Test Loss: -0.0449\n",
      "Epoch 20/100, Train Loss: -0.0290, Test Loss: -0.0494\n",
      "Epoch 21/100, Train Loss: -0.0314, Test Loss: -0.0497\n",
      "Epoch 22/100, Train Loss: -0.0316, Test Loss: -0.0513\n",
      "Epoch 23/100, Train Loss: -0.0326, Test Loss: -0.0515\n",
      "Epoch 24/100, Train Loss: -0.0329, Test Loss: -0.0490\n",
      "Epoch 25/100, Train Loss: -0.0276, Test Loss: -0.0523\n",
      "Epoch 26/100, Train Loss: -0.0302, Test Loss: -0.0483\n",
      "Epoch 27/100, Train Loss: -0.0308, Test Loss: -0.0517\n",
      "Epoch 28/100, Train Loss: -0.0311, Test Loss: -0.0532\n",
      "Epoch 29/100, Train Loss: -0.0324, Test Loss: -0.0506\n",
      "Epoch 30/100, Train Loss: -0.0329, Test Loss: -0.0519\n",
      "Epoch 31/100, Train Loss: -0.0338, Test Loss: -0.0514\n",
      "Epoch 32/100, Train Loss: -0.0339, Test Loss: -0.0548\n",
      "Epoch 33/100, Train Loss: -0.0356, Test Loss: -0.0560\n",
      "Epoch 34/100, Train Loss: -0.0362, Test Loss: -0.0559\n",
      "Epoch 35/100, Train Loss: -0.0361, Test Loss: -0.0571\n",
      "Epoch 36/100, Train Loss: -0.0361, Test Loss: -0.0586\n",
      "Epoch 37/100, Train Loss: -0.0353, Test Loss: -0.0588\n",
      "Epoch 38/100, Train Loss: -0.0335, Test Loss: -0.0560\n",
      "Epoch 39/100, Train Loss: -0.0322, Test Loss: -0.0470\n",
      "Epoch 40/100, Train Loss: -0.0288, Test Loss: -0.0603\n",
      "Epoch 41/100, Train Loss: -0.0328, Test Loss: -0.0543\n",
      "Epoch 42/100, Train Loss: -0.0334, Test Loss: -0.0526\n",
      "Epoch 43/100, Train Loss: -0.0328, Test Loss: -0.0574\n",
      "Epoch 44/100, Train Loss: -0.0334, Test Loss: -0.0567\n",
      "Epoch 45/100, Train Loss: -0.0354, Test Loss: -0.0553\n",
      "Epoch 46/100, Train Loss: -0.0364, Test Loss: -0.0572\n",
      "Epoch 47/100, Train Loss: -0.0368, Test Loss: -0.0576\n",
      "Epoch 48/100, Train Loss: -0.0372, Test Loss: -0.0571\n",
      "Epoch 49/100, Train Loss: -0.0379, Test Loss: -0.0592\n",
      "Epoch 50/100, Train Loss: -0.0386, Test Loss: -0.0604\n",
      "Epoch 51/100, Train Loss: -0.0393, Test Loss: -0.0610\n",
      "Epoch 52/100, Train Loss: -0.0401, Test Loss: -0.0619\n",
      "Epoch 53/100, Train Loss: -0.0402, Test Loss: -0.0610\n",
      "Epoch 54/100, Train Loss: -0.0399, Test Loss: -0.0629\n",
      "Epoch 55/100, Train Loss: -0.0346, Test Loss: -0.0298\n",
      "Epoch 56/100, Train Loss: -0.0245, Test Loss: -0.0591\n",
      "Epoch 57/100, Train Loss: -0.0316, Test Loss: -0.0592\n",
      "Epoch 58/100, Train Loss: -0.0331, Test Loss: -0.0578\n",
      "Epoch 59/100, Train Loss: -0.0359, Test Loss: -0.0526\n",
      "Epoch 60/100, Train Loss: -0.0352, Test Loss: -0.0579\n",
      "Epoch 61/100, Train Loss: -0.0369, Test Loss: -0.0610\n",
      "Epoch 62/100, Train Loss: -0.0393, Test Loss: -0.0608\n",
      "Epoch 63/100, Train Loss: -0.0399, Test Loss: -0.0623\n",
      "Epoch 64/100, Train Loss: -0.0405, Test Loss: -0.0617\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: -0.0012, Test Loss: -0.0048\n",
      "Epoch 2/100, Train Loss: -0.0030, Test Loss: -0.0059\n",
      "Epoch 3/100, Train Loss: -0.0040, Test Loss: -0.0078\n",
      "Epoch 4/100, Train Loss: -0.0054, Test Loss: -0.0103\n",
      "Epoch 5/100, Train Loss: -0.0070, Test Loss: -0.0132\n",
      "Epoch 6/100, Train Loss: -0.0087, Test Loss: -0.0164\n",
      "Epoch 7/100, Train Loss: -0.0106, Test Loss: -0.0196\n",
      "Epoch 8/100, Train Loss: -0.0127, Test Loss: -0.0232\n",
      "Epoch 9/100, Train Loss: -0.0152, Test Loss: -0.0275\n",
      "Epoch 10/100, Train Loss: -0.0175, Test Loss: -0.0312\n",
      "Epoch 11/100, Train Loss: -0.0200, Test Loss: -0.0349\n",
      "Epoch 12/100, Train Loss: -0.0211, Test Loss: -0.0376\n",
      "Epoch 13/100, Train Loss: -0.0235, Test Loss: -0.0402\n",
      "Epoch 14/100, Train Loss: -0.0232, Test Loss: -0.0385\n",
      "Epoch 15/100, Train Loss: -0.0243, Test Loss: -0.0383\n",
      "Epoch 16/100, Train Loss: -0.0229, Test Loss: -0.0410\n",
      "Epoch 17/100, Train Loss: -0.0257, Test Loss: -0.0460\n",
      "Epoch 18/100, Train Loss: -0.0276, Test Loss: -0.0462\n",
      "Epoch 19/100, Train Loss: -0.0274, Test Loss: -0.0477\n",
      "Epoch 20/100, Train Loss: -0.0283, Test Loss: -0.0483\n",
      "Epoch 21/100, Train Loss: -0.0292, Test Loss: -0.0509\n",
      "Epoch 22/100, Train Loss: -0.0295, Test Loss: -0.0515\n",
      "Epoch 23/100, Train Loss: -0.0296, Test Loss: -0.0498\n",
      "Epoch 24/100, Train Loss: -0.0299, Test Loss: -0.0494\n",
      "Epoch 25/100, Train Loss: -0.0281, Test Loss: -0.0504\n",
      "Epoch 26/100, Train Loss: -0.0299, Test Loss: -0.0531\n",
      "Epoch 27/100, Train Loss: -0.0279, Test Loss: -0.0474\n",
      "Epoch 28/100, Train Loss: -0.0285, Test Loss: -0.0473\n",
      "Epoch 29/100, Train Loss: -0.0259, Test Loss: -0.0494\n",
      "Epoch 30/100, Train Loss: -0.0276, Test Loss: -0.0497\n",
      "Epoch 31/100, Train Loss: -0.0305, Test Loss: -0.0482\n",
      "Epoch 32/100, Train Loss: -0.0296, Test Loss: -0.0538\n",
      "Epoch 33/100, Train Loss: -0.0291, Test Loss: -0.0116\n",
      "Epoch 34/100, Train Loss: -0.0232, Test Loss: -0.0443\n",
      "Epoch 35/100, Train Loss: -0.0241, Test Loss: -0.0299\n",
      "Epoch 36/100, Train Loss: -0.0236, Test Loss: -0.0496\n",
      "Epoch 37/100, Train Loss: -0.0302, Test Loss: -0.0517\n",
      "Epoch 38/100, Train Loss: -0.0310, Test Loss: -0.0512\n",
      "Epoch 39/100, Train Loss: -0.0315, Test Loss: -0.0527\n",
      "Epoch 40/100, Train Loss: -0.0319, Test Loss: -0.0533\n",
      "Epoch 41/100, Train Loss: -0.0321, Test Loss: -0.0553\n",
      "Epoch 42/100, Train Loss: -0.0328, Test Loss: -0.0565\n",
      "Epoch 43/100, Train Loss: -0.0334, Test Loss: -0.0563\n",
      "Epoch 44/100, Train Loss: -0.0339, Test Loss: -0.0579\n",
      "Epoch 45/100, Train Loss: -0.0346, Test Loss: -0.0582\n",
      "Epoch 46/100, Train Loss: -0.0347, Test Loss: -0.0594\n",
      "Epoch 47/100, Train Loss: -0.0355, Test Loss: -0.0606\n",
      "Epoch 48/100, Train Loss: -0.0352, Test Loss: -0.0613\n",
      "Epoch 49/100, Train Loss: -0.0358, Test Loss: -0.0627\n",
      "Epoch 50/100, Train Loss: -0.0347, Test Loss: -0.0608\n",
      "Epoch 51/100, Train Loss: -0.0357, Test Loss: -0.0616\n",
      "Epoch 52/100, Train Loss: -0.0346, Test Loss: -0.0563\n",
      "Epoch 53/100, Train Loss: -0.0347, Test Loss: -0.0578\n",
      "Epoch 54/100, Train Loss: -0.0329, Test Loss: -0.0504\n",
      "Epoch 55/100, Train Loss: -0.0321, Test Loss: -0.0566\n",
      "Epoch 56/100, Train Loss: -0.0313, Test Loss: -0.0594\n",
      "Epoch 57/100, Train Loss: -0.0319, Test Loss: -0.0544\n",
      "Epoch 58/100, Train Loss: -0.0334, Test Loss: -0.0551\n",
      "Epoch 59/100, Train Loss: -0.0330, Test Loss: -0.0584\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: -0.0021, Test Loss: -0.0052\n",
      "Epoch 2/100, Train Loss: -0.0035, Test Loss: -0.0070\n",
      "Epoch 3/100, Train Loss: -0.0048, Test Loss: -0.0097\n",
      "Epoch 4/100, Train Loss: -0.0065, Test Loss: -0.0124\n",
      "Epoch 5/100, Train Loss: -0.0081, Test Loss: -0.0155\n",
      "Epoch 6/100, Train Loss: -0.0099, Test Loss: -0.0187\n",
      "Epoch 7/100, Train Loss: -0.0119, Test Loss: -0.0222\n",
      "Epoch 8/100, Train Loss: -0.0141, Test Loss: -0.0263\n",
      "Epoch 9/100, Train Loss: -0.0163, Test Loss: -0.0294\n",
      "Epoch 10/100, Train Loss: -0.0181, Test Loss: -0.0328\n",
      "Epoch 11/100, Train Loss: -0.0199, Test Loss: -0.0328\n",
      "Epoch 12/100, Train Loss: -0.0195, Test Loss: -0.0366\n",
      "Epoch 13/100, Train Loss: -0.0173, Test Loss: -0.0360\n",
      "Epoch 14/100, Train Loss: -0.0207, Test Loss: -0.0327\n",
      "Epoch 15/100, Train Loss: -0.0207, Test Loss: -0.0408\n",
      "Epoch 16/100, Train Loss: -0.0229, Test Loss: -0.0316\n",
      "Epoch 17/100, Train Loss: -0.0205, Test Loss: -0.0362\n",
      "Epoch 18/100, Train Loss: -0.0232, Test Loss: -0.0372\n",
      "Epoch 19/100, Train Loss: -0.0223, Test Loss: -0.0277\n",
      "Epoch 20/100, Train Loss: -0.0189, Test Loss: -0.0244\n",
      "Epoch 21/100, Train Loss: -0.0063, Test Loss: -0.0403\n",
      "Epoch 22/100, Train Loss: -0.0021, Test Loss: -0.0126\n",
      "Epoch 23/100, Train Loss: -0.0209, Test Loss: -0.0360\n",
      "Epoch 24/100, Train Loss: -0.0180, Test Loss: -0.0267\n",
      "Epoch 25/100, Train Loss: -0.0187, Test Loss: -0.0340\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.02e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 302       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 6         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.02e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 290       |\n",
      "|    iterations             | 2         |\n",
      "|    time_elapsed           | 14        |\n",
      "|    total_timesteps        | 4096      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | -0.00363  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00803   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 1         |\n",
      "|    policy_objective       | 0.00951   |\n",
      "|    std                    | 0.987     |\n",
      "|    value_loss             | 5.85e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.04e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 290       |\n",
      "|    iterations             | 3         |\n",
      "|    time_elapsed           | 21        |\n",
      "|    total_timesteps        | 6144      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.00461   |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00626   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 2         |\n",
      "|    policy_objective       | 0.00486   |\n",
      "|    std                    | 1.03      |\n",
      "|    value_loss             | 5.66e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.05e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 290       |\n",
      "|    iterations             | 4         |\n",
      "|    time_elapsed           | 28        |\n",
      "|    total_timesteps        | 8192      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | -4.2e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00828   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 3         |\n",
      "|    policy_objective       | 0.00593   |\n",
      "|    std                    | 0.975     |\n",
      "|    value_loss             | 5.56e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.05e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 291       |\n",
      "|    iterations             | 5         |\n",
      "|    time_elapsed           | 35        |\n",
      "|    total_timesteps        | 10240     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.000265  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00887   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 4         |\n",
      "|    policy_objective       | 0.00835   |\n",
      "|    std                    | 0.984     |\n",
      "|    value_loss             | 5.65e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.05e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 290       |\n",
      "|    iterations             | 6         |\n",
      "|    time_elapsed           | 42        |\n",
      "|    total_timesteps        | 12288     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.000283  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00898   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 5         |\n",
      "|    policy_objective       | 0.00984   |\n",
      "|    std                    | 0.957     |\n",
      "|    value_loss             | 5.16e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.05e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 289       |\n",
      "|    iterations             | 7         |\n",
      "|    time_elapsed           | 49        |\n",
      "|    total_timesteps        | 14336     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.000166  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00651   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 6         |\n",
      "|    policy_objective       | 0.00688   |\n",
      "|    std                    | 0.97      |\n",
      "|    value_loss             | 4.86e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.04e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 287       |\n",
      "|    iterations             | 8         |\n",
      "|    time_elapsed           | 56        |\n",
      "|    total_timesteps        | 16384     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.000138  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00734   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 7         |\n",
      "|    policy_objective       | 0.00596   |\n",
      "|    std                    | 0.974     |\n",
      "|    value_loss             | 4.53e+03  |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-1191.731351493299\n",
      "------------------------------\n",
      "round: 1\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.2363, Test Loss: 0.2307\n",
      "Epoch 2/100, Train Loss: 0.1937, Test Loss: 0.2417\n",
      "Epoch 3/100, Train Loss: 0.0923, Test Loss: -0.0054\n",
      "Epoch 4/100, Train Loss: 0.0041, Test Loss: 0.0717\n",
      "Epoch 5/100, Train Loss: 0.0451, Test Loss: -0.0001\n",
      "Epoch 6/100, Train Loss: -0.0137, Test Loss: -0.0416\n",
      "Epoch 7/100, Train Loss: -0.0228, Test Loss: -0.0296\n",
      "Epoch 8/100, Train Loss: -0.0210, Test Loss: -0.0251\n",
      "Epoch 9/100, Train Loss: -0.0191, Test Loss: -0.0327\n",
      "Epoch 10/100, Train Loss: -0.0274, Test Loss: -0.0514\n",
      "Epoch 11/100, Train Loss: -0.0336, Test Loss: -0.0526\n",
      "Epoch 12/100, Train Loss: -0.0329, Test Loss: -0.0519\n",
      "Epoch 13/100, Train Loss: -0.0327, Test Loss: -0.0514\n",
      "Epoch 14/100, Train Loss: -0.0334, Test Loss: -0.0544\n",
      "Epoch 15/100, Train Loss: -0.0355, Test Loss: -0.0559\n",
      "Epoch 16/100, Train Loss: -0.0362, Test Loss: -0.0559\n",
      "Epoch 17/100, Train Loss: -0.0365, Test Loss: -0.0557\n",
      "Epoch 18/100, Train Loss: -0.0365, Test Loss: -0.0559\n",
      "Epoch 19/100, Train Loss: -0.0370, Test Loss: -0.0574\n",
      "Epoch 20/100, Train Loss: -0.0377, Test Loss: -0.0579\n",
      "Epoch 21/100, Train Loss: -0.0380, Test Loss: -0.0584\n",
      "Epoch 22/100, Train Loss: -0.0384, Test Loss: -0.0589\n",
      "Epoch 23/100, Train Loss: -0.0388, Test Loss: -0.0593\n",
      "Epoch 24/100, Train Loss: -0.0391, Test Loss: -0.0596\n",
      "Epoch 25/100, Train Loss: -0.0395, Test Loss: -0.0601\n",
      "Epoch 26/100, Train Loss: -0.0399, Test Loss: -0.0605\n",
      "Epoch 27/100, Train Loss: -0.0402, Test Loss: -0.0609\n",
      "Epoch 28/100, Train Loss: -0.0405, Test Loss: -0.0613\n",
      "Epoch 29/100, Train Loss: -0.0410, Test Loss: -0.0618\n",
      "Epoch 30/100, Train Loss: -0.0413, Test Loss: -0.0618\n",
      "Epoch 31/100, Train Loss: -0.0409, Test Loss: -0.0612\n",
      "Epoch 32/100, Train Loss: -0.0410, Test Loss: -0.0622\n",
      "Epoch 33/100, Train Loss: -0.0419, Test Loss: -0.0616\n",
      "Epoch 34/100, Train Loss: -0.0358, Test Loss: -0.0621\n",
      "Epoch 35/100, Train Loss: -0.0394, Test Loss: -0.0627\n",
      "Epoch 36/100, Train Loss: -0.0382, Test Loss: -0.0636\n",
      "Epoch 37/100, Train Loss: -0.0385, Test Loss: -0.0624\n",
      "Epoch 38/100, Train Loss: -0.0357, Test Loss: -0.0583\n",
      "Epoch 39/100, Train Loss: -0.0355, Test Loss: -0.0532\n",
      "Epoch 40/100, Train Loss: -0.0351, Test Loss: -0.0598\n",
      "Epoch 41/100, Train Loss: -0.0376, Test Loss: -0.0614\n",
      "Epoch 42/100, Train Loss: -0.0394, Test Loss: -0.0599\n",
      "Epoch 43/100, Train Loss: -0.0373, Test Loss: -0.0594\n",
      "Epoch 44/100, Train Loss: -0.0363, Test Loss: -0.0565\n",
      "Epoch 45/100, Train Loss: -0.0363, Test Loss: -0.0539\n",
      "Epoch 46/100, Train Loss: -0.0365, Test Loss: -0.0574\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 0.0252, Test Loss: -0.0112\n",
      "Epoch 2/100, Train Loss: 0.0054, Test Loss: -0.0192\n",
      "Epoch 3/100, Train Loss: -0.0169, Test Loss: -0.0245\n",
      "Epoch 4/100, Train Loss: -0.0161, Test Loss: -0.0310\n",
      "Epoch 5/100, Train Loss: -0.0227, Test Loss: -0.0443\n",
      "Epoch 6/100, Train Loss: -0.0244, Test Loss: -0.0402\n",
      "Epoch 7/100, Train Loss: -0.0246, Test Loss: -0.0458\n",
      "Epoch 8/100, Train Loss: -0.0272, Test Loss: -0.0437\n",
      "Epoch 9/100, Train Loss: -0.0264, Test Loss: -0.0439\n",
      "Epoch 10/100, Train Loss: -0.0272, Test Loss: -0.0475\n",
      "Epoch 11/100, Train Loss: -0.0283, Test Loss: -0.0467\n",
      "Epoch 12/100, Train Loss: -0.0278, Test Loss: -0.0471\n",
      "Epoch 13/100, Train Loss: -0.0285, Test Loss: -0.0483\n",
      "Epoch 14/100, Train Loss: -0.0289, Test Loss: -0.0477\n",
      "Epoch 15/100, Train Loss: -0.0289, Test Loss: -0.0489\n",
      "Epoch 16/100, Train Loss: -0.0294, Test Loss: -0.0497\n",
      "Epoch 17/100, Train Loss: -0.0297, Test Loss: -0.0501\n",
      "Epoch 18/100, Train Loss: -0.0300, Test Loss: -0.0505\n",
      "Epoch 19/100, Train Loss: -0.0303, Test Loss: -0.0511\n",
      "Epoch 20/100, Train Loss: -0.0306, Test Loss: -0.0519\n",
      "Epoch 21/100, Train Loss: -0.0310, Test Loss: -0.0524\n",
      "Epoch 22/100, Train Loss: -0.0313, Test Loss: -0.0530\n",
      "Epoch 23/100, Train Loss: -0.0316, Test Loss: -0.0538\n",
      "Epoch 24/100, Train Loss: -0.0319, Test Loss: -0.0542\n",
      "Epoch 25/100, Train Loss: -0.0320, Test Loss: -0.0549\n",
      "Epoch 26/100, Train Loss: -0.0317, Test Loss: -0.0543\n",
      "Epoch 27/100, Train Loss: -0.0303, Test Loss: -0.0551\n",
      "Epoch 28/100, Train Loss: -0.0278, Test Loss: -0.0527\n",
      "Epoch 29/100, Train Loss: -0.0256, Test Loss: -0.0509\n",
      "Epoch 30/100, Train Loss: -0.0296, Test Loss: -0.0491\n",
      "Epoch 31/100, Train Loss: -0.0299, Test Loss: -0.0550\n",
      "Epoch 32/100, Train Loss: -0.0302, Test Loss: -0.0542\n",
      "Epoch 33/100, Train Loss: -0.0313, Test Loss: -0.0544\n",
      "Epoch 34/100, Train Loss: -0.0319, Test Loss: -0.0526\n",
      "Epoch 35/100, Train Loss: -0.0326, Test Loss: -0.0521\n",
      "Epoch 36/100, Train Loss: -0.0317, Test Loss: -0.0535\n",
      "Epoch 37/100, Train Loss: -0.0323, Test Loss: -0.0542\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: -0.0103, Test Loss: -0.0277\n",
      "Epoch 2/100, Train Loss: -0.0156, Test Loss: -0.0304\n",
      "Epoch 3/100, Train Loss: -0.0256, Test Loss: -0.0351\n",
      "Epoch 4/100, Train Loss: -0.0243, Test Loss: -0.0352\n",
      "Epoch 5/100, Train Loss: -0.0261, Test Loss: -0.0420\n",
      "Epoch 6/100, Train Loss: -0.0291, Test Loss: -0.0411\n",
      "Epoch 7/100, Train Loss: -0.0286, Test Loss: -0.0421\n",
      "Epoch 8/100, Train Loss: -0.0300, Test Loss: -0.0458\n",
      "Epoch 9/100, Train Loss: -0.0310, Test Loss: -0.0462\n",
      "Epoch 10/100, Train Loss: -0.0317, Test Loss: -0.0470\n",
      "Epoch 11/100, Train Loss: -0.0317, Test Loss: -0.0481\n",
      "Epoch 12/100, Train Loss: -0.0327, Test Loss: -0.0486\n",
      "Epoch 13/100, Train Loss: -0.0322, Test Loss: -0.0440\n",
      "Epoch 14/100, Train Loss: -0.0313, Test Loss: -0.0405\n",
      "Epoch 15/100, Train Loss: -0.0282, Test Loss: -0.0439\n",
      "Epoch 16/100, Train Loss: -0.0302, Test Loss: -0.0400\n",
      "Epoch 17/100, Train Loss: -0.0304, Test Loss: -0.0441\n",
      "Epoch 18/100, Train Loss: -0.0335, Test Loss: -0.0490\n",
      "Epoch 19/100, Train Loss: -0.0338, Test Loss: -0.0491\n",
      "Epoch 20/100, Train Loss: -0.0352, Test Loss: -0.0508\n",
      "Epoch 21/100, Train Loss: -0.0351, Test Loss: -0.0524\n",
      "Epoch 22/100, Train Loss: -0.0360, Test Loss: -0.0461\n",
      "Epoch 23/100, Train Loss: -0.0345, Test Loss: -0.0534\n",
      "Epoch 24/100, Train Loss: -0.0361, Test Loss: -0.0315\n",
      "Epoch 25/100, Train Loss: -0.0299, Test Loss: -0.0503\n",
      "Epoch 26/100, Train Loss: -0.0345, Test Loss: -0.0347\n",
      "Epoch 27/100, Train Loss: -0.0309, Test Loss: -0.0370\n",
      "Epoch 28/100, Train Loss: -0.0329, Test Loss: -0.0421\n",
      "Epoch 29/100, Train Loss: -0.0322, Test Loss: -0.0252\n",
      "Epoch 30/100, Train Loss: -0.0287, Test Loss: -0.0413\n",
      "Epoch 31/100, Train Loss: -0.0277, Test Loss: -0.0485\n",
      "Epoch 32/100, Train Loss: -0.0344, Test Loss: -0.0414\n",
      "Epoch 33/100, Train Loss: -0.0333, Test Loss: -0.0505\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.03e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 302       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 6         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.05e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 292       |\n",
      "|    iterations             | 2         |\n",
      "|    time_elapsed           | 14        |\n",
      "|    total_timesteps        | 4096      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 5.96e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00816   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 9         |\n",
      "|    policy_objective       | 0.0103    |\n",
      "|    std                    | 0.979     |\n",
      "|    value_loss             | 4.16e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.05e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 290       |\n",
      "|    iterations             | 3         |\n",
      "|    time_elapsed           | 21        |\n",
      "|    total_timesteps        | 6144      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 5.45e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00766   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 10        |\n",
      "|    policy_objective       | 0.00767   |\n",
      "|    std                    | 1.03      |\n",
      "|    value_loss             | 4.15e+03  |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.04e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 290       |\n",
      "|    iterations             | 4         |\n",
      "|    time_elapsed           | 28        |\n",
      "|    total_timesteps        | 8192      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 5.51e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00579   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 11        |\n",
      "|    policy_objective       | 0.0102    |\n",
      "|    std                    | 0.998     |\n",
      "|    value_loss             | 3.99e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.03e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 293       |\n",
      "|    iterations             | 5         |\n",
      "|    time_elapsed           | 34        |\n",
      "|    total_timesteps        | 10240     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 2.42e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00529   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 12        |\n",
      "|    policy_objective       | 0.00496   |\n",
      "|    std                    | 1         |\n",
      "|    value_loss             | 3.36e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.04e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 294       |\n",
      "|    iterations             | 6         |\n",
      "|    time_elapsed           | 41        |\n",
      "|    total_timesteps        | 12288     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 3.4e-05   |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00637   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 13        |\n",
      "|    policy_objective       | 0.0111    |\n",
      "|    std                    | 1.01      |\n",
      "|    value_loss             | 3.69e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.04e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 295       |\n",
      "|    iterations             | 7         |\n",
      "|    time_elapsed           | 48        |\n",
      "|    total_timesteps        | 14336     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 4.97e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00639   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 14        |\n",
      "|    policy_objective       | 0.00886   |\n",
      "|    std                    | 0.999     |\n",
      "|    value_loss             | 3.77e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.05e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 295       |\n",
      "|    iterations             | 8         |\n",
      "|    time_elapsed           | 55        |\n",
      "|    total_timesteps        | 16384     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 2.94e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.0078    |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 15        |\n",
      "|    policy_objective       | 0.00591   |\n",
      "|    std                    | 0.946     |\n",
      "|    value_loss             | 3.25e+03  |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-1201.7008823909914\n",
      "------------------------------\n",
      "round: 2\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: -0.0268, Test Loss: -0.0298\n",
      "Epoch 2/100, Train Loss: -0.0286, Test Loss: -0.0271\n",
      "Epoch 3/100, Train Loss: -0.0312, Test Loss: -0.0181\n",
      "Epoch 4/100, Train Loss: -0.0328, Test Loss: -0.0091\n",
      "Epoch 5/100, Train Loss: -0.0344, Test Loss: 0.0073\n",
      "Epoch 6/100, Train Loss: -0.0361, Test Loss: 0.0265\n",
      "Epoch 7/100, Train Loss: -0.0379, Test Loss: 0.0524\n",
      "Epoch 8/100, Train Loss: -0.0396, Test Loss: 0.0889\n",
      "Epoch 9/100, Train Loss: -0.0403, Test Loss: 0.1249\n",
      "Epoch 10/100, Train Loss: -0.0403, Test Loss: 0.1603\n",
      "Epoch 11/100, Train Loss: -0.0419, Test Loss: 0.1971\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: -0.0000, Test Loss: -0.0256\n",
      "Epoch 2/100, Train Loss: 0.0065, Test Loss: -0.0331\n",
      "Epoch 3/100, Train Loss: -0.0166, Test Loss: -0.0344\n",
      "Epoch 4/100, Train Loss: -0.0114, Test Loss: -0.0351\n",
      "Epoch 5/100, Train Loss: -0.0197, Test Loss: -0.0459\n",
      "Epoch 6/100, Train Loss: -0.0226, Test Loss: -0.0424\n",
      "Epoch 7/100, Train Loss: -0.0214, Test Loss: -0.0417\n",
      "Epoch 8/100, Train Loss: -0.0227, Test Loss: -0.0450\n",
      "Epoch 9/100, Train Loss: -0.0250, Test Loss: -0.0460\n",
      "Epoch 10/100, Train Loss: -0.0253, Test Loss: -0.0454\n",
      "Epoch 11/100, Train Loss: -0.0254, Test Loss: -0.0457\n",
      "Epoch 12/100, Train Loss: -0.0261, Test Loss: -0.0472\n",
      "Epoch 13/100, Train Loss: -0.0269, Test Loss: -0.0480\n",
      "Epoch 14/100, Train Loss: -0.0270, Test Loss: -0.0483\n",
      "Epoch 15/100, Train Loss: -0.0264, Test Loss: -0.0484\n",
      "Epoch 16/100, Train Loss: -0.0243, Test Loss: -0.0496\n",
      "Epoch 17/100, Train Loss: -0.0232, Test Loss: -0.0486\n",
      "Epoch 18/100, Train Loss: -0.0271, Test Loss: -0.0475\n",
      "Epoch 19/100, Train Loss: -0.0267, Test Loss: -0.0499\n",
      "Epoch 20/100, Train Loss: -0.0279, Test Loss: -0.0493\n",
      "Epoch 21/100, Train Loss: -0.0278, Test Loss: -0.0499\n",
      "Epoch 22/100, Train Loss: -0.0283, Test Loss: -0.0493\n",
      "Epoch 23/100, Train Loss: -0.0276, Test Loss: -0.0492\n",
      "Epoch 24/100, Train Loss: -0.0274, Test Loss: -0.0495\n",
      "Epoch 25/100, Train Loss: -0.0252, Test Loss: -0.0492\n",
      "Epoch 26/100, Train Loss: -0.0235, Test Loss: -0.0486\n",
      "Epoch 27/100, Train Loss: -0.0246, Test Loss: -0.0482\n",
      "Epoch 28/100, Train Loss: -0.0256, Test Loss: -0.0498\n",
      "Epoch 29/100, Train Loss: -0.0269, Test Loss: -0.0454\n",
      "Epoch 30/100, Train Loss: -0.0278, Test Loss: -0.0500\n",
      "Epoch 31/100, Train Loss: -0.0282, Test Loss: -0.0492\n",
      "Epoch 32/100, Train Loss: -0.0287, Test Loss: -0.0498\n",
      "Epoch 33/100, Train Loss: -0.0292, Test Loss: -0.0509\n",
      "Epoch 34/100, Train Loss: -0.0294, Test Loss: -0.0509\n",
      "Epoch 35/100, Train Loss: -0.0299, Test Loss: -0.0520\n",
      "Epoch 36/100, Train Loss: -0.0303, Test Loss: -0.0521\n",
      "Epoch 37/100, Train Loss: -0.0306, Test Loss: -0.0531\n",
      "Epoch 38/100, Train Loss: -0.0310, Test Loss: -0.0539\n",
      "Epoch 39/100, Train Loss: -0.0315, Test Loss: -0.0544\n",
      "Epoch 40/100, Train Loss: -0.0319, Test Loss: -0.0554\n",
      "Epoch 41/100, Train Loss: -0.0323, Test Loss: -0.0563\n",
      "Epoch 42/100, Train Loss: -0.0328, Test Loss: -0.0570\n",
      "Epoch 43/100, Train Loss: -0.0332, Test Loss: -0.0578\n",
      "Epoch 44/100, Train Loss: -0.0337, Test Loss: -0.0586\n",
      "Epoch 45/100, Train Loss: -0.0341, Test Loss: -0.0596\n",
      "Epoch 46/100, Train Loss: -0.0345, Test Loss: -0.0599\n",
      "Epoch 47/100, Train Loss: -0.0348, Test Loss: -0.0608\n",
      "Epoch 48/100, Train Loss: -0.0350, Test Loss: -0.0612\n",
      "Epoch 49/100, Train Loss: -0.0351, Test Loss: -0.0620\n",
      "Epoch 50/100, Train Loss: -0.0349, Test Loss: -0.0617\n",
      "Epoch 51/100, Train Loss: -0.0343, Test Loss: -0.0605\n",
      "Epoch 52/100, Train Loss: -0.0323, Test Loss: -0.0598\n",
      "Epoch 53/100, Train Loss: -0.0280, Test Loss: -0.0615\n",
      "Epoch 54/100, Train Loss: -0.0216, Test Loss: -0.0585\n",
      "Epoch 55/100, Train Loss: -0.0259, Test Loss: -0.0431\n",
      "Epoch 56/100, Train Loss: -0.0283, Test Loss: -0.0592\n",
      "Epoch 57/100, Train Loss: -0.0326, Test Loss: -0.0504\n",
      "Epoch 58/100, Train Loss: -0.0308, Test Loss: -0.0567\n",
      "Epoch 59/100, Train Loss: -0.0343, Test Loss: -0.0580\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: -0.0204, Test Loss: -0.0370\n",
      "Epoch 2/100, Train Loss: -0.0206, Test Loss: -0.0391\n",
      "Epoch 3/100, Train Loss: -0.0249, Test Loss: -0.0417\n",
      "Epoch 4/100, Train Loss: -0.0247, Test Loss: -0.0425\n",
      "Epoch 5/100, Train Loss: -0.0263, Test Loss: -0.0455\n",
      "Epoch 6/100, Train Loss: -0.0275, Test Loss: -0.0462\n",
      "Epoch 7/100, Train Loss: -0.0279, Test Loss: -0.0485\n",
      "Epoch 8/100, Train Loss: -0.0294, Test Loss: -0.0508\n",
      "Epoch 9/100, Train Loss: -0.0300, Test Loss: -0.0510\n",
      "Epoch 10/100, Train Loss: -0.0308, Test Loss: -0.0521\n",
      "Epoch 11/100, Train Loss: -0.0313, Test Loss: -0.0518\n",
      "Epoch 12/100, Train Loss: -0.0317, Test Loss: -0.0492\n",
      "Epoch 13/100, Train Loss: -0.0305, Test Loss: -0.0526\n",
      "Epoch 14/100, Train Loss: -0.0315, Test Loss: -0.0550\n",
      "Epoch 15/100, Train Loss: -0.0320, Test Loss: -0.0526\n",
      "Epoch 16/100, Train Loss: -0.0325, Test Loss: -0.0523\n",
      "Epoch 17/100, Train Loss: -0.0322, Test Loss: -0.0497\n",
      "Epoch 18/100, Train Loss: -0.0332, Test Loss: -0.0467\n",
      "Epoch 19/100, Train Loss: -0.0325, Test Loss: -0.0493\n",
      "Epoch 20/100, Train Loss: -0.0331, Test Loss: -0.0566\n",
      "Epoch 21/100, Train Loss: -0.0328, Test Loss: -0.0585\n",
      "Epoch 22/100, Train Loss: -0.0350, Test Loss: -0.0547\n",
      "Epoch 23/100, Train Loss: -0.0341, Test Loss: -0.0572\n",
      "Epoch 24/100, Train Loss: -0.0353, Test Loss: -0.0555\n",
      "Epoch 25/100, Train Loss: -0.0342, Test Loss: -0.0589\n",
      "Epoch 26/100, Train Loss: -0.0355, Test Loss: -0.0541\n",
      "Epoch 27/100, Train Loss: -0.0351, Test Loss: -0.0567\n",
      "Epoch 28/100, Train Loss: -0.0349, Test Loss: -0.0589\n",
      "Epoch 29/100, Train Loss: -0.0361, Test Loss: -0.0640\n",
      "Epoch 30/100, Train Loss: -0.0375, Test Loss: -0.0551\n",
      "Epoch 31/100, Train Loss: -0.0378, Test Loss: -0.0658\n",
      "Epoch 32/100, Train Loss: -0.0385, Test Loss: -0.0645\n",
      "Epoch 33/100, Train Loss: -0.0391, Test Loss: -0.0650\n",
      "Epoch 34/100, Train Loss: -0.0390, Test Loss: -0.0562\n",
      "Epoch 35/100, Train Loss: -0.0380, Test Loss: -0.0574\n",
      "Epoch 36/100, Train Loss: -0.0371, Test Loss: -0.0685\n",
      "Epoch 37/100, Train Loss: -0.0368, Test Loss: -0.0451\n",
      "Epoch 38/100, Train Loss: -0.0273, Test Loss: -0.0651\n",
      "Epoch 39/100, Train Loss: -0.0335, Test Loss: -0.0598\n",
      "Epoch 40/100, Train Loss: -0.0358, Test Loss: -0.0385\n",
      "Epoch 41/100, Train Loss: -0.0313, Test Loss: -0.0512\n",
      "Epoch 42/100, Train Loss: -0.0331, Test Loss: -0.0499\n",
      "Epoch 43/100, Train Loss: -0.0337, Test Loss: -0.0532\n",
      "Epoch 44/100, Train Loss: -0.0358, Test Loss: -0.0444\n",
      "Epoch 45/100, Train Loss: -0.0360, Test Loss: -0.0531\n",
      "Epoch 46/100, Train Loss: -0.0366, Test Loss: -0.0531\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -847     |\n",
      "| time/              |          |\n",
      "|    fps             | 305      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -971     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 290      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 14       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 1.36e-05 |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00625  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 17       |\n",
      "|    policy_objective       | 0.00876  |\n",
      "|    std                    | 0.912    |\n",
      "|    value_loss             | 1.95e+03 |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.01e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 289       |\n",
      "|    iterations             | 3         |\n",
      "|    time_elapsed           | 21        |\n",
      "|    total_timesteps        | 6144      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 2.06e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00755   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 18        |\n",
      "|    policy_objective       | 0.0082    |\n",
      "|    std                    | 0.91      |\n",
      "|    value_loss             | 3.39e+03  |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.04e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 287       |\n",
      "|    iterations             | 4         |\n",
      "|    time_elapsed           | 28        |\n",
      "|    total_timesteps        | 8192      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 2.91e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00621   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 19        |\n",
      "|    policy_objective       | 0.00685   |\n",
      "|    std                    | 0.903     |\n",
      "|    value_loss             | 3.25e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.05e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 286       |\n",
      "|    iterations             | 5         |\n",
      "|    time_elapsed           | 35        |\n",
      "|    total_timesteps        | 10240     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 3.42e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00763   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 20        |\n",
      "|    policy_objective       | 0.00905   |\n",
      "|    std                    | 0.908     |\n",
      "|    value_loss             | 3.07e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.05e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 283       |\n",
      "|    iterations             | 6         |\n",
      "|    time_elapsed           | 43        |\n",
      "|    total_timesteps        | 12288     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 7.06e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00847   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 21        |\n",
      "|    policy_objective       | 0.00824   |\n",
      "|    std                    | 0.913     |\n",
      "|    value_loss             | 2.69e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.03e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 280       |\n",
      "|    iterations             | 7         |\n",
      "|    time_elapsed           | 51        |\n",
      "|    total_timesteps        | 14336     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.67e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00664   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 22        |\n",
      "|    policy_objective       | 0.00541   |\n",
      "|    std                    | 0.882     |\n",
      "|    value_loss             | 2.26e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.04e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 282       |\n",
      "|    iterations             | 8         |\n",
      "|    time_elapsed           | 58        |\n",
      "|    total_timesteps        | 16384     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 2.24e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00812   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 23        |\n",
      "|    policy_objective       | 0.0113    |\n",
      "|    std                    | 0.875     |\n",
      "|    value_loss             | 1.61e+03  |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-1100.020173463307\n",
      "------------------------------\n",
      "round: 3\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.2333, Test Loss: 1.1549\n",
      "Epoch 2/100, Train Loss: 0.1704, Test Loss: 0.8589\n",
      "Epoch 3/100, Train Loss: 0.0921, Test Loss: 0.5800\n",
      "Epoch 4/100, Train Loss: 0.0139, Test Loss: 0.5406\n",
      "Epoch 5/100, Train Loss: 0.0300, Test Loss: 0.4250\n",
      "Epoch 6/100, Train Loss: -0.0231, Test Loss: 0.3784\n",
      "Epoch 7/100, Train Loss: -0.0108, Test Loss: 0.3180\n",
      "Epoch 8/100, Train Loss: -0.0263, Test Loss: 0.2610\n",
      "Epoch 9/100, Train Loss: -0.0339, Test Loss: 0.2415\n",
      "Epoch 10/100, Train Loss: -0.0303, Test Loss: 0.2196\n",
      "Epoch 11/100, Train Loss: -0.0388, Test Loss: 0.2117\n",
      "Epoch 12/100, Train Loss: -0.0399, Test Loss: 0.2100\n",
      "Epoch 13/100, Train Loss: -0.0385, Test Loss: 0.1995\n",
      "Epoch 14/100, Train Loss: -0.0412, Test Loss: 0.1862\n",
      "Epoch 15/100, Train Loss: -0.0423, Test Loss: 0.1818\n",
      "Epoch 16/100, Train Loss: -0.0413, Test Loss: 0.1799\n",
      "Epoch 17/100, Train Loss: -0.0430, Test Loss: 0.1818\n",
      "Epoch 18/100, Train Loss: -0.0429, Test Loss: 0.1855\n",
      "Epoch 19/100, Train Loss: -0.0428, Test Loss: 0.1863\n",
      "Epoch 20/100, Train Loss: -0.0435, Test Loss: 0.1882\n",
      "Epoch 21/100, Train Loss: -0.0433, Test Loss: 0.1908\n",
      "Epoch 22/100, Train Loss: -0.0437, Test Loss: 0.1948\n",
      "Epoch 23/100, Train Loss: -0.0438, Test Loss: 0.2000\n",
      "Epoch 24/100, Train Loss: -0.0440, Test Loss: 0.2055\n",
      "Epoch 25/100, Train Loss: -0.0441, Test Loss: 0.2114\n",
      "Epoch 26/100, Train Loss: -0.0443, Test Loss: 0.2177\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 0.0020, Test Loss: -0.0479\n",
      "Epoch 2/100, Train Loss: -0.0046, Test Loss: -0.0475\n",
      "Epoch 3/100, Train Loss: -0.0221, Test Loss: -0.0524\n",
      "Epoch 4/100, Train Loss: -0.0212, Test Loss: -0.0520\n",
      "Epoch 5/100, Train Loss: -0.0247, Test Loss: -0.0557\n",
      "Epoch 6/100, Train Loss: -0.0285, Test Loss: -0.0543\n",
      "Epoch 7/100, Train Loss: -0.0277, Test Loss: -0.0541\n",
      "Epoch 8/100, Train Loss: -0.0283, Test Loss: -0.0566\n",
      "Epoch 9/100, Train Loss: -0.0301, Test Loss: -0.0571\n",
      "Epoch 10/100, Train Loss: -0.0305, Test Loss: -0.0572\n",
      "Epoch 11/100, Train Loss: -0.0308, Test Loss: -0.0578\n",
      "Epoch 12/100, Train Loss: -0.0315, Test Loss: -0.0597\n",
      "Epoch 13/100, Train Loss: -0.0324, Test Loss: -0.0610\n",
      "Epoch 14/100, Train Loss: -0.0331, Test Loss: -0.0617\n",
      "Epoch 15/100, Train Loss: -0.0336, Test Loss: -0.0633\n",
      "Epoch 16/100, Train Loss: -0.0345, Test Loss: -0.0648\n",
      "Epoch 17/100, Train Loss: -0.0353, Test Loss: -0.0663\n",
      "Epoch 18/100, Train Loss: -0.0360, Test Loss: -0.0675\n",
      "Epoch 19/100, Train Loss: -0.0368, Test Loss: -0.0691\n",
      "Epoch 20/100, Train Loss: -0.0376, Test Loss: -0.0700\n",
      "Epoch 21/100, Train Loss: -0.0379, Test Loss: -0.0712\n",
      "Epoch 22/100, Train Loss: -0.0386, Test Loss: -0.0717\n",
      "Epoch 23/100, Train Loss: -0.0379, Test Loss: -0.0667\n",
      "Epoch 24/100, Train Loss: -0.0345, Test Loss: -0.0616\n",
      "Epoch 25/100, Train Loss: -0.0311, Test Loss: -0.0455\n",
      "Epoch 26/100, Train Loss: -0.0269, Test Loss: -0.0506\n",
      "Epoch 27/100, Train Loss: -0.0285, Test Loss: -0.0620\n",
      "Epoch 28/100, Train Loss: -0.0258, Test Loss: -0.0624\n",
      "Epoch 29/100, Train Loss: -0.0226, Test Loss: -0.0485\n",
      "Epoch 30/100, Train Loss: -0.0169, Test Loss: -0.0665\n",
      "Epoch 31/100, Train Loss: -0.0251, Test Loss: -0.0583\n",
      "Epoch 32/100, Train Loss: -0.0332, Test Loss: -0.0640\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 0.0567, Test Loss: -0.0627\n",
      "Epoch 2/100, Train Loss: 0.0080, Test Loss: -0.0645\n",
      "Epoch 3/100, Train Loss: -0.0125, Test Loss: -0.0563\n",
      "Epoch 4/100, Train Loss: -0.0199, Test Loss: -0.0618\n",
      "Epoch 5/100, Train Loss: -0.0270, Test Loss: -0.0612\n",
      "Epoch 6/100, Train Loss: -0.0290, Test Loss: -0.0621\n",
      "Epoch 7/100, Train Loss: -0.0323, Test Loss: -0.0619\n",
      "Epoch 8/100, Train Loss: -0.0327, Test Loss: -0.0623\n",
      "Epoch 9/100, Train Loss: -0.0343, Test Loss: -0.0630\n",
      "Epoch 10/100, Train Loss: -0.0350, Test Loss: -0.0623\n",
      "Epoch 11/100, Train Loss: -0.0359, Test Loss: -0.0640\n",
      "Epoch 12/100, Train Loss: -0.0365, Test Loss: -0.0646\n",
      "Epoch 13/100, Train Loss: -0.0373, Test Loss: -0.0652\n",
      "Epoch 14/100, Train Loss: -0.0379, Test Loss: -0.0664\n",
      "Epoch 15/100, Train Loss: -0.0387, Test Loss: -0.0672\n",
      "Epoch 16/100, Train Loss: -0.0394, Test Loss: -0.0679\n",
      "Epoch 17/100, Train Loss: -0.0402, Test Loss: -0.0694\n",
      "Epoch 18/100, Train Loss: -0.0409, Test Loss: -0.0703\n",
      "Epoch 19/100, Train Loss: -0.0417, Test Loss: -0.0705\n",
      "Epoch 20/100, Train Loss: -0.0424, Test Loss: -0.0705\n",
      "Epoch 21/100, Train Loss: -0.0430, Test Loss: -0.0682\n",
      "Epoch 22/100, Train Loss: -0.0431, Test Loss: -0.0667\n",
      "Epoch 23/100, Train Loss: -0.0433, Test Loss: -0.0635\n",
      "Epoch 24/100, Train Loss: -0.0427, Test Loss: -0.0606\n",
      "Epoch 25/100, Train Loss: -0.0421, Test Loss: -0.0595\n",
      "Epoch 26/100, Train Loss: -0.0405, Test Loss: -0.0651\n",
      "Epoch 27/100, Train Loss: -0.0406, Test Loss: -0.0701\n",
      "Epoch 28/100, Train Loss: -0.0394, Test Loss: -0.0478\n",
      "Epoch 29/100, Train Loss: -0.0369, Test Loss: -0.0721\n",
      "Epoch 30/100, Train Loss: -0.0427, Test Loss: -0.0718\n",
      "Epoch 31/100, Train Loss: -0.0423, Test Loss: -0.0695\n",
      "Epoch 32/100, Train Loss: -0.0430, Test Loss: -0.0706\n",
      "Epoch 33/100, Train Loss: -0.0443, Test Loss: -0.0705\n",
      "Epoch 34/100, Train Loss: -0.0443, Test Loss: -0.0754\n",
      "Epoch 35/100, Train Loss: -0.0451, Test Loss: -0.0776\n",
      "Epoch 36/100, Train Loss: -0.0452, Test Loss: -0.0761\n",
      "Epoch 37/100, Train Loss: -0.0459, Test Loss: -0.0769\n",
      "Epoch 38/100, Train Loss: -0.0455, Test Loss: -0.0755\n",
      "Epoch 39/100, Train Loss: -0.0457, Test Loss: -0.0774\n",
      "Epoch 40/100, Train Loss: -0.0457, Test Loss: -0.0781\n",
      "Epoch 41/100, Train Loss: -0.0453, Test Loss: -0.0732\n",
      "Epoch 42/100, Train Loss: -0.0441, Test Loss: -0.0773\n",
      "Epoch 43/100, Train Loss: -0.0441, Test Loss: -0.0769\n",
      "Epoch 44/100, Train Loss: -0.0424, Test Loss: -0.0789\n",
      "Epoch 45/100, Train Loss: -0.0404, Test Loss: -0.0788\n",
      "Epoch 46/100, Train Loss: -0.0383, Test Loss: -0.0713\n",
      "Epoch 47/100, Train Loss: -0.0418, Test Loss: -0.0705\n",
      "Epoch 48/100, Train Loss: -0.0455, Test Loss: -0.0720\n",
      "Epoch 49/100, Train Loss: -0.0448, Test Loss: -0.0783\n",
      "Epoch 50/100, Train Loss: -0.0464, Test Loss: -0.0749\n",
      "Epoch 51/100, Train Loss: -0.0471, Test Loss: -0.0749\n",
      "Epoch 52/100, Train Loss: -0.0469, Test Loss: -0.0772\n",
      "Epoch 53/100, Train Loss: -0.0482, Test Loss: -0.0775\n",
      "Epoch 54/100, Train Loss: -0.0475, Test Loss: -0.0788\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.04e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 343       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 5         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.06e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 312       |\n",
      "|    iterations             | 2         |\n",
      "|    time_elapsed           | 13        |\n",
      "|    total_timesteps        | 4096      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 4.33e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00659   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 25        |\n",
      "|    policy_objective       | 0.0103    |\n",
      "|    std                    | 0.876     |\n",
      "|    value_loss             | 2.18e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.07e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 296       |\n",
      "|    iterations             | 3         |\n",
      "|    time_elapsed           | 20        |\n",
      "|    total_timesteps        | 6144      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.45e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00906   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 26        |\n",
      "|    policy_objective       | 0.00962   |\n",
      "|    std                    | 0.876     |\n",
      "|    value_loss             | 2.33e+03  |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.05e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 291       |\n",
      "|    iterations             | 4         |\n",
      "|    time_elapsed           | 28        |\n",
      "|    total_timesteps        | 8192      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 2.71e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.0083    |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 27        |\n",
      "|    policy_objective       | 0.0112    |\n",
      "|    std                    | 0.869     |\n",
      "|    value_loss             | 2.07e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.03e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 289       |\n",
      "|    iterations             | 5         |\n",
      "|    time_elapsed           | 35        |\n",
      "|    total_timesteps        | 10240     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 3.19e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00799   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 28        |\n",
      "|    policy_objective       | 0.0132    |\n",
      "|    std                    | 0.906     |\n",
      "|    value_loss             | 1.29e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.04e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 288       |\n",
      "|    iterations             | 6         |\n",
      "|    time_elapsed           | 42        |\n",
      "|    total_timesteps        | 12288     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.96e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00653   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 29        |\n",
      "|    policy_objective       | 0.0071    |\n",
      "|    std                    | 0.957     |\n",
      "|    value_loss             | 1.29e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.02e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 290       |\n",
      "|    iterations             | 7         |\n",
      "|    time_elapsed           | 49        |\n",
      "|    total_timesteps        | 14336     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 2.03e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00877   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 30        |\n",
      "|    policy_objective       | 0.0106    |\n",
      "|    std                    | 0.947     |\n",
      "|    value_loss             | 1.85e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.02e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 291       |\n",
      "|    iterations             | 8         |\n",
      "|    time_elapsed           | 56        |\n",
      "|    total_timesteps        | 16384     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 2.03e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00748   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 31        |\n",
      "|    policy_objective       | 0.00894   |\n",
      "|    std                    | 0.922     |\n",
      "|    value_loss             | 902       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-1114.5696154884995\n",
      "------------------------------\n",
      "round: 4\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.7402, Test Loss: -0.0004\n",
      "Epoch 2/100, Train Loss: 0.3688, Test Loss: 0.0007\n",
      "Epoch 3/100, Train Loss: 0.1734, Test Loss: -0.0308\n",
      "Epoch 4/100, Train Loss: 0.0926, Test Loss: -0.0343\n",
      "Epoch 5/100, Train Loss: 0.0464, Test Loss: -0.0544\n",
      "Epoch 6/100, Train Loss: 0.0167, Test Loss: -0.0574\n",
      "Epoch 7/100, Train Loss: 0.0046, Test Loss: -0.0550\n",
      "Epoch 8/100, Train Loss: -0.0030, Test Loss: -0.0581\n",
      "Epoch 9/100, Train Loss: -0.0126, Test Loss: -0.0598\n",
      "Epoch 10/100, Train Loss: -0.0172, Test Loss: -0.0567\n",
      "Epoch 11/100, Train Loss: -0.0180, Test Loss: -0.0558\n",
      "Epoch 12/100, Train Loss: -0.0198, Test Loss: -0.0575\n",
      "Epoch 13/100, Train Loss: -0.0224, Test Loss: -0.0591\n",
      "Epoch 14/100, Train Loss: -0.0238, Test Loss: -0.0591\n",
      "Epoch 15/100, Train Loss: -0.0242, Test Loss: -0.0594\n",
      "Epoch 16/100, Train Loss: -0.0251, Test Loss: -0.0597\n",
      "Epoch 17/100, Train Loss: -0.0259, Test Loss: -0.0598\n",
      "Epoch 18/100, Train Loss: -0.0265, Test Loss: -0.0596\n",
      "Epoch 19/100, Train Loss: -0.0269, Test Loss: -0.0596\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 0.0563, Test Loss: -0.0296\n",
      "Epoch 2/100, Train Loss: 0.0268, Test Loss: -0.0310\n",
      "Epoch 3/100, Train Loss: -0.0276, Test Loss: -0.0322\n",
      "Epoch 4/100, Train Loss: -0.0103, Test Loss: -0.0383\n",
      "Epoch 5/100, Train Loss: -0.0309, Test Loss: -0.0523\n",
      "Epoch 6/100, Train Loss: -0.0329, Test Loss: -0.0431\n",
      "Epoch 7/100, Train Loss: -0.0291, Test Loss: -0.0493\n",
      "Epoch 8/100, Train Loss: -0.0364, Test Loss: -0.0531\n",
      "Epoch 9/100, Train Loss: -0.0374, Test Loss: -0.0509\n",
      "Epoch 10/100, Train Loss: -0.0356, Test Loss: -0.0516\n",
      "Epoch 11/100, Train Loss: -0.0375, Test Loss: -0.0539\n",
      "Epoch 12/100, Train Loss: -0.0391, Test Loss: -0.0537\n",
      "Epoch 13/100, Train Loss: -0.0388, Test Loss: -0.0538\n",
      "Epoch 14/100, Train Loss: -0.0392, Test Loss: -0.0552\n",
      "Epoch 15/100, Train Loss: -0.0403, Test Loss: -0.0561\n",
      "Epoch 16/100, Train Loss: -0.0406, Test Loss: -0.0564\n",
      "Epoch 17/100, Train Loss: -0.0410, Test Loss: -0.0573\n",
      "Epoch 18/100, Train Loss: -0.0418, Test Loss: -0.0581\n",
      "Epoch 19/100, Train Loss: -0.0423, Test Loss: -0.0586\n",
      "Epoch 20/100, Train Loss: -0.0428, Test Loss: -0.0593\n",
      "Epoch 21/100, Train Loss: -0.0434, Test Loss: -0.0600\n",
      "Epoch 22/100, Train Loss: -0.0439, Test Loss: -0.0608\n",
      "Epoch 23/100, Train Loss: -0.0445, Test Loss: -0.0614\n",
      "Epoch 24/100, Train Loss: -0.0450, Test Loss: -0.0620\n",
      "Epoch 25/100, Train Loss: -0.0456, Test Loss: -0.0624\n",
      "Epoch 26/100, Train Loss: -0.0460, Test Loss: -0.0628\n",
      "Epoch 27/100, Train Loss: -0.0466, Test Loss: -0.0633\n",
      "Epoch 28/100, Train Loss: -0.0470, Test Loss: -0.0636\n",
      "Epoch 29/100, Train Loss: -0.0475, Test Loss: -0.0637\n",
      "Epoch 30/100, Train Loss: -0.0481, Test Loss: -0.0639\n",
      "Epoch 31/100, Train Loss: -0.0487, Test Loss: -0.0642\n",
      "Epoch 32/100, Train Loss: -0.0488, Test Loss: -0.0639\n",
      "Epoch 33/100, Train Loss: -0.0490, Test Loss: -0.0624\n",
      "Epoch 34/100, Train Loss: -0.0480, Test Loss: -0.0612\n",
      "Epoch 35/100, Train Loss: -0.0453, Test Loss: -0.0551\n",
      "Epoch 36/100, Train Loss: -0.0456, Test Loss: -0.0581\n",
      "Epoch 37/100, Train Loss: -0.0448, Test Loss: -0.0564\n",
      "Epoch 38/100, Train Loss: -0.0466, Test Loss: -0.0604\n",
      "Epoch 39/100, Train Loss: -0.0465, Test Loss: -0.0609\n",
      "Epoch 40/100, Train Loss: -0.0459, Test Loss: -0.0582\n",
      "Epoch 41/100, Train Loss: -0.0432, Test Loss: -0.0500\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 0.4487, Test Loss: 0.3242\n",
      "Epoch 2/100, Train Loss: 0.3863, Test Loss: 0.0918\n",
      "Epoch 3/100, Train Loss: 0.0618, Test Loss: 0.3251\n",
      "Epoch 4/100, Train Loss: 0.1730, Test Loss: -0.0349\n",
      "Epoch 5/100, Train Loss: 0.0070, Test Loss: 0.1372\n",
      "Epoch 6/100, Train Loss: 0.0620, Test Loss: -0.0505\n",
      "Epoch 7/100, Train Loss: -0.0231, Test Loss: 0.0104\n",
      "Epoch 8/100, Train Loss: 0.0132, Test Loss: -0.0475\n",
      "Epoch 9/100, Train Loss: -0.0269, Test Loss: -0.0462\n",
      "Epoch 10/100, Train Loss: -0.0172, Test Loss: -0.0513\n",
      "Epoch 11/100, Train Loss: -0.0322, Test Loss: -0.0695\n",
      "Epoch 12/100, Train Loss: -0.0308, Test Loss: -0.0599\n",
      "Epoch 13/100, Train Loss: -0.0339, Test Loss: -0.0744\n",
      "Epoch 14/100, Train Loss: -0.0371, Test Loss: -0.0647\n",
      "Epoch 15/100, Train Loss: -0.0370, Test Loss: -0.0760\n",
      "Epoch 16/100, Train Loss: -0.0399, Test Loss: -0.0698\n",
      "Epoch 17/100, Train Loss: -0.0389, Test Loss: -0.0762\n",
      "Epoch 18/100, Train Loss: -0.0410, Test Loss: -0.0728\n",
      "Epoch 19/100, Train Loss: -0.0406, Test Loss: -0.0765\n",
      "Epoch 20/100, Train Loss: -0.0418, Test Loss: -0.0747\n",
      "Epoch 21/100, Train Loss: -0.0415, Test Loss: -0.0765\n",
      "Epoch 22/100, Train Loss: -0.0422, Test Loss: -0.0755\n",
      "Epoch 23/100, Train Loss: -0.0422, Test Loss: -0.0765\n",
      "Epoch 24/100, Train Loss: -0.0426, Test Loss: -0.0762\n",
      "Epoch 25/100, Train Loss: -0.0427, Test Loss: -0.0767\n",
      "Epoch 26/100, Train Loss: -0.0429, Test Loss: -0.0768\n",
      "Epoch 27/100, Train Loss: -0.0431, Test Loss: -0.0771\n",
      "Epoch 28/100, Train Loss: -0.0433, Test Loss: -0.0773\n",
      "Epoch 29/100, Train Loss: -0.0435, Test Loss: -0.0773\n",
      "Epoch 30/100, Train Loss: -0.0436, Test Loss: -0.0775\n",
      "Epoch 31/100, Train Loss: -0.0438, Test Loss: -0.0775\n",
      "Epoch 32/100, Train Loss: -0.0439, Test Loss: -0.0777\n",
      "Epoch 33/100, Train Loss: -0.0441, Test Loss: -0.0779\n",
      "Epoch 34/100, Train Loss: -0.0442, Test Loss: -0.0780\n",
      "Epoch 35/100, Train Loss: -0.0444, Test Loss: -0.0783\n",
      "Epoch 36/100, Train Loss: -0.0445, Test Loss: -0.0784\n",
      "Epoch 37/100, Train Loss: -0.0446, Test Loss: -0.0785\n",
      "Epoch 38/100, Train Loss: -0.0448, Test Loss: -0.0786\n",
      "Epoch 39/100, Train Loss: -0.0449, Test Loss: -0.0788\n",
      "Epoch 40/100, Train Loss: -0.0451, Test Loss: -0.0789\n",
      "Epoch 41/100, Train Loss: -0.0452, Test Loss: -0.0791\n",
      "Epoch 42/100, Train Loss: -0.0453, Test Loss: -0.0792\n",
      "Epoch 43/100, Train Loss: -0.0455, Test Loss: -0.0794\n",
      "Epoch 44/100, Train Loss: -0.0456, Test Loss: -0.0796\n",
      "Epoch 45/100, Train Loss: -0.0458, Test Loss: -0.0798\n",
      "Epoch 46/100, Train Loss: -0.0459, Test Loss: -0.0799\n",
      "Epoch 47/100, Train Loss: -0.0460, Test Loss: -0.0801\n",
      "Epoch 48/100, Train Loss: -0.0462, Test Loss: -0.0803\n",
      "Epoch 49/100, Train Loss: -0.0463, Test Loss: -0.0805\n",
      "Epoch 50/100, Train Loss: -0.0465, Test Loss: -0.0807\n",
      "Epoch 51/100, Train Loss: -0.0466, Test Loss: -0.0808\n",
      "Epoch 52/100, Train Loss: -0.0468, Test Loss: -0.0810\n",
      "Epoch 53/100, Train Loss: -0.0469, Test Loss: -0.0812\n",
      "Epoch 54/100, Train Loss: -0.0471, Test Loss: -0.0815\n",
      "Epoch 55/100, Train Loss: -0.0472, Test Loss: -0.0817\n",
      "Epoch 56/100, Train Loss: -0.0474, Test Loss: -0.0819\n",
      "Epoch 57/100, Train Loss: -0.0475, Test Loss: -0.0821\n",
      "Epoch 58/100, Train Loss: -0.0477, Test Loss: -0.0823\n",
      "Epoch 59/100, Train Loss: -0.0478, Test Loss: -0.0826\n",
      "Epoch 60/100, Train Loss: -0.0480, Test Loss: -0.0828\n",
      "Epoch 61/100, Train Loss: -0.0482, Test Loss: -0.0831\n",
      "Epoch 62/100, Train Loss: -0.0483, Test Loss: -0.0833\n",
      "Epoch 63/100, Train Loss: -0.0485, Test Loss: -0.0835\n",
      "Epoch 64/100, Train Loss: -0.0486, Test Loss: -0.0838\n",
      "Epoch 65/100, Train Loss: -0.0488, Test Loss: -0.0840\n",
      "Epoch 66/100, Train Loss: -0.0489, Test Loss: -0.0842\n",
      "Epoch 67/100, Train Loss: -0.0491, Test Loss: -0.0844\n",
      "Epoch 68/100, Train Loss: -0.0493, Test Loss: -0.0846\n",
      "Epoch 69/100, Train Loss: -0.0494, Test Loss: -0.0849\n",
      "Epoch 70/100, Train Loss: -0.0496, Test Loss: -0.0851\n",
      "Epoch 71/100, Train Loss: -0.0497, Test Loss: -0.0853\n",
      "Epoch 72/100, Train Loss: -0.0499, Test Loss: -0.0855\n",
      "Epoch 73/100, Train Loss: -0.0501, Test Loss: -0.0858\n",
      "Epoch 74/100, Train Loss: -0.0502, Test Loss: -0.0860\n",
      "Epoch 75/100, Train Loss: -0.0504, Test Loss: -0.0862\n",
      "Epoch 76/100, Train Loss: -0.0505, Test Loss: -0.0865\n",
      "Epoch 77/100, Train Loss: -0.0507, Test Loss: -0.0867\n",
      "Epoch 78/100, Train Loss: -0.0509, Test Loss: -0.0870\n",
      "Epoch 79/100, Train Loss: -0.0510, Test Loss: -0.0872\n",
      "Epoch 80/100, Train Loss: -0.0512, Test Loss: -0.0874\n",
      "Epoch 81/100, Train Loss: -0.0513, Test Loss: -0.0876\n",
      "Epoch 82/100, Train Loss: -0.0515, Test Loss: -0.0878\n",
      "Epoch 83/100, Train Loss: -0.0517, Test Loss: -0.0881\n",
      "Epoch 84/100, Train Loss: -0.0518, Test Loss: -0.0883\n",
      "Epoch 85/100, Train Loss: -0.0520, Test Loss: -0.0885\n",
      "Epoch 86/100, Train Loss: -0.0522, Test Loss: -0.0888\n",
      "Epoch 87/100, Train Loss: -0.0523, Test Loss: -0.0890\n",
      "Epoch 88/100, Train Loss: -0.0525, Test Loss: -0.0893\n",
      "Epoch 89/100, Train Loss: -0.0526, Test Loss: -0.0895\n",
      "Epoch 90/100, Train Loss: -0.0528, Test Loss: -0.0897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/100, Train Loss: -0.0530, Test Loss: -0.0899\n",
      "Epoch 92/100, Train Loss: -0.0531, Test Loss: -0.0901\n",
      "Epoch 93/100, Train Loss: -0.0533, Test Loss: -0.0903\n",
      "Epoch 94/100, Train Loss: -0.0534, Test Loss: -0.0906\n",
      "Epoch 95/100, Train Loss: -0.0536, Test Loss: -0.0908\n",
      "Epoch 96/100, Train Loss: -0.0538, Test Loss: -0.0910\n",
      "Epoch 97/100, Train Loss: -0.0539, Test Loss: -0.0912\n",
      "Epoch 98/100, Train Loss: -0.0541, Test Loss: -0.0914\n",
      "Epoch 99/100, Train Loss: -0.0542, Test Loss: -0.0915\n",
      "Epoch 100/100, Train Loss: -0.0544, Test Loss: -0.0917\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -960     |\n",
      "| time/              |          |\n",
      "|    fps             | 330      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.01e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 320       |\n",
      "|    iterations             | 2         |\n",
      "|    time_elapsed           | 12        |\n",
      "|    total_timesteps        | 4096      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.34e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00853   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 33        |\n",
      "|    policy_objective       | 0.00937   |\n",
      "|    std                    | 0.872     |\n",
      "|    value_loss             | 1.3e+03   |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -963     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 322      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 19       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 1.56e-05 |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00696  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 34       |\n",
      "|    policy_objective       | 0.0102   |\n",
      "|    std                    | 0.846    |\n",
      "|    value_loss             | 1.38e+03 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -972     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 328      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 24       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 1.59e-05 |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00616  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 35       |\n",
      "|    policy_objective       | 0.00668  |\n",
      "|    std                    | 0.863    |\n",
      "|    value_loss             | 697      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -966     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 330      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 31       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 2.1e-05  |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00628  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 36       |\n",
      "|    policy_objective       | 0.0126   |\n",
      "|    std                    | 0.847    |\n",
      "|    value_loss             | 1.2e+03  |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -963     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 332      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 36       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 1.61e-05 |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00836  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 37       |\n",
      "|    policy_objective       | 0.0101   |\n",
      "|    std                    | 0.854    |\n",
      "|    value_loss             | 799      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -967     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 334      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 42       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 1.54e-05 |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00771  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 38       |\n",
      "|    policy_objective       | 0.00987  |\n",
      "|    std                    | 0.836    |\n",
      "|    value_loss             | 861      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -960     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 335      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 48       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 1.25e-05 |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00747  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 39       |\n",
      "|    policy_objective       | 0.00708  |\n",
      "|    std                    | 0.825    |\n",
      "|    value_loss             | 790      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-1011.0279759377241\n",
      "------------------------------\n",
      "round: 5\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.8455, Test Loss: -0.0471\n",
      "Epoch 2/100, Train Loss: 0.3217, Test Loss: -0.0243\n",
      "Epoch 3/100, Train Loss: 0.1408, Test Loss: -0.0332\n",
      "Epoch 4/100, Train Loss: 0.0668, Test Loss: -0.0378\n",
      "Epoch 5/100, Train Loss: 0.0297, Test Loss: -0.0449\n",
      "Epoch 6/100, Train Loss: 0.0095, Test Loss: -0.0466\n",
      "Epoch 7/100, Train Loss: -0.0009, Test Loss: -0.0468\n",
      "Epoch 8/100, Train Loss: -0.0071, Test Loss: -0.0459\n",
      "Epoch 9/100, Train Loss: -0.0107, Test Loss: -0.0469\n",
      "Epoch 10/100, Train Loss: -0.0142, Test Loss: -0.0481\n",
      "Epoch 11/100, Train Loss: -0.0168, Test Loss: -0.0492\n",
      "Epoch 12/100, Train Loss: -0.0188, Test Loss: -0.0498\n",
      "Epoch 13/100, Train Loss: -0.0200, Test Loss: -0.0505\n",
      "Epoch 14/100, Train Loss: -0.0213, Test Loss: -0.0515\n",
      "Epoch 15/100, Train Loss: -0.0225, Test Loss: -0.0525\n",
      "Epoch 16/100, Train Loss: -0.0238, Test Loss: -0.0538\n",
      "Epoch 17/100, Train Loss: -0.0249, Test Loss: -0.0547\n",
      "Epoch 18/100, Train Loss: -0.0255, Test Loss: -0.0558\n",
      "Epoch 19/100, Train Loss: -0.0263, Test Loss: -0.0564\n",
      "Epoch 20/100, Train Loss: -0.0270, Test Loss: -0.0577\n",
      "Epoch 21/100, Train Loss: -0.0268, Test Loss: -0.0546\n",
      "Epoch 22/100, Train Loss: -0.0271, Test Loss: -0.0591\n",
      "Epoch 23/100, Train Loss: -0.0286, Test Loss: -0.0587\n",
      "Epoch 24/100, Train Loss: -0.0290, Test Loss: -0.0587\n",
      "Epoch 25/100, Train Loss: -0.0299, Test Loss: -0.0585\n",
      "Epoch 26/100, Train Loss: -0.0284, Test Loss: -0.0613\n",
      "Epoch 27/100, Train Loss: -0.0292, Test Loss: -0.0518\n",
      "Epoch 28/100, Train Loss: -0.0261, Test Loss: -0.0579\n",
      "Epoch 29/100, Train Loss: -0.0302, Test Loss: -0.0594\n",
      "Epoch 30/100, Train Loss: -0.0312, Test Loss: -0.0612\n",
      "Epoch 31/100, Train Loss: -0.0323, Test Loss: -0.0626\n",
      "Epoch 32/100, Train Loss: -0.0332, Test Loss: -0.0634\n",
      "Epoch 33/100, Train Loss: -0.0338, Test Loss: -0.0634\n",
      "Epoch 34/100, Train Loss: -0.0340, Test Loss: -0.0639\n",
      "Epoch 35/100, Train Loss: -0.0342, Test Loss: -0.0651\n",
      "Epoch 36/100, Train Loss: -0.0351, Test Loss: -0.0636\n",
      "Epoch 37/100, Train Loss: -0.0331, Test Loss: -0.0658\n",
      "Epoch 38/100, Train Loss: -0.0340, Test Loss: -0.0571\n",
      "Epoch 39/100, Train Loss: -0.0301, Test Loss: -0.0585\n",
      "Epoch 40/100, Train Loss: -0.0281, Test Loss: -0.0520\n",
      "Epoch 41/100, Train Loss: -0.0304, Test Loss: -0.0582\n",
      "Epoch 42/100, Train Loss: -0.0342, Test Loss: -0.0615\n",
      "Epoch 43/100, Train Loss: -0.0349, Test Loss: -0.0630\n",
      "Epoch 44/100, Train Loss: -0.0360, Test Loss: -0.0656\n",
      "Epoch 45/100, Train Loss: -0.0363, Test Loss: -0.0661\n",
      "Epoch 46/100, Train Loss: -0.0364, Test Loss: -0.0670\n",
      "Epoch 47/100, Train Loss: -0.0370, Test Loss: -0.0679\n",
      "Epoch 48/100, Train Loss: -0.0374, Test Loss: -0.0679\n",
      "Epoch 49/100, Train Loss: -0.0385, Test Loss: -0.0683\n",
      "Epoch 50/100, Train Loss: -0.0385, Test Loss: -0.0682\n",
      "Epoch 51/100, Train Loss: -0.0388, Test Loss: -0.0689\n",
      "Epoch 52/100, Train Loss: -0.0393, Test Loss: -0.0592\n",
      "Epoch 53/100, Train Loss: -0.0339, Test Loss: -0.0622\n",
      "Epoch 54/100, Train Loss: -0.0329, Test Loss: -0.0587\n",
      "Epoch 55/100, Train Loss: -0.0323, Test Loss: -0.0672\n",
      "Epoch 56/100, Train Loss: -0.0327, Test Loss: -0.0527\n",
      "Epoch 57/100, Train Loss: -0.0341, Test Loss: -0.0596\n",
      "Epoch 58/100, Train Loss: -0.0378, Test Loss: -0.0608\n",
      "Epoch 59/100, Train Loss: -0.0379, Test Loss: -0.0651\n",
      "Epoch 60/100, Train Loss: -0.0390, Test Loss: -0.0662\n",
      "Epoch 61/100, Train Loss: -0.0391, Test Loss: -0.0693\n",
      "Epoch 62/100, Train Loss: -0.0397, Test Loss: -0.0711\n",
      "Epoch 63/100, Train Loss: -0.0406, Test Loss: -0.0712\n",
      "Epoch 64/100, Train Loss: -0.0417, Test Loss: -0.0719\n",
      "Epoch 65/100, Train Loss: -0.0426, Test Loss: -0.0715\n",
      "Epoch 66/100, Train Loss: -0.0429, Test Loss: -0.0730\n",
      "Epoch 67/100, Train Loss: -0.0433, Test Loss: -0.0738\n",
      "Epoch 68/100, Train Loss: -0.0437, Test Loss: -0.0743\n",
      "Epoch 69/100, Train Loss: -0.0442, Test Loss: -0.0748\n",
      "Epoch 70/100, Train Loss: -0.0447, Test Loss: -0.0753\n",
      "Epoch 71/100, Train Loss: -0.0451, Test Loss: -0.0761\n",
      "Epoch 72/100, Train Loss: -0.0456, Test Loss: -0.0767\n",
      "Epoch 73/100, Train Loss: -0.0460, Test Loss: -0.0775\n",
      "Epoch 74/100, Train Loss: -0.0465, Test Loss: -0.0782\n",
      "Epoch 75/100, Train Loss: -0.0470, Test Loss: -0.0786\n",
      "Epoch 76/100, Train Loss: -0.0473, Test Loss: -0.0791\n",
      "Epoch 77/100, Train Loss: -0.0476, Test Loss: -0.0790\n",
      "Epoch 78/100, Train Loss: -0.0480, Test Loss: -0.0774\n",
      "Epoch 79/100, Train Loss: -0.0475, Test Loss: -0.0752\n",
      "Epoch 80/100, Train Loss: -0.0460, Test Loss: -0.0782\n",
      "Epoch 81/100, Train Loss: -0.0453, Test Loss: -0.0522\n",
      "Epoch 82/100, Train Loss: -0.0292, Test Loss: -0.0767\n",
      "Epoch 83/100, Train Loss: -0.0379, Test Loss: -0.0543\n",
      "Epoch 84/100, Train Loss: -0.0346, Test Loss: -0.0491\n",
      "Epoch 85/100, Train Loss: -0.0320, Test Loss: -0.0722\n",
      "Epoch 86/100, Train Loss: -0.0404, Test Loss: -0.0510\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 1.6586, Test Loss: -0.0336\n",
      "Epoch 2/100, Train Loss: 0.6618, Test Loss: 0.0712\n",
      "Epoch 3/100, Train Loss: 0.2803, Test Loss: -0.0500\n",
      "Epoch 4/100, Train Loss: 0.1161, Test Loss: -0.0184\n",
      "Epoch 5/100, Train Loss: 0.0778, Test Loss: -0.0346\n",
      "Epoch 6/100, Train Loss: 0.0295, Test Loss: -0.0525\n",
      "Epoch 7/100, Train Loss: 0.0047, Test Loss: -0.0480\n",
      "Epoch 8/100, Train Loss: 0.0007, Test Loss: -0.0462\n",
      "Epoch 9/100, Train Loss: -0.0036, Test Loss: -0.0516\n",
      "Epoch 10/100, Train Loss: -0.0106, Test Loss: -0.0552\n",
      "Epoch 11/100, Train Loss: -0.0155, Test Loss: -0.0543\n",
      "Epoch 12/100, Train Loss: -0.0170, Test Loss: -0.0527\n",
      "Epoch 13/100, Train Loss: -0.0178, Test Loss: -0.0534\n",
      "Epoch 14/100, Train Loss: -0.0193, Test Loss: -0.0554\n",
      "Epoch 15/100, Train Loss: -0.0211, Test Loss: -0.0566\n",
      "Epoch 16/100, Train Loss: -0.0222, Test Loss: -0.0568\n",
      "Epoch 17/100, Train Loss: -0.0229, Test Loss: -0.0571\n",
      "Epoch 18/100, Train Loss: -0.0235, Test Loss: -0.0580\n",
      "Epoch 19/100, Train Loss: -0.0244, Test Loss: -0.0589\n",
      "Epoch 20/100, Train Loss: -0.0252, Test Loss: -0.0596\n",
      "Epoch 21/100, Train Loss: -0.0258, Test Loss: -0.0605\n",
      "Epoch 22/100, Train Loss: -0.0265, Test Loss: -0.0614\n",
      "Epoch 23/100, Train Loss: -0.0272, Test Loss: -0.0621\n",
      "Epoch 24/100, Train Loss: -0.0278, Test Loss: -0.0631\n",
      "Epoch 25/100, Train Loss: -0.0285, Test Loss: -0.0640\n",
      "Epoch 26/100, Train Loss: -0.0291, Test Loss: -0.0649\n",
      "Epoch 27/100, Train Loss: -0.0297, Test Loss: -0.0657\n",
      "Epoch 28/100, Train Loss: -0.0303, Test Loss: -0.0668\n",
      "Epoch 29/100, Train Loss: -0.0309, Test Loss: -0.0676\n",
      "Epoch 30/100, Train Loss: -0.0315, Test Loss: -0.0683\n",
      "Epoch 31/100, Train Loss: -0.0321, Test Loss: -0.0694\n",
      "Epoch 32/100, Train Loss: -0.0326, Test Loss: -0.0697\n",
      "Epoch 33/100, Train Loss: -0.0330, Test Loss: -0.0698\n",
      "Epoch 34/100, Train Loss: -0.0335, Test Loss: -0.0710\n",
      "Epoch 35/100, Train Loss: -0.0343, Test Loss: -0.0717\n",
      "Epoch 36/100, Train Loss: -0.0346, Test Loss: -0.0691\n",
      "Epoch 37/100, Train Loss: -0.0343, Test Loss: -0.0726\n",
      "Epoch 38/100, Train Loss: -0.0350, Test Loss: -0.0644\n",
      "Epoch 39/100, Train Loss: -0.0318, Test Loss: -0.0729\n",
      "Epoch 40/100, Train Loss: -0.0349, Test Loss: -0.0697\n",
      "Epoch 41/100, Train Loss: -0.0361, Test Loss: -0.0714\n",
      "Epoch 42/100, Train Loss: -0.0366, Test Loss: -0.0730\n",
      "Epoch 43/100, Train Loss: -0.0372, Test Loss: -0.0734\n",
      "Epoch 44/100, Train Loss: -0.0373, Test Loss: -0.0743\n",
      "Epoch 45/100, Train Loss: -0.0376, Test Loss: -0.0748\n",
      "Epoch 46/100, Train Loss: -0.0376, Test Loss: -0.0760\n",
      "Epoch 47/100, Train Loss: -0.0383, Test Loss: -0.0761\n",
      "Epoch 48/100, Train Loss: -0.0382, Test Loss: -0.0766\n",
      "Epoch 49/100, Train Loss: -0.0392, Test Loss: -0.0772\n",
      "Epoch 50/100, Train Loss: -0.0396, Test Loss: -0.0770\n",
      "Epoch 51/100, Train Loss: -0.0402, Test Loss: -0.0777\n",
      "Epoch 52/100, Train Loss: -0.0406, Test Loss: -0.0781\n",
      "Epoch 53/100, Train Loss: -0.0407, Test Loss: -0.0781\n",
      "Epoch 54/100, Train Loss: -0.0413, Test Loss: -0.0796\n",
      "Epoch 55/100, Train Loss: -0.0418, Test Loss: -0.0802\n",
      "Epoch 56/100, Train Loss: -0.0423, Test Loss: -0.0802\n",
      "Epoch 57/100, Train Loss: -0.0422, Test Loss: -0.0762\n",
      "Epoch 58/100, Train Loss: -0.0412, Test Loss: -0.0738\n",
      "Epoch 59/100, Train Loss: -0.0414, Test Loss: -0.0801\n",
      "Epoch 60/100, Train Loss: -0.0421, Test Loss: -0.0720\n",
      "Epoch 61/100, Train Loss: -0.0401, Test Loss: -0.0791\n",
      "Epoch 62/100, Train Loss: -0.0420, Test Loss: -0.0655\n",
      "Epoch 63/100, Train Loss: -0.0381, Test Loss: -0.0736\n",
      "Epoch 64/100, Train Loss: -0.0391, Test Loss: -0.0710\n",
      "Epoch 65/100, Train Loss: -0.0404, Test Loss: -0.0787\n",
      "Epoch 66/100, Train Loss: -0.0411, Test Loss: -0.0643\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.3455, Test Loss: 1.1498\n",
      "Epoch 2/100, Train Loss: 0.3845, Test Loss: 0.2012\n",
      "Epoch 3/100, Train Loss: 0.1322, Test Loss: 0.4665\n",
      "Epoch 4/100, Train Loss: 0.0630, Test Loss: 0.3259\n",
      "Epoch 5/100, Train Loss: 0.0880, Test Loss: 0.1521\n",
      "Epoch 6/100, Train Loss: -0.0103, Test Loss: 0.2658\n",
      "Epoch 7/100, Train Loss: 0.0273, Test Loss: 0.0974\n",
      "Epoch 8/100, Train Loss: -0.0195, Test Loss: 0.1740\n",
      "Epoch 9/100, Train Loss: -0.0130, Test Loss: 0.0942\n",
      "Epoch 10/100, Train Loss: -0.0243, Test Loss: 0.1141\n",
      "Epoch 11/100, Train Loss: -0.0327, Test Loss: 0.1008\n",
      "Epoch 12/100, Train Loss: -0.0291, Test Loss: 0.0790\n",
      "Epoch 13/100, Train Loss: -0.0395, Test Loss: 0.0888\n",
      "Epoch 14/100, Train Loss: -0.0367, Test Loss: 0.0718\n",
      "Epoch 15/100, Train Loss: -0.0409, Test Loss: 0.0801\n",
      "Epoch 16/100, Train Loss: -0.0419, Test Loss: 0.0753\n",
      "Epoch 17/100, Train Loss: -0.0419, Test Loss: 0.0718\n",
      "Epoch 18/100, Train Loss: -0.0442, Test Loss: 0.0762\n",
      "Epoch 19/100, Train Loss: -0.0439, Test Loss: 0.0735\n",
      "Epoch 20/100, Train Loss: -0.0445, Test Loss: 0.0735\n",
      "Epoch 21/100, Train Loss: -0.0455, Test Loss: 0.0772\n",
      "Epoch 22/100, Train Loss: -0.0455, Test Loss: 0.0775\n",
      "Epoch 23/100, Train Loss: -0.0460, Test Loss: 0.0796\n",
      "Epoch 24/100, Train Loss: -0.0466, Test Loss: 0.0825\n",
      "Epoch 25/100, Train Loss: -0.0468, Test Loss: 0.0844\n",
      "Epoch 26/100, Train Loss: -0.0471, Test Loss: 0.0867\n",
      "Epoch 27/100, Train Loss: -0.0476, Test Loss: 0.0900\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -833     |\n",
      "| time/              |          |\n",
      "|    fps             | 375      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -891     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 351      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 11       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 1.2e-05  |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00862  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 41       |\n",
      "|    policy_objective       | 0.0109   |\n",
      "|    std                    | 0.801    |\n",
      "|    value_loss             | 586      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -906     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 348      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 17       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 2.13e-05 |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00761  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 42       |\n",
      "|    policy_objective       | 0.0123   |\n",
      "|    std                    | 0.78     |\n",
      "|    value_loss             | 813      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -918     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 347      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 23       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.102    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00786  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 43       |\n",
      "|    policy_objective       | 0.00989  |\n",
      "|    std                    | 0.736    |\n",
      "|    value_loss             | 655      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -921     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 347      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 29       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.397    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00783  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 44       |\n",
      "|    policy_objective       | 0.00691  |\n",
      "|    std                    | 0.758    |\n",
      "|    value_loss             | 588      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -940     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 344      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 35       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.436    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00558  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 45       |\n",
      "|    policy_objective       | 0.0107   |\n",
      "|    std                    | 0.755    |\n",
      "|    value_loss             | 535      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -944     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 344      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 41       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.236    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00727  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 46       |\n",
      "|    policy_objective       | 0.00968  |\n",
      "|    std                    | 0.752    |\n",
      "|    value_loss             | 657      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -932     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 342      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 47       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.32     |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00714  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 47       |\n",
      "|    policy_objective       | 0.00848  |\n",
      "|    std                    | 0.733    |\n",
      "|    value_loss             | 575      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-898.0612158778589\n",
      "------------------------------\n",
      "round: 6\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 10.6875, Test Loss: 2.2321\n",
      "Epoch 2/100, Train Loss: 5.8580, Test Loss: 1.1414\n",
      "Epoch 3/100, Train Loss: 2.7226, Test Loss: 0.6354\n",
      "Epoch 4/100, Train Loss: 1.3152, Test Loss: 0.4689\n",
      "Epoch 5/100, Train Loss: 0.7326, Test Loss: 0.3070\n",
      "Epoch 6/100, Train Loss: 0.4657, Test Loss: 0.2305\n",
      "Epoch 7/100, Train Loss: 0.3295, Test Loss: 0.1790\n",
      "Epoch 8/100, Train Loss: 0.2401, Test Loss: 0.1478\n",
      "Epoch 9/100, Train Loss: 0.1775, Test Loss: 0.1177\n",
      "Epoch 10/100, Train Loss: 0.1312, Test Loss: 0.0952\n",
      "Epoch 11/100, Train Loss: 0.1001, Test Loss: 0.0815\n",
      "Epoch 12/100, Train Loss: 0.0783, Test Loss: 0.0714\n",
      "Epoch 13/100, Train Loss: 0.0610, Test Loss: 0.0635\n",
      "Epoch 14/100, Train Loss: 0.0474, Test Loss: 0.0580\n",
      "Epoch 15/100, Train Loss: 0.0372, Test Loss: 0.0539\n",
      "Epoch 16/100, Train Loss: 0.0293, Test Loss: 0.0503\n",
      "Epoch 17/100, Train Loss: 0.0229, Test Loss: 0.0473\n",
      "Epoch 18/100, Train Loss: 0.0178, Test Loss: 0.0449\n",
      "Epoch 19/100, Train Loss: 0.0137, Test Loss: 0.0431\n",
      "Epoch 20/100, Train Loss: 0.0102, Test Loss: 0.0417\n",
      "Epoch 21/100, Train Loss: 0.0072, Test Loss: 0.0406\n",
      "Epoch 22/100, Train Loss: 0.0047, Test Loss: 0.0397\n",
      "Epoch 23/100, Train Loss: 0.0025, Test Loss: 0.0390\n",
      "Epoch 24/100, Train Loss: 0.0006, Test Loss: 0.0383\n",
      "Epoch 25/100, Train Loss: -0.0012, Test Loss: 0.0378\n",
      "Epoch 26/100, Train Loss: -0.0027, Test Loss: 0.0372\n",
      "Epoch 27/100, Train Loss: -0.0041, Test Loss: 0.0368\n",
      "Epoch 28/100, Train Loss: -0.0054, Test Loss: 0.0363\n",
      "Epoch 29/100, Train Loss: -0.0065, Test Loss: 0.0360\n",
      "Epoch 30/100, Train Loss: -0.0076, Test Loss: 0.0357\n",
      "Epoch 31/100, Train Loss: -0.0086, Test Loss: 0.0357\n",
      "Epoch 32/100, Train Loss: -0.0096, Test Loss: 0.0355\n",
      "Epoch 33/100, Train Loss: -0.0104, Test Loss: 0.0353\n",
      "Epoch 34/100, Train Loss: -0.0113, Test Loss: 0.0351\n",
      "Epoch 35/100, Train Loss: -0.0120, Test Loss: 0.0349\n",
      "Epoch 36/100, Train Loss: -0.0128, Test Loss: 0.0348\n",
      "Epoch 37/100, Train Loss: -0.0135, Test Loss: 0.0348\n",
      "Epoch 38/100, Train Loss: -0.0142, Test Loss: 0.0347\n",
      "Epoch 39/100, Train Loss: -0.0148, Test Loss: 0.0346\n",
      "Epoch 40/100, Train Loss: -0.0154, Test Loss: 0.0345\n",
      "Epoch 41/100, Train Loss: -0.0160, Test Loss: 0.0345\n",
      "Epoch 42/100, Train Loss: -0.0165, Test Loss: 0.0345\n",
      "Epoch 43/100, Train Loss: -0.0171, Test Loss: 0.0346\n",
      "Epoch 44/100, Train Loss: -0.0176, Test Loss: 0.0346\n",
      "Epoch 45/100, Train Loss: -0.0181, Test Loss: 0.0346\n",
      "Epoch 46/100, Train Loss: -0.0186, Test Loss: 0.0347\n",
      "Epoch 47/100, Train Loss: -0.0191, Test Loss: 0.0348\n",
      "Epoch 48/100, Train Loss: -0.0195, Test Loss: 0.0349\n",
      "Epoch 49/100, Train Loss: -0.0200, Test Loss: 0.0351\n",
      "Epoch 50/100, Train Loss: -0.0204, Test Loss: 0.0352\n",
      "Epoch 51/100, Train Loss: -0.0208, Test Loss: 0.0354\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 1.9755, Test Loss: 0.0198\n",
      "Epoch 2/100, Train Loss: 0.8462, Test Loss: -0.0433\n",
      "Epoch 3/100, Train Loss: 0.4217, Test Loss: -0.0454\n",
      "Epoch 4/100, Train Loss: 0.2316, Test Loss: -0.0488\n",
      "Epoch 5/100, Train Loss: 0.1342, Test Loss: -0.0606\n",
      "Epoch 6/100, Train Loss: 0.0835, Test Loss: -0.0529\n",
      "Epoch 7/100, Train Loss: 0.0555, Test Loss: -0.0573\n",
      "Epoch 8/100, Train Loss: 0.0330, Test Loss: -0.0624\n",
      "Epoch 9/100, Train Loss: 0.0193, Test Loss: -0.0599\n",
      "Epoch 10/100, Train Loss: 0.0118, Test Loss: -0.0593\n",
      "Epoch 11/100, Train Loss: 0.0050, Test Loss: -0.0614\n",
      "Epoch 12/100, Train Loss: -0.0002, Test Loss: -0.0615\n",
      "Epoch 13/100, Train Loss: -0.0039, Test Loss: -0.0607\n",
      "Epoch 14/100, Train Loss: -0.0067, Test Loss: -0.0614\n",
      "Epoch 15/100, Train Loss: -0.0092, Test Loss: -0.0619\n",
      "Epoch 16/100, Train Loss: -0.0113, Test Loss: -0.0619\n",
      "Epoch 17/100, Train Loss: -0.0128, Test Loss: -0.0625\n",
      "Epoch 18/100, Train Loss: -0.0146, Test Loss: -0.0629\n",
      "Epoch 19/100, Train Loss: -0.0159, Test Loss: -0.0633\n",
      "Epoch 20/100, Train Loss: -0.0172, Test Loss: -0.0636\n",
      "Epoch 21/100, Train Loss: -0.0183, Test Loss: -0.0635\n",
      "Epoch 22/100, Train Loss: -0.0191, Test Loss: -0.0628\n",
      "Epoch 23/100, Train Loss: -0.0191, Test Loss: -0.0610\n",
      "Epoch 24/100, Train Loss: -0.0185, Test Loss: -0.0572\n",
      "Epoch 25/100, Train Loss: -0.0155, Test Loss: -0.0574\n",
      "Epoch 26/100, Train Loss: -0.0083, Test Loss: -0.0636\n",
      "Epoch 27/100, Train Loss: -0.0052, Test Loss: -0.0558\n",
      "Epoch 28/100, Train Loss: -0.0202, Test Loss: -0.0515\n",
      "Epoch 29/100, Train Loss: -0.0156, Test Loss: -0.0620\n",
      "Epoch 30/100, Train Loss: -0.0228, Test Loss: -0.0578\n",
      "Epoch 31/100, Train Loss: -0.0200, Test Loss: -0.0595\n",
      "Epoch 32/100, Train Loss: -0.0232, Test Loss: -0.0628\n",
      "Epoch 33/100, Train Loss: -0.0235, Test Loss: -0.0604\n",
      "Epoch 34/100, Train Loss: -0.0235, Test Loss: -0.0627\n",
      "Epoch 35/100, Train Loss: -0.0251, Test Loss: -0.0630\n",
      "Epoch 36/100, Train Loss: -0.0251, Test Loss: -0.0628\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 2.5146, Test Loss: 8.2141\n",
      "Epoch 2/100, Train Loss: 2.7756, Test Loss: 6.6615\n",
      "Epoch 3/100, Train Loss: 1.4193, Test Loss: 6.5925\n",
      "Epoch 4/100, Train Loss: 1.5359, Test Loss: 5.3727\n",
      "Epoch 5/100, Train Loss: 0.9342, Test Loss: 5.4439\n",
      "Epoch 6/100, Train Loss: 0.9508, Test Loss: 4.2856\n",
      "Epoch 7/100, Train Loss: 0.5737, Test Loss: 3.7818\n",
      "Epoch 8/100, Train Loss: 0.6193, Test Loss: 3.2008\n",
      "Epoch 9/100, Train Loss: 0.4011, Test Loss: 2.9110\n",
      "Epoch 10/100, Train Loss: 0.4122, Test Loss: 2.5158\n",
      "Epoch 11/100, Train Loss: 0.2920, Test Loss: 2.1245\n",
      "Epoch 12/100, Train Loss: 0.2763, Test Loss: 1.8895\n",
      "Epoch 13/100, Train Loss: 0.2364, Test Loss: 1.6664\n",
      "Epoch 14/100, Train Loss: 0.2013, Test Loss: 1.5201\n",
      "Epoch 15/100, Train Loss: 0.1859, Test Loss: 1.3385\n",
      "Epoch 16/100, Train Loss: 0.1512, Test Loss: 1.2062\n",
      "Epoch 17/100, Train Loss: 0.1437, Test Loss: 1.0951\n",
      "Epoch 18/100, Train Loss: 0.1242, Test Loss: 0.9981\n",
      "Epoch 19/100, Train Loss: 0.1119, Test Loss: 0.9176\n",
      "Epoch 20/100, Train Loss: 0.0993, Test Loss: 0.8378\n",
      "Epoch 21/100, Train Loss: 0.0852, Test Loss: 0.7713\n",
      "Epoch 22/100, Train Loss: 0.0770, Test Loss: 0.7114\n",
      "Epoch 23/100, Train Loss: 0.0671, Test Loss: 0.6564\n",
      "Epoch 24/100, Train Loss: 0.0580, Test Loss: 0.6087\n",
      "Epoch 25/100, Train Loss: 0.0508, Test Loss: 0.5646\n",
      "Epoch 26/100, Train Loss: 0.0426, Test Loss: 0.5232\n",
      "Epoch 27/100, Train Loss: 0.0358, Test Loss: 0.4940\n",
      "Epoch 28/100, Train Loss: 0.0339, Test Loss: 0.4543\n",
      "Epoch 29/100, Train Loss: 0.0266, Test Loss: 0.4274\n",
      "Epoch 30/100, Train Loss: 0.0209, Test Loss: 0.3996\n",
      "Epoch 31/100, Train Loss: 0.0155, Test Loss: 0.3734\n",
      "Epoch 32/100, Train Loss: 0.0113, Test Loss: 0.3499\n",
      "Epoch 33/100, Train Loss: 0.0070, Test Loss: 0.3292\n",
      "Epoch 34/100, Train Loss: 0.0032, Test Loss: 0.3111\n",
      "Epoch 35/100, Train Loss: -0.0001, Test Loss: 0.2948\n",
      "Epoch 36/100, Train Loss: -0.0028, Test Loss: 0.2787\n",
      "Epoch 37/100, Train Loss: -0.0055, Test Loss: 0.2642\n",
      "Epoch 38/100, Train Loss: -0.0077, Test Loss: 0.2514\n",
      "Epoch 39/100, Train Loss: -0.0102, Test Loss: 0.2396\n",
      "Epoch 40/100, Train Loss: -0.0122, Test Loss: 0.2290\n",
      "Epoch 41/100, Train Loss: -0.0140, Test Loss: 0.2188\n",
      "Epoch 42/100, Train Loss: -0.0157, Test Loss: 0.2092\n",
      "Epoch 43/100, Train Loss: -0.0173, Test Loss: 0.2001\n",
      "Epoch 44/100, Train Loss: -0.0188, Test Loss: 0.1924\n",
      "Epoch 45/100, Train Loss: -0.0200, Test Loss: 0.1847\n",
      "Epoch 46/100, Train Loss: -0.0213, Test Loss: 0.1778\n",
      "Epoch 47/100, Train Loss: -0.0224, Test Loss: 0.1715\n",
      "Epoch 48/100, Train Loss: -0.0235, Test Loss: 0.1655\n",
      "Epoch 49/100, Train Loss: -0.0244, Test Loss: 0.1601\n",
      "Epoch 50/100, Train Loss: -0.0251, Test Loss: 0.1543\n",
      "Epoch 51/100, Train Loss: -0.0259, Test Loss: 0.1488\n",
      "Epoch 52/100, Train Loss: -0.0264, Test Loss: 0.1466\n",
      "Epoch 53/100, Train Loss: -0.0261, Test Loss: 0.1470\n",
      "Epoch 54/100, Train Loss: -0.0236, Test Loss: 0.1349\n",
      "Epoch 55/100, Train Loss: -0.0261, Test Loss: 0.1426\n",
      "Epoch 56/100, Train Loss: -0.0257, Test Loss: 0.1314\n",
      "Epoch 57/100, Train Loss: -0.0282, Test Loss: 0.1265\n",
      "Epoch 58/100, Train Loss: -0.0290, Test Loss: 0.1211\n",
      "Epoch 59/100, Train Loss: -0.0301, Test Loss: 0.1178\n",
      "Epoch 60/100, Train Loss: -0.0308, Test Loss: 0.1142\n",
      "Epoch 61/100, Train Loss: -0.0314, Test Loss: 0.1114\n",
      "Epoch 62/100, Train Loss: -0.0319, Test Loss: 0.1071\n",
      "Epoch 63/100, Train Loss: -0.0324, Test Loss: 0.1049\n",
      "Epoch 64/100, Train Loss: -0.0329, Test Loss: 0.1014\n",
      "Epoch 65/100, Train Loss: -0.0333, Test Loss: 0.0988\n",
      "Epoch 66/100, Train Loss: -0.0338, Test Loss: 0.0957\n",
      "Epoch 67/100, Train Loss: -0.0342, Test Loss: 0.0937\n",
      "Epoch 68/100, Train Loss: -0.0347, Test Loss: 0.0921\n",
      "Epoch 69/100, Train Loss: -0.0349, Test Loss: 0.0894\n",
      "Epoch 70/100, Train Loss: -0.0353, Test Loss: 0.0869\n",
      "Epoch 71/100, Train Loss: -0.0355, Test Loss: 0.0871\n",
      "Epoch 72/100, Train Loss: -0.0346, Test Loss: 0.0850\n",
      "Epoch 73/100, Train Loss: -0.0335, Test Loss: 0.0816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100, Train Loss: -0.0343, Test Loss: 0.0968\n",
      "Epoch 75/100, Train Loss: -0.0273, Test Loss: 0.0827\n",
      "Epoch 76/100, Train Loss: -0.0320, Test Loss: 0.0962\n",
      "Epoch 77/100, Train Loss: -0.0312, Test Loss: 0.0913\n",
      "Epoch 78/100, Train Loss: -0.0346, Test Loss: 0.0834\n",
      "Epoch 79/100, Train Loss: -0.0345, Test Loss: 0.0785\n",
      "Epoch 80/100, Train Loss: -0.0363, Test Loss: 0.0757\n",
      "Epoch 81/100, Train Loss: -0.0367, Test Loss: 0.0758\n",
      "Epoch 82/100, Train Loss: -0.0369, Test Loss: 0.0745\n",
      "Epoch 83/100, Train Loss: -0.0374, Test Loss: 0.0720\n",
      "Epoch 84/100, Train Loss: -0.0378, Test Loss: 0.0698\n",
      "Epoch 85/100, Train Loss: -0.0383, Test Loss: 0.0688\n",
      "Epoch 86/100, Train Loss: -0.0386, Test Loss: 0.0671\n",
      "Epoch 87/100, Train Loss: -0.0389, Test Loss: 0.0652\n",
      "Epoch 88/100, Train Loss: -0.0391, Test Loss: 0.0649\n",
      "Epoch 89/100, Train Loss: -0.0394, Test Loss: 0.0646\n",
      "Epoch 90/100, Train Loss: -0.0396, Test Loss: 0.0646\n",
      "Epoch 91/100, Train Loss: -0.0399, Test Loss: 0.0627\n",
      "Epoch 92/100, Train Loss: -0.0401, Test Loss: 0.0612\n",
      "Epoch 93/100, Train Loss: -0.0404, Test Loss: 0.0606\n",
      "Epoch 94/100, Train Loss: -0.0406, Test Loss: 0.0603\n",
      "Epoch 95/100, Train Loss: -0.0408, Test Loss: 0.0600\n",
      "Epoch 96/100, Train Loss: -0.0411, Test Loss: 0.0588\n",
      "Epoch 97/100, Train Loss: -0.0413, Test Loss: 0.0574\n",
      "Epoch 98/100, Train Loss: -0.0415, Test Loss: 0.0569\n",
      "Epoch 99/100, Train Loss: -0.0415, Test Loss: 0.0569\n",
      "Epoch 100/100, Train Loss: -0.0415, Test Loss: 0.0575\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -973     |\n",
      "| time/              |          |\n",
      "|    fps             | 367      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -932     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 354      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 11       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.59     |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00994  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 49       |\n",
      "|    policy_objective       | 0.00853  |\n",
      "|    std                    | 0.718    |\n",
      "|    value_loss             | 513      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -890     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 352      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 17       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.271    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00938  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 50       |\n",
      "|    policy_objective       | 0.0102   |\n",
      "|    std                    | 0.752    |\n",
      "|    value_loss             | 484      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -905     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 349      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 23       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.551    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00637  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 51       |\n",
      "|    policy_objective       | 0.00708  |\n",
      "|    std                    | 0.754    |\n",
      "|    value_loss             | 580      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -894     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 349      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 29       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.576    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00722  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 52       |\n",
      "|    policy_objective       | 0.0124   |\n",
      "|    std                    | 0.742    |\n",
      "|    value_loss             | 668      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -900     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 344      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 35       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.626    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.0074   |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 53       |\n",
      "|    policy_objective       | 0.00733  |\n",
      "|    std                    | 0.706    |\n",
      "|    value_loss             | 576      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -910     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 344      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 41       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.525    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.0091   |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 54       |\n",
      "|    policy_objective       | 0.0123   |\n",
      "|    std                    | 0.68     |\n",
      "|    value_loss             | 605      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -919     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 345      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 47       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.396    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00634  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 55       |\n",
      "|    policy_objective       | 0.00978  |\n",
      "|    std                    | 0.695    |\n",
      "|    value_loss             | 323      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-981.6393348578364\n",
      "------------------------------\n",
      "round: 7\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: -0.0032, Test Loss: 0.0111\n",
      "Epoch 2/100, Train Loss: -0.0033, Test Loss: -0.0422\n",
      "Epoch 3/100, Train Loss: -0.0257, Test Loss: -0.0247\n",
      "Epoch 4/100, Train Loss: -0.0238, Test Loss: -0.0440\n",
      "Epoch 5/100, Train Loss: -0.0319, Test Loss: -0.0423\n",
      "Epoch 6/100, Train Loss: -0.0302, Test Loss: -0.0440\n",
      "Epoch 7/100, Train Loss: -0.0322, Test Loss: -0.0468\n",
      "Epoch 8/100, Train Loss: -0.0342, Test Loss: -0.0482\n",
      "Epoch 9/100, Train Loss: -0.0346, Test Loss: -0.0476\n",
      "Epoch 10/100, Train Loss: -0.0350, Test Loss: -0.0500\n",
      "Epoch 11/100, Train Loss: -0.0366, Test Loss: -0.0503\n",
      "Epoch 12/100, Train Loss: -0.0371, Test Loss: -0.0511\n",
      "Epoch 13/100, Train Loss: -0.0376, Test Loss: -0.0508\n",
      "Epoch 14/100, Train Loss: -0.0384, Test Loss: -0.0517\n",
      "Epoch 15/100, Train Loss: -0.0391, Test Loss: -0.0521\n",
      "Epoch 16/100, Train Loss: -0.0393, Test Loss: -0.0508\n",
      "Epoch 17/100, Train Loss: -0.0398, Test Loss: -0.0512\n",
      "Epoch 18/100, Train Loss: -0.0399, Test Loss: -0.0507\n",
      "Epoch 19/100, Train Loss: -0.0399, Test Loss: -0.0514\n",
      "Epoch 20/100, Train Loss: -0.0400, Test Loss: -0.0462\n",
      "Epoch 21/100, Train Loss: -0.0403, Test Loss: -0.0510\n",
      "Epoch 22/100, Train Loss: -0.0406, Test Loss: -0.0514\n",
      "Epoch 23/100, Train Loss: -0.0417, Test Loss: -0.0491\n",
      "Epoch 24/100, Train Loss: -0.0412, Test Loss: -0.0495\n",
      "Epoch 25/100, Train Loss: -0.0410, Test Loss: -0.0483\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: -0.0177, Test Loss: -0.0461\n",
      "Epoch 2/100, Train Loss: -0.0195, Test Loss: -0.0523\n",
      "Epoch 3/100, Train Loss: -0.0323, Test Loss: -0.0512\n",
      "Epoch 4/100, Train Loss: -0.0284, Test Loss: -0.0518\n",
      "Epoch 5/100, Train Loss: -0.0323, Test Loss: -0.0574\n",
      "Epoch 6/100, Train Loss: -0.0333, Test Loss: -0.0539\n",
      "Epoch 7/100, Train Loss: -0.0326, Test Loss: -0.0567\n",
      "Epoch 8/100, Train Loss: -0.0346, Test Loss: -0.0582\n",
      "Epoch 9/100, Train Loss: -0.0348, Test Loss: -0.0588\n",
      "Epoch 10/100, Train Loss: -0.0356, Test Loss: -0.0608\n",
      "Epoch 11/100, Train Loss: -0.0364, Test Loss: -0.0622\n",
      "Epoch 12/100, Train Loss: -0.0373, Test Loss: -0.0633\n",
      "Epoch 13/100, Train Loss: -0.0379, Test Loss: -0.0653\n",
      "Epoch 14/100, Train Loss: -0.0390, Test Loss: -0.0667\n",
      "Epoch 15/100, Train Loss: -0.0397, Test Loss: -0.0681\n",
      "Epoch 16/100, Train Loss: -0.0406, Test Loss: -0.0697\n",
      "Epoch 17/100, Train Loss: -0.0413, Test Loss: -0.0709\n",
      "Epoch 18/100, Train Loss: -0.0421, Test Loss: -0.0724\n",
      "Epoch 19/100, Train Loss: -0.0427, Test Loss: -0.0730\n",
      "Epoch 20/100, Train Loss: -0.0435, Test Loss: -0.0746\n",
      "Epoch 21/100, Train Loss: -0.0435, Test Loss: -0.0696\n",
      "Epoch 22/100, Train Loss: -0.0417, Test Loss: -0.0744\n",
      "Epoch 23/100, Train Loss: -0.0439, Test Loss: -0.0725\n",
      "Epoch 24/100, Train Loss: -0.0430, Test Loss: -0.0703\n",
      "Epoch 25/100, Train Loss: -0.0409, Test Loss: -0.0678\n",
      "Epoch 26/100, Train Loss: -0.0382, Test Loss: -0.0702\n",
      "Epoch 27/100, Train Loss: -0.0404, Test Loss: -0.0697\n",
      "Epoch 28/100, Train Loss: -0.0419, Test Loss: -0.0690\n",
      "Epoch 29/100, Train Loss: -0.0428, Test Loss: -0.0689\n",
      "Epoch 30/100, Train Loss: -0.0434, Test Loss: -0.0722\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 0.2801, Test Loss: 2.5608\n",
      "Epoch 2/100, Train Loss: 0.2094, Test Loss: 1.6935\n",
      "Epoch 3/100, Train Loss: 0.0337, Test Loss: 1.3122\n",
      "Epoch 4/100, Train Loss: 0.0633, Test Loss: 0.9183\n",
      "Epoch 5/100, Train Loss: -0.0128, Test Loss: 0.7943\n",
      "Epoch 6/100, Train Loss: 0.0047, Test Loss: 0.6510\n",
      "Epoch 7/100, Train Loss: -0.0227, Test Loss: 0.4950\n",
      "Epoch 8/100, Train Loss: -0.0334, Test Loss: 0.4430\n",
      "Epoch 9/100, Train Loss: -0.0247, Test Loss: 0.3861\n",
      "Epoch 10/100, Train Loss: -0.0348, Test Loss: 0.3429\n",
      "Epoch 11/100, Train Loss: -0.0399, Test Loss: 0.3322\n",
      "Epoch 12/100, Train Loss: -0.0363, Test Loss: 0.3173\n",
      "Epoch 13/100, Train Loss: -0.0376, Test Loss: 0.2924\n",
      "Epoch 14/100, Train Loss: -0.0413, Test Loss: 0.2775\n",
      "Epoch 15/100, Train Loss: -0.0411, Test Loss: 0.2723\n",
      "Epoch 16/100, Train Loss: -0.0405, Test Loss: 0.2698\n",
      "Epoch 17/100, Train Loss: -0.0419, Test Loss: 0.2701\n",
      "Epoch 18/100, Train Loss: -0.0427, Test Loss: 0.2732\n",
      "Epoch 19/100, Train Loss: -0.0427, Test Loss: 0.2754\n",
      "Epoch 20/100, Train Loss: -0.0431, Test Loss: 0.2763\n",
      "Epoch 21/100, Train Loss: -0.0438, Test Loss: 0.2789\n",
      "Epoch 22/100, Train Loss: -0.0441, Test Loss: 0.2838\n",
      "Epoch 23/100, Train Loss: -0.0444, Test Loss: 0.2906\n",
      "Epoch 24/100, Train Loss: -0.0450, Test Loss: 0.2987\n",
      "Epoch 25/100, Train Loss: -0.0453, Test Loss: 0.3067\n",
      "Epoch 26/100, Train Loss: -0.0456, Test Loss: 0.3140\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -911     |\n",
      "| time/              |          |\n",
      "|    fps             | 378      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -908     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 363      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 11       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.413    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00744  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 57       |\n",
      "|    policy_objective       | 0.00819  |\n",
      "|    std                    | 0.72     |\n",
      "|    value_loss             | 607      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -925     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 356      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 17       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.541    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00902  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 58       |\n",
      "|    policy_objective       | 0.0122   |\n",
      "|    std                    | 0.725    |\n",
      "|    value_loss             | 498      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -901     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 354      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 23       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.683    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00624  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 59       |\n",
      "|    policy_objective       | 0.00844  |\n",
      "|    std                    | 0.732    |\n",
      "|    value_loss             | 347      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -912     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 353      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 28       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.596    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00675  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 60       |\n",
      "|    policy_objective       | 0.00895  |\n",
      "|    std                    | 0.72     |\n",
      "|    value_loss             | 634      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -912     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 352      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 34       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.659    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00852  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 61       |\n",
      "|    policy_objective       | 0.0118   |\n",
      "|    std                    | 0.722    |\n",
      "|    value_loss             | 489      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -900     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 352      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 40       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.542    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00855  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 62       |\n",
      "|    policy_objective       | 0.00943  |\n",
      "|    std                    | 0.732    |\n",
      "|    value_loss             | 478      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -902     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 351      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 46       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.613    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00858  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 63       |\n",
      "|    policy_objective       | 0.0136   |\n",
      "|    std                    | 0.763    |\n",
      "|    value_loss             | 452      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-1121.6699736960231\n",
      "------------------------------\n",
      "round: 8\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.2203, Test Loss: 0.1357\n",
      "Epoch 2/100, Train Loss: 0.1288, Test Loss: 0.0940\n",
      "Epoch 3/100, Train Loss: 0.0421, Test Loss: 0.0279\n",
      "Epoch 4/100, Train Loss: 0.0196, Test Loss: -0.0212\n",
      "Epoch 5/100, Train Loss: -0.0021, Test Loss: -0.0310\n",
      "Epoch 6/100, Train Loss: -0.0122, Test Loss: -0.0470\n",
      "Epoch 7/100, Train Loss: -0.0230, Test Loss: -0.0399\n",
      "Epoch 8/100, Train Loss: -0.0201, Test Loss: -0.0346\n",
      "Epoch 9/100, Train Loss: -0.0214, Test Loss: -0.0450\n",
      "Epoch 10/100, Train Loss: -0.0267, Test Loss: -0.0512\n",
      "Epoch 11/100, Train Loss: -0.0281, Test Loss: -0.0529\n",
      "Epoch 12/100, Train Loss: -0.0286, Test Loss: -0.0545\n",
      "Epoch 13/100, Train Loss: -0.0292, Test Loss: -0.0538\n",
      "Epoch 14/100, Train Loss: -0.0288, Test Loss: -0.0531\n",
      "Epoch 15/100, Train Loss: -0.0289, Test Loss: -0.0541\n",
      "Epoch 16/100, Train Loss: -0.0296, Test Loss: -0.0545\n",
      "Epoch 17/100, Train Loss: -0.0298, Test Loss: -0.0543\n",
      "Epoch 18/100, Train Loss: -0.0300, Test Loss: -0.0543\n",
      "Epoch 19/100, Train Loss: -0.0302, Test Loss: -0.0541\n",
      "Epoch 20/100, Train Loss: -0.0303, Test Loss: -0.0542\n",
      "Epoch 21/100, Train Loss: -0.0304, Test Loss: -0.0546\n",
      "Epoch 22/100, Train Loss: -0.0306, Test Loss: -0.0551\n",
      "Epoch 23/100, Train Loss: -0.0307, Test Loss: -0.0554\n",
      "Epoch 24/100, Train Loss: -0.0309, Test Loss: -0.0557\n",
      "Epoch 25/100, Train Loss: -0.0310, Test Loss: -0.0558\n",
      "Epoch 26/100, Train Loss: -0.0311, Test Loss: -0.0560\n",
      "Epoch 27/100, Train Loss: -0.0313, Test Loss: -0.0563\n",
      "Epoch 28/100, Train Loss: -0.0314, Test Loss: -0.0565\n",
      "Epoch 29/100, Train Loss: -0.0315, Test Loss: -0.0567\n",
      "Epoch 30/100, Train Loss: -0.0317, Test Loss: -0.0568\n",
      "Epoch 31/100, Train Loss: -0.0318, Test Loss: -0.0570\n",
      "Epoch 32/100, Train Loss: -0.0319, Test Loss: -0.0573\n",
      "Epoch 33/100, Train Loss: -0.0321, Test Loss: -0.0576\n",
      "Epoch 34/100, Train Loss: -0.0322, Test Loss: -0.0579\n",
      "Epoch 35/100, Train Loss: -0.0323, Test Loss: -0.0582\n",
      "Epoch 36/100, Train Loss: -0.0324, Test Loss: -0.0584\n",
      "Epoch 37/100, Train Loss: -0.0326, Test Loss: -0.0586\n",
      "Epoch 38/100, Train Loss: -0.0327, Test Loss: -0.0588\n",
      "Epoch 39/100, Train Loss: -0.0329, Test Loss: -0.0590\n",
      "Epoch 40/100, Train Loss: -0.0330, Test Loss: -0.0593\n",
      "Epoch 41/100, Train Loss: -0.0331, Test Loss: -0.0596\n",
      "Epoch 42/100, Train Loss: -0.0333, Test Loss: -0.0599\n",
      "Epoch 43/100, Train Loss: -0.0334, Test Loss: -0.0601\n",
      "Epoch 44/100, Train Loss: -0.0336, Test Loss: -0.0603\n",
      "Epoch 45/100, Train Loss: -0.0337, Test Loss: -0.0605\n",
      "Epoch 46/100, Train Loss: -0.0338, Test Loss: -0.0607\n",
      "Epoch 47/100, Train Loss: -0.0339, Test Loss: -0.0609\n",
      "Epoch 48/100, Train Loss: -0.0341, Test Loss: -0.0610\n",
      "Epoch 49/100, Train Loss: -0.0342, Test Loss: -0.0612\n",
      "Epoch 50/100, Train Loss: -0.0343, Test Loss: -0.0615\n",
      "Epoch 51/100, Train Loss: -0.0345, Test Loss: -0.0617\n",
      "Epoch 52/100, Train Loss: -0.0346, Test Loss: -0.0619\n",
      "Epoch 53/100, Train Loss: -0.0348, Test Loss: -0.0621\n",
      "Epoch 54/100, Train Loss: -0.0349, Test Loss: -0.0623\n",
      "Epoch 55/100, Train Loss: -0.0350, Test Loss: -0.0624\n",
      "Epoch 56/100, Train Loss: -0.0351, Test Loss: -0.0625\n",
      "Epoch 57/100, Train Loss: -0.0352, Test Loss: -0.0627\n",
      "Epoch 58/100, Train Loss: -0.0353, Test Loss: -0.0628\n",
      "Epoch 59/100, Train Loss: -0.0354, Test Loss: -0.0630\n",
      "Epoch 60/100, Train Loss: -0.0356, Test Loss: -0.0632\n",
      "Epoch 61/100, Train Loss: -0.0357, Test Loss: -0.0634\n",
      "Epoch 62/100, Train Loss: -0.0358, Test Loss: -0.0635\n",
      "Epoch 63/100, Train Loss: -0.0360, Test Loss: -0.0636\n",
      "Epoch 64/100, Train Loss: -0.0361, Test Loss: -0.0637\n",
      "Epoch 65/100, Train Loss: -0.0362, Test Loss: -0.0638\n",
      "Epoch 66/100, Train Loss: -0.0363, Test Loss: -0.0640\n",
      "Epoch 67/100, Train Loss: -0.0364, Test Loss: -0.0642\n",
      "Epoch 68/100, Train Loss: -0.0366, Test Loss: -0.0644\n",
      "Epoch 69/100, Train Loss: -0.0367, Test Loss: -0.0645\n",
      "Epoch 70/100, Train Loss: -0.0368, Test Loss: -0.0646\n",
      "Epoch 71/100, Train Loss: -0.0369, Test Loss: -0.0647\n",
      "Epoch 72/100, Train Loss: -0.0371, Test Loss: -0.0649\n",
      "Epoch 73/100, Train Loss: -0.0372, Test Loss: -0.0650\n",
      "Epoch 74/100, Train Loss: -0.0373, Test Loss: -0.0651\n",
      "Epoch 75/100, Train Loss: -0.0374, Test Loss: -0.0652\n",
      "Epoch 76/100, Train Loss: -0.0375, Test Loss: -0.0653\n",
      "Epoch 77/100, Train Loss: -0.0376, Test Loss: -0.0653\n",
      "Epoch 78/100, Train Loss: -0.0376, Test Loss: -0.0645\n",
      "Epoch 79/100, Train Loss: -0.0373, Test Loss: -0.0626\n",
      "Epoch 80/100, Train Loss: -0.0365, Test Loss: -0.0609\n",
      "Epoch 81/100, Train Loss: -0.0358, Test Loss: -0.0647\n",
      "Epoch 82/100, Train Loss: -0.0371, Test Loss: -0.0599\n",
      "Epoch 83/100, Train Loss: -0.0332, Test Loss: -0.0595\n",
      "Epoch 84/100, Train Loss: -0.0352, Test Loss: -0.0596\n",
      "Epoch 85/100, Train Loss: -0.0351, Test Loss: -0.0653\n",
      "Epoch 86/100, Train Loss: -0.0367, Test Loss: -0.0646\n",
      "Epoch 87/100, Train Loss: -0.0368, Test Loss: -0.0641\n",
      "Epoch 88/100, Train Loss: -0.0366, Test Loss: -0.0633\n",
      "Epoch 89/100, Train Loss: -0.0367, Test Loss: -0.0635\n",
      "Epoch 90/100, Train Loss: -0.0367, Test Loss: -0.0645\n",
      "Epoch 91/100, Train Loss: -0.0369, Test Loss: -0.0642\n",
      "Epoch 92/100, Train Loss: -0.0370, Test Loss: -0.0651\n",
      "Epoch 93/100, Train Loss: -0.0370, Test Loss: -0.0650\n",
      "Epoch 94/100, Train Loss: -0.0369, Test Loss: -0.0652\n",
      "Epoch 95/100, Train Loss: -0.0368, Test Loss: -0.0653\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 0.6926, Test Loss: -0.0425\n",
      "Epoch 2/100, Train Loss: 0.1236, Test Loss: -0.0602\n",
      "Epoch 3/100, Train Loss: 0.0176, Test Loss: -0.0499\n",
      "Epoch 4/100, Train Loss: -0.0073, Test Loss: -0.0529\n",
      "Epoch 5/100, Train Loss: -0.0193, Test Loss: -0.0533\n",
      "Epoch 6/100, Train Loss: -0.0232, Test Loss: -0.0519\n",
      "Epoch 7/100, Train Loss: -0.0249, Test Loss: -0.0519\n",
      "Epoch 8/100, Train Loss: -0.0263, Test Loss: -0.0524\n",
      "Epoch 9/100, Train Loss: -0.0272, Test Loss: -0.0527\n",
      "Epoch 10/100, Train Loss: -0.0279, Test Loss: -0.0528\n",
      "Epoch 11/100, Train Loss: -0.0284, Test Loss: -0.0532\n",
      "Epoch 12/100, Train Loss: -0.0292, Test Loss: -0.0537\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 0.1437, Test Loss: 0.3213\n",
      "Epoch 2/100, Train Loss: 0.1683, Test Loss: 0.0200\n",
      "Epoch 3/100, Train Loss: 0.0194, Test Loss: 0.0871\n",
      "Epoch 4/100, Train Loss: 0.0286, Test Loss: -0.0096\n",
      "Epoch 5/100, Train Loss: -0.0138, Test Loss: -0.0121\n",
      "Epoch 6/100, Train Loss: -0.0096, Test Loss: -0.0086\n",
      "Epoch 7/100, Train Loss: -0.0150, Test Loss: -0.0331\n",
      "Epoch 8/100, Train Loss: -0.0266, Test Loss: -0.0408\n",
      "Epoch 9/100, Train Loss: -0.0273, Test Loss: -0.0355\n",
      "Epoch 10/100, Train Loss: -0.0250, Test Loss: -0.0366\n",
      "Epoch 11/100, Train Loss: -0.0271, Test Loss: -0.0431\n",
      "Epoch 12/100, Train Loss: -0.0303, Test Loss: -0.0457\n",
      "Epoch 13/100, Train Loss: -0.0308, Test Loss: -0.0440\n",
      "Epoch 14/100, Train Loss: -0.0298, Test Loss: -0.0438\n",
      "Epoch 15/100, Train Loss: -0.0302, Test Loss: -0.0454\n",
      "Epoch 16/100, Train Loss: -0.0312, Test Loss: -0.0465\n",
      "Epoch 17/100, Train Loss: -0.0317, Test Loss: -0.0463\n",
      "Epoch 18/100, Train Loss: -0.0315, Test Loss: -0.0460\n",
      "Epoch 19/100, Train Loss: -0.0316, Test Loss: -0.0464\n",
      "Epoch 20/100, Train Loss: -0.0319, Test Loss: -0.0470\n",
      "Epoch 21/100, Train Loss: -0.0322, Test Loss: -0.0471\n",
      "Epoch 22/100, Train Loss: -0.0323, Test Loss: -0.0472\n",
      "Epoch 23/100, Train Loss: -0.0324, Test Loss: -0.0473\n",
      "Epoch 24/100, Train Loss: -0.0326, Test Loss: -0.0476\n",
      "Epoch 25/100, Train Loss: -0.0328, Test Loss: -0.0477\n",
      "Epoch 26/100, Train Loss: -0.0329, Test Loss: -0.0477\n",
      "Epoch 27/100, Train Loss: -0.0331, Test Loss: -0.0478\n",
      "Epoch 28/100, Train Loss: -0.0333, Test Loss: -0.0479\n",
      "Epoch 29/100, Train Loss: -0.0335, Test Loss: -0.0478\n",
      "Epoch 30/100, Train Loss: -0.0337, Test Loss: -0.0478\n",
      "Epoch 31/100, Train Loss: -0.0339, Test Loss: -0.0478\n",
      "Epoch 32/100, Train Loss: -0.0342, Test Loss: -0.0478\n",
      "Epoch 33/100, Train Loss: -0.0344, Test Loss: -0.0477\n",
      "Epoch 34/100, Train Loss: -0.0347, Test Loss: -0.0476\n",
      "Epoch 35/100, Train Loss: -0.0349, Test Loss: -0.0475\n",
      "Epoch 36/100, Train Loss: -0.0352, Test Loss: -0.0474\n",
      "Epoch 37/100, Train Loss: -0.0355, Test Loss: -0.0473\n",
      "Epoch 38/100, Train Loss: -0.0358, Test Loss: -0.0469\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -911     |\n",
      "| time/              |          |\n",
      "|    fps             | 371      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -887     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 360      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 11       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.67     |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.0063   |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 65       |\n",
      "|    policy_objective       | 0.0114   |\n",
      "|    std                    | 0.72     |\n",
      "|    value_loss             | 428      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -916     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 355      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 17       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.616    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00636  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 66       |\n",
      "|    policy_objective       | 0.0123   |\n",
      "|    std                    | 0.733    |\n",
      "|    value_loss             | 619      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -903     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 352      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 23       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.708    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00729  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 67       |\n",
      "|    policy_objective       | 0.0141   |\n",
      "|    std                    | 0.722    |\n",
      "|    value_loss             | 471      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -900     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 350      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 29       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.795    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00598  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 68       |\n",
      "|    policy_objective       | 0.0074   |\n",
      "|    std                    | 0.732    |\n",
      "|    value_loss             | 520      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -901     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 347      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 35       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.537    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00481  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 69       |\n",
      "|    policy_objective       | 0.00711  |\n",
      "|    std                    | 0.708    |\n",
      "|    value_loss             | 447      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -909     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 345      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 41       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.642    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00475  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 70       |\n",
      "|    policy_objective       | 0.0109   |\n",
      "|    std                    | 0.726    |\n",
      "|    value_loss             | 592      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -925     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 342      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 47       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.727    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00635  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 71       |\n",
      "|    policy_objective       | 0.0104   |\n",
      "|    std                    | 0.739    |\n",
      "|    value_loss             | 396      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-1039.6600154669954\n",
      "------------------------------\n",
      "round: 9\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.0447, Test Loss: -0.0452\n",
      "Epoch 2/100, Train Loss: 0.0058, Test Loss: -0.0405\n",
      "Epoch 3/100, Train Loss: -0.0111, Test Loss: -0.0610\n",
      "Epoch 4/100, Train Loss: -0.0216, Test Loss: -0.0539\n",
      "Epoch 5/100, Train Loss: -0.0230, Test Loss: -0.0593\n",
      "Epoch 6/100, Train Loss: -0.0277, Test Loss: -0.0553\n",
      "Epoch 7/100, Train Loss: -0.0269, Test Loss: -0.0559\n",
      "Epoch 8/100, Train Loss: -0.0287, Test Loss: -0.0575\n",
      "Epoch 9/100, Train Loss: -0.0290, Test Loss: -0.0559\n",
      "Epoch 10/100, Train Loss: -0.0290, Test Loss: -0.0573\n",
      "Epoch 11/100, Train Loss: -0.0298, Test Loss: -0.0563\n",
      "Epoch 12/100, Train Loss: -0.0297, Test Loss: -0.0565\n",
      "Epoch 13/100, Train Loss: -0.0300, Test Loss: -0.0571\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: -0.0128, Test Loss: -0.0448\n",
      "Epoch 2/100, Train Loss: -0.0184, Test Loss: -0.0482\n",
      "Epoch 3/100, Train Loss: -0.0223, Test Loss: -0.0466\n",
      "Epoch 4/100, Train Loss: -0.0238, Test Loss: -0.0477\n",
      "Epoch 5/100, Train Loss: -0.0247, Test Loss: -0.0474\n",
      "Epoch 6/100, Train Loss: -0.0258, Test Loss: -0.0489\n",
      "Epoch 7/100, Train Loss: -0.0266, Test Loss: -0.0503\n",
      "Epoch 8/100, Train Loss: -0.0276, Test Loss: -0.0520\n",
      "Epoch 9/100, Train Loss: -0.0283, Test Loss: -0.0538\n",
      "Epoch 10/100, Train Loss: -0.0294, Test Loss: -0.0550\n",
      "Epoch 11/100, Train Loss: -0.0301, Test Loss: -0.0564\n",
      "Epoch 12/100, Train Loss: -0.0309, Test Loss: -0.0575\n",
      "Epoch 13/100, Train Loss: -0.0318, Test Loss: -0.0584\n",
      "Epoch 14/100, Train Loss: -0.0324, Test Loss: -0.0593\n",
      "Epoch 15/100, Train Loss: -0.0331, Test Loss: -0.0606\n",
      "Epoch 16/100, Train Loss: -0.0338, Test Loss: -0.0616\n",
      "Epoch 17/100, Train Loss: -0.0345, Test Loss: -0.0627\n",
      "Epoch 18/100, Train Loss: -0.0351, Test Loss: -0.0631\n",
      "Epoch 19/100, Train Loss: -0.0356, Test Loss: -0.0642\n",
      "Epoch 20/100, Train Loss: -0.0363, Test Loss: -0.0651\n",
      "Epoch 21/100, Train Loss: -0.0370, Test Loss: -0.0652\n",
      "Epoch 22/100, Train Loss: -0.0372, Test Loss: -0.0663\n",
      "Epoch 23/100, Train Loss: -0.0379, Test Loss: -0.0648\n",
      "Epoch 24/100, Train Loss: -0.0375, Test Loss: -0.0601\n",
      "Epoch 25/100, Train Loss: -0.0347, Test Loss: -0.0595\n",
      "Epoch 26/100, Train Loss: -0.0340, Test Loss: -0.0651\n",
      "Epoch 27/100, Train Loss: -0.0351, Test Loss: -0.0655\n",
      "Epoch 28/100, Train Loss: -0.0367, Test Loss: -0.0649\n",
      "Epoch 29/100, Train Loss: -0.0371, Test Loss: -0.0662\n",
      "Epoch 30/100, Train Loss: -0.0373, Test Loss: -0.0664\n",
      "Epoch 31/100, Train Loss: -0.0379, Test Loss: -0.0677\n",
      "Epoch 32/100, Train Loss: -0.0382, Test Loss: -0.0667\n",
      "Epoch 33/100, Train Loss: -0.0383, Test Loss: -0.0674\n",
      "Epoch 34/100, Train Loss: -0.0384, Test Loss: -0.0703\n",
      "Epoch 35/100, Train Loss: -0.0390, Test Loss: -0.0705\n",
      "Epoch 36/100, Train Loss: -0.0398, Test Loss: -0.0693\n",
      "Epoch 37/100, Train Loss: -0.0399, Test Loss: -0.0697\n",
      "Epoch 38/100, Train Loss: -0.0399, Test Loss: -0.0719\n",
      "Epoch 39/100, Train Loss: -0.0411, Test Loss: -0.0732\n",
      "Epoch 40/100, Train Loss: -0.0419, Test Loss: -0.0725\n",
      "Epoch 41/100, Train Loss: -0.0419, Test Loss: -0.0738\n",
      "Epoch 42/100, Train Loss: -0.0426, Test Loss: -0.0753\n",
      "Epoch 43/100, Train Loss: -0.0432, Test Loss: -0.0750\n",
      "Epoch 44/100, Train Loss: -0.0432, Test Loss: -0.0763\n",
      "Epoch 45/100, Train Loss: -0.0439, Test Loss: -0.0766\n",
      "Epoch 46/100, Train Loss: -0.0442, Test Loss: -0.0770\n",
      "Epoch 47/100, Train Loss: -0.0444, Test Loss: -0.0776\n",
      "Epoch 48/100, Train Loss: -0.0446, Test Loss: -0.0790\n",
      "Epoch 49/100, Train Loss: -0.0455, Test Loss: -0.0792\n",
      "Epoch 50/100, Train Loss: -0.0452, Test Loss: -0.0777\n",
      "Epoch 51/100, Train Loss: -0.0400, Test Loss: -0.0662\n",
      "Epoch 52/100, Train Loss: -0.0421, Test Loss: -0.0711\n",
      "Epoch 53/100, Train Loss: -0.0425, Test Loss: -0.0743\n",
      "Epoch 54/100, Train Loss: -0.0433, Test Loss: -0.0790\n",
      "Epoch 55/100, Train Loss: -0.0449, Test Loss: -0.0743\n",
      "Epoch 56/100, Train Loss: -0.0423, Test Loss: -0.0777\n",
      "Epoch 57/100, Train Loss: -0.0434, Test Loss: -0.0743\n",
      "Epoch 58/100, Train Loss: -0.0437, Test Loss: -0.0763\n",
      "Epoch 59/100, Train Loss: -0.0444, Test Loss: -0.0794\n",
      "Epoch 60/100, Train Loss: -0.0455, Test Loss: -0.0778\n",
      "Epoch 61/100, Train Loss: -0.0452, Test Loss: -0.0790\n",
      "Epoch 62/100, Train Loss: -0.0457, Test Loss: -0.0795\n",
      "Epoch 63/100, Train Loss: -0.0464, Test Loss: -0.0813\n",
      "Epoch 64/100, Train Loss: -0.0468, Test Loss: -0.0795\n",
      "Epoch 65/100, Train Loss: -0.0460, Test Loss: -0.0809\n",
      "Epoch 66/100, Train Loss: -0.0461, Test Loss: -0.0813\n",
      "Epoch 67/100, Train Loss: -0.0468, Test Loss: -0.0827\n",
      "Epoch 68/100, Train Loss: -0.0482, Test Loss: -0.0810\n",
      "Epoch 69/100, Train Loss: -0.0478, Test Loss: -0.0778\n",
      "Epoch 70/100, Train Loss: -0.0460, Test Loss: -0.0809\n",
      "Epoch 71/100, Train Loss: -0.0463, Test Loss: -0.0819\n",
      "Epoch 72/100, Train Loss: -0.0474, Test Loss: -0.0770\n",
      "Epoch 73/100, Train Loss: -0.0459, Test Loss: -0.0839\n",
      "Epoch 74/100, Train Loss: -0.0476, Test Loss: -0.0818\n",
      "Epoch 75/100, Train Loss: -0.0477, Test Loss: -0.0814\n",
      "Epoch 76/100, Train Loss: -0.0476, Test Loss: -0.0820\n",
      "Epoch 77/100, Train Loss: -0.0473, Test Loss: -0.0793\n",
      "Epoch 78/100, Train Loss: -0.0460, Test Loss: -0.0843\n",
      "Epoch 79/100, Train Loss: -0.0471, Test Loss: -0.0841\n",
      "Epoch 80/100, Train Loss: -0.0469, Test Loss: -0.0865\n",
      "Epoch 81/100, Train Loss: -0.0477, Test Loss: -0.0790\n",
      "Epoch 82/100, Train Loss: -0.0472, Test Loss: -0.0704\n",
      "Epoch 83/100, Train Loss: -0.0443, Test Loss: -0.0731\n",
      "Epoch 84/100, Train Loss: -0.0434, Test Loss: -0.0862\n",
      "Epoch 85/100, Train Loss: -0.0475, Test Loss: -0.0758\n",
      "Epoch 86/100, Train Loss: -0.0452, Test Loss: -0.0855\n",
      "Epoch 87/100, Train Loss: -0.0489, Test Loss: -0.0778\n",
      "Epoch 88/100, Train Loss: -0.0483, Test Loss: -0.0858\n",
      "Epoch 89/100, Train Loss: -0.0499, Test Loss: -0.0854\n",
      "Epoch 90/100, Train Loss: -0.0500, Test Loss: -0.0863\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 0.6718, Test Loss: 0.0071\n",
      "Epoch 2/100, Train Loss: 0.1346, Test Loss: -0.0143\n",
      "Epoch 3/100, Train Loss: 0.0404, Test Loss: -0.0226\n",
      "Epoch 4/100, Train Loss: 0.0066, Test Loss: -0.0279\n",
      "Epoch 5/100, Train Loss: -0.0066, Test Loss: -0.0304\n",
      "Epoch 6/100, Train Loss: -0.0137, Test Loss: -0.0315\n",
      "Epoch 7/100, Train Loss: -0.0175, Test Loss: -0.0320\n",
      "Epoch 8/100, Train Loss: -0.0201, Test Loss: -0.0323\n",
      "Epoch 9/100, Train Loss: -0.0219, Test Loss: -0.0326\n",
      "Epoch 10/100, Train Loss: -0.0234, Test Loss: -0.0324\n",
      "Epoch 11/100, Train Loss: -0.0242, Test Loss: -0.0320\n",
      "Epoch 12/100, Train Loss: -0.0251, Test Loss: -0.0312\n",
      "Epoch 13/100, Train Loss: -0.0259, Test Loss: -0.0308\n",
      "Epoch 14/100, Train Loss: -0.0267, Test Loss: -0.0301\n",
      "Epoch 15/100, Train Loss: -0.0274, Test Loss: -0.0288\n",
      "Epoch 16/100, Train Loss: -0.0281, Test Loss: -0.0272\n",
      "Epoch 17/100, Train Loss: -0.0287, Test Loss: -0.0260\n",
      "Epoch 18/100, Train Loss: -0.0295, Test Loss: -0.0242\n",
      "Epoch 19/100, Train Loss: -0.0302, Test Loss: -0.0218\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.03e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 384       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 5         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -984     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 361      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 11       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.688    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00748  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 73       |\n",
      "|    policy_objective       | 0.00952  |\n",
      "|    std                    | 0.748    |\n",
      "|    value_loss             | 704      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -967     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 333      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 18       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.573    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00667  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 74       |\n",
      "|    policy_objective       | 0.0101   |\n",
      "|    std                    | 0.762    |\n",
      "|    value_loss             | 345      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -964     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 327      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 25       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.714    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00862  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 75       |\n",
      "|    policy_objective       | 0.00913  |\n",
      "|    std                    | 0.79     |\n",
      "|    value_loss             | 620      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -975     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 323      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 31       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.712    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.0052   |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 76       |\n",
      "|    policy_objective       | 0.00886  |\n",
      "|    std                    | 0.785    |\n",
      "|    value_loss             | 532      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -957     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 320      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 38       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.728    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00817  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 77       |\n",
      "|    policy_objective       | 0.0137   |\n",
      "|    std                    | 0.77     |\n",
      "|    value_loss             | 311      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -951     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 319      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 44       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.686    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00623  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 78       |\n",
      "|    policy_objective       | 0.0124   |\n",
      "|    std                    | 0.771    |\n",
      "|    value_loss             | 379      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -948     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 317      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 51       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.599    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00551  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 79       |\n",
      "|    policy_objective       | 0.0088   |\n",
      "|    std                    | 0.764    |\n",
      "|    value_loss             | 333      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-1036.7660726503468\n",
      "------------------------------\n",
      "round: 10\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.0031, Test Loss: -0.0527\n",
      "Epoch 2/100, Train Loss: -0.0102, Test Loss: -0.0425\n",
      "Epoch 3/100, Train Loss: -0.0252, Test Loss: -0.0514\n",
      "Epoch 4/100, Train Loss: -0.0251, Test Loss: -0.0454\n",
      "Epoch 5/100, Train Loss: -0.0267, Test Loss: -0.0537\n",
      "Epoch 6/100, Train Loss: -0.0310, Test Loss: -0.0543\n",
      "Epoch 7/100, Train Loss: -0.0309, Test Loss: -0.0532\n",
      "Epoch 8/100, Train Loss: -0.0309, Test Loss: -0.0547\n",
      "Epoch 9/100, Train Loss: -0.0325, Test Loss: -0.0573\n",
      "Epoch 10/100, Train Loss: -0.0337, Test Loss: -0.0581\n",
      "Epoch 11/100, Train Loss: -0.0341, Test Loss: -0.0586\n",
      "Epoch 12/100, Train Loss: -0.0348, Test Loss: -0.0604\n",
      "Epoch 13/100, Train Loss: -0.0358, Test Loss: -0.0612\n",
      "Epoch 14/100, Train Loss: -0.0363, Test Loss: -0.0589\n",
      "Epoch 15/100, Train Loss: -0.0352, Test Loss: -0.0568\n",
      "Epoch 16/100, Train Loss: -0.0341, Test Loss: -0.0607\n",
      "Epoch 17/100, Train Loss: -0.0346, Test Loss: -0.0657\n",
      "Epoch 18/100, Train Loss: -0.0358, Test Loss: -0.0594\n",
      "Epoch 19/100, Train Loss: -0.0353, Test Loss: -0.0666\n",
      "Epoch 20/100, Train Loss: -0.0372, Test Loss: -0.0639\n",
      "Epoch 21/100, Train Loss: -0.0375, Test Loss: -0.0661\n",
      "Epoch 22/100, Train Loss: -0.0380, Test Loss: -0.0666\n",
      "Epoch 23/100, Train Loss: -0.0387, Test Loss: -0.0672\n",
      "Epoch 24/100, Train Loss: -0.0388, Test Loss: -0.0682\n",
      "Epoch 25/100, Train Loss: -0.0389, Test Loss: -0.0689\n",
      "Epoch 26/100, Train Loss: -0.0399, Test Loss: -0.0695\n",
      "Epoch 27/100, Train Loss: -0.0404, Test Loss: -0.0711\n",
      "Epoch 28/100, Train Loss: -0.0410, Test Loss: -0.0721\n",
      "Epoch 29/100, Train Loss: -0.0415, Test Loss: -0.0709\n",
      "Epoch 30/100, Train Loss: -0.0417, Test Loss: -0.0729\n",
      "Epoch 31/100, Train Loss: -0.0404, Test Loss: -0.0533\n",
      "Epoch 32/100, Train Loss: -0.0259, Test Loss: -0.0717\n",
      "Epoch 33/100, Train Loss: -0.0389, Test Loss: -0.0674\n",
      "Epoch 34/100, Train Loss: -0.0388, Test Loss: -0.0709\n",
      "Epoch 35/100, Train Loss: -0.0398, Test Loss: -0.0664\n",
      "Epoch 36/100, Train Loss: -0.0392, Test Loss: -0.0697\n",
      "Epoch 37/100, Train Loss: -0.0410, Test Loss: -0.0719\n",
      "Epoch 38/100, Train Loss: -0.0416, Test Loss: -0.0716\n",
      "Epoch 39/100, Train Loss: -0.0418, Test Loss: -0.0724\n",
      "Epoch 40/100, Train Loss: -0.0423, Test Loss: -0.0740\n",
      "Epoch 41/100, Train Loss: -0.0429, Test Loss: -0.0740\n",
      "Epoch 42/100, Train Loss: -0.0431, Test Loss: -0.0753\n",
      "Epoch 43/100, Train Loss: -0.0436, Test Loss: -0.0757\n",
      "Epoch 44/100, Train Loss: -0.0441, Test Loss: -0.0767\n",
      "Epoch 45/100, Train Loss: -0.0445, Test Loss: -0.0775\n",
      "Epoch 46/100, Train Loss: -0.0450, Test Loss: -0.0779\n",
      "Epoch 47/100, Train Loss: -0.0454, Test Loss: -0.0772\n",
      "Epoch 48/100, Train Loss: -0.0452, Test Loss: -0.0785\n",
      "Epoch 49/100, Train Loss: -0.0455, Test Loss: -0.0795\n",
      "Epoch 50/100, Train Loss: -0.0448, Test Loss: -0.0563\n",
      "Epoch 51/100, Train Loss: -0.0212, Test Loss: -0.0701\n",
      "Epoch 52/100, Train Loss: -0.0382, Test Loss: -0.0783\n",
      "Epoch 53/100, Train Loss: -0.0422, Test Loss: -0.0669\n",
      "Epoch 54/100, Train Loss: -0.0400, Test Loss: -0.0776\n",
      "Epoch 55/100, Train Loss: -0.0425, Test Loss: -0.0730\n",
      "Epoch 56/100, Train Loss: -0.0428, Test Loss: -0.0729\n",
      "Epoch 57/100, Train Loss: -0.0433, Test Loss: -0.0764\n",
      "Epoch 58/100, Train Loss: -0.0441, Test Loss: -0.0766\n",
      "Epoch 59/100, Train Loss: -0.0442, Test Loss: -0.0764\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 0.1582, Test Loss: 0.0400\n",
      "Epoch 2/100, Train Loss: 0.1029, Test Loss: -0.0380\n",
      "Epoch 3/100, Train Loss: 0.0220, Test Loss: -0.0164\n",
      "Epoch 4/100, Train Loss: 0.0084, Test Loss: -0.0391\n",
      "Epoch 5/100, Train Loss: -0.0192, Test Loss: -0.0466\n",
      "Epoch 6/100, Train Loss: -0.0173, Test Loss: -0.0477\n",
      "Epoch 7/100, Train Loss: -0.0266, Test Loss: -0.0572\n",
      "Epoch 8/100, Train Loss: -0.0300, Test Loss: -0.0527\n",
      "Epoch 9/100, Train Loss: -0.0286, Test Loss: -0.0560\n",
      "Epoch 10/100, Train Loss: -0.0328, Test Loss: -0.0624\n",
      "Epoch 11/100, Train Loss: -0.0350, Test Loss: -0.0583\n",
      "Epoch 12/100, Train Loss: -0.0336, Test Loss: -0.0592\n",
      "Epoch 13/100, Train Loss: -0.0351, Test Loss: -0.0627\n",
      "Epoch 14/100, Train Loss: -0.0362, Test Loss: -0.0628\n",
      "Epoch 15/100, Train Loss: -0.0361, Test Loss: -0.0634\n",
      "Epoch 16/100, Train Loss: -0.0366, Test Loss: -0.0647\n",
      "Epoch 17/100, Train Loss: -0.0375, Test Loss: -0.0651\n",
      "Epoch 18/100, Train Loss: -0.0377, Test Loss: -0.0655\n",
      "Epoch 19/100, Train Loss: -0.0381, Test Loss: -0.0665\n",
      "Epoch 20/100, Train Loss: -0.0387, Test Loss: -0.0673\n",
      "Epoch 21/100, Train Loss: -0.0392, Test Loss: -0.0682\n",
      "Epoch 22/100, Train Loss: -0.0398, Test Loss: -0.0692\n",
      "Epoch 23/100, Train Loss: -0.0403, Test Loss: -0.0700\n",
      "Epoch 24/100, Train Loss: -0.0408, Test Loss: -0.0713\n",
      "Epoch 25/100, Train Loss: -0.0414, Test Loss: -0.0724\n",
      "Epoch 26/100, Train Loss: -0.0419, Test Loss: -0.0735\n",
      "Epoch 27/100, Train Loss: -0.0425, Test Loss: -0.0748\n",
      "Epoch 28/100, Train Loss: -0.0431, Test Loss: -0.0760\n",
      "Epoch 29/100, Train Loss: -0.0437, Test Loss: -0.0772\n",
      "Epoch 30/100, Train Loss: -0.0443, Test Loss: -0.0784\n",
      "Epoch 31/100, Train Loss: -0.0448, Test Loss: -0.0796\n",
      "Epoch 32/100, Train Loss: -0.0454, Test Loss: -0.0807\n",
      "Epoch 33/100, Train Loss: -0.0459, Test Loss: -0.0818\n",
      "Epoch 34/100, Train Loss: -0.0464, Test Loss: -0.0826\n",
      "Epoch 35/100, Train Loss: -0.0468, Test Loss: -0.0833\n",
      "Epoch 36/100, Train Loss: -0.0472, Test Loss: -0.0839\n",
      "Epoch 37/100, Train Loss: -0.0473, Test Loss: -0.0845\n",
      "Epoch 38/100, Train Loss: -0.0465, Test Loss: -0.0841\n",
      "Epoch 39/100, Train Loss: -0.0457, Test Loss: -0.0780\n",
      "Epoch 40/100, Train Loss: -0.0434, Test Loss: -0.0661\n",
      "Epoch 41/100, Train Loss: -0.0417, Test Loss: -0.0794\n",
      "Epoch 42/100, Train Loss: -0.0410, Test Loss: -0.0762\n",
      "Epoch 43/100, Train Loss: -0.0400, Test Loss: -0.0760\n",
      "Epoch 44/100, Train Loss: -0.0389, Test Loss: -0.0770\n",
      "Epoch 45/100, Train Loss: -0.0387, Test Loss: -0.0771\n",
      "Epoch 46/100, Train Loss: -0.0423, Test Loss: -0.0762\n",
      "Epoch 47/100, Train Loss: -0.0444, Test Loss: -0.0778\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 0.3566, Test Loss: 0.2146\n",
      "Epoch 2/100, Train Loss: 0.1674, Test Loss: -0.0161\n",
      "Epoch 3/100, Train Loss: 0.0216, Test Loss: 0.0468\n",
      "Epoch 4/100, Train Loss: 0.0385, Test Loss: -0.0050\n",
      "Epoch 5/100, Train Loss: -0.0126, Test Loss: -0.0499\n",
      "Epoch 6/100, Train Loss: -0.0272, Test Loss: -0.0393\n",
      "Epoch 7/100, Train Loss: -0.0163, Test Loss: -0.0309\n",
      "Epoch 8/100, Train Loss: -0.0178, Test Loss: -0.0419\n",
      "Epoch 9/100, Train Loss: -0.0282, Test Loss: -0.0523\n",
      "Epoch 10/100, Train Loss: -0.0332, Test Loss: -0.0542\n",
      "Epoch 11/100, Train Loss: -0.0329, Test Loss: -0.0535\n",
      "Epoch 12/100, Train Loss: -0.0323, Test Loss: -0.0534\n",
      "Epoch 13/100, Train Loss: -0.0331, Test Loss: -0.0554\n",
      "Epoch 14/100, Train Loss: -0.0350, Test Loss: -0.0580\n",
      "Epoch 15/100, Train Loss: -0.0367, Test Loss: -0.0592\n",
      "Epoch 16/100, Train Loss: -0.0372, Test Loss: -0.0599\n",
      "Epoch 17/100, Train Loss: -0.0374, Test Loss: -0.0603\n",
      "Epoch 18/100, Train Loss: -0.0379, Test Loss: -0.0615\n",
      "Epoch 19/100, Train Loss: -0.0387, Test Loss: -0.0623\n",
      "Epoch 20/100, Train Loss: -0.0393, Test Loss: -0.0631\n",
      "Epoch 21/100, Train Loss: -0.0398, Test Loss: -0.0636\n",
      "Epoch 22/100, Train Loss: -0.0402, Test Loss: -0.0638\n",
      "Epoch 23/100, Train Loss: -0.0407, Test Loss: -0.0643\n",
      "Epoch 24/100, Train Loss: -0.0412, Test Loss: -0.0647\n",
      "Epoch 25/100, Train Loss: -0.0418, Test Loss: -0.0652\n",
      "Epoch 26/100, Train Loss: -0.0422, Test Loss: -0.0654\n",
      "Epoch 27/100, Train Loss: -0.0427, Test Loss: -0.0659\n",
      "Epoch 28/100, Train Loss: -0.0432, Test Loss: -0.0659\n",
      "Epoch 29/100, Train Loss: -0.0415, Test Loss: -0.0599\n",
      "Epoch 30/100, Train Loss: -0.0420, Test Loss: -0.0641\n",
      "Epoch 31/100, Train Loss: -0.0436, Test Loss: -0.0653\n",
      "Epoch 32/100, Train Loss: -0.0443, Test Loss: -0.0658\n",
      "Epoch 33/100, Train Loss: -0.0443, Test Loss: -0.0656\n",
      "Epoch 34/100, Train Loss: -0.0430, Test Loss: -0.0616\n",
      "Epoch 35/100, Train Loss: -0.0410, Test Loss: -0.0475\n",
      "Epoch 36/100, Train Loss: -0.0381, Test Loss: -0.0645\n",
      "Epoch 37/100, Train Loss: -0.0420, Test Loss: -0.0587\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -824     |\n",
      "| time/              |          |\n",
      "|    fps             | 352      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -853     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 336      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 12       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.679    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00621  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 81       |\n",
      "|    policy_objective       | 0.00658  |\n",
      "|    std                    | 0.733    |\n",
      "|    value_loss             | 609      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -888     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 333      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 18       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.718    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00802  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 82       |\n",
      "|    policy_objective       | 0.0205   |\n",
      "|    std                    | 0.77     |\n",
      "|    value_loss             | 371      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -894     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 332      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 24       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.76     |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00755  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 83       |\n",
      "|    policy_objective       | 0.0157   |\n",
      "|    std                    | 0.799    |\n",
      "|    value_loss             | 428      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -884     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 333      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 30       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.656    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00725  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 84       |\n",
      "|    policy_objective       | 0.0147   |\n",
      "|    std                    | 0.787    |\n",
      "|    value_loss             | 461      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -881     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 334      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 36       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.708    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00649  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 85       |\n",
      "|    policy_objective       | 0.015    |\n",
      "|    std                    | 0.792    |\n",
      "|    value_loss             | 311      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -893     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 333      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 42       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.528    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00621  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 86       |\n",
      "|    policy_objective       | 0.0104   |\n",
      "|    std                    | 0.789    |\n",
      "|    value_loss             | 475      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -896     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 333      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 49       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.613    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00674  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 87       |\n",
      "|    policy_objective       | 0.00958  |\n",
      "|    std                    | 0.791    |\n",
      "|    value_loss             | 466      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-930.416009156406\n",
      "------------------------------\n",
      "round: 11\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.0993, Test Loss: 0.1461\n",
      "Epoch 2/100, Train Loss: 0.1107, Test Loss: 0.0573\n",
      "Epoch 3/100, Train Loss: -0.0243, Test Loss: 0.0498\n",
      "Epoch 4/100, Train Loss: 0.0174, Test Loss: 0.0222\n",
      "Epoch 5/100, Train Loss: -0.0239, Test Loss: -0.0291\n",
      "Epoch 6/100, Train Loss: -0.0356, Test Loss: -0.0115\n",
      "Epoch 7/100, Train Loss: -0.0272, Test Loss: -0.0270\n",
      "Epoch 8/100, Train Loss: -0.0384, Test Loss: -0.0404\n",
      "Epoch 9/100, Train Loss: -0.0423, Test Loss: -0.0368\n",
      "Epoch 10/100, Train Loss: -0.0396, Test Loss: -0.0372\n",
      "Epoch 11/100, Train Loss: -0.0416, Test Loss: -0.0416\n",
      "Epoch 12/100, Train Loss: -0.0452, Test Loss: -0.0416\n",
      "Epoch 13/100, Train Loss: -0.0455, Test Loss: -0.0378\n",
      "Epoch 14/100, Train Loss: -0.0448, Test Loss: -0.0368\n",
      "Epoch 15/100, Train Loss: -0.0463, Test Loss: -0.0375\n",
      "Epoch 16/100, Train Loss: -0.0477, Test Loss: -0.0356\n",
      "Epoch 17/100, Train Loss: -0.0480, Test Loss: -0.0323\n",
      "Epoch 18/100, Train Loss: -0.0482, Test Loss: -0.0296\n",
      "Epoch 19/100, Train Loss: -0.0491, Test Loss: -0.0268\n",
      "Epoch 20/100, Train Loss: -0.0496, Test Loss: -0.0232\n",
      "Epoch 21/100, Train Loss: -0.0501, Test Loss: -0.0200\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 0.6463, Test Loss: -0.0475\n",
      "Epoch 2/100, Train Loss: 0.3083, Test Loss: 0.0674\n",
      "Epoch 3/100, Train Loss: 0.1542, Test Loss: -0.0124\n",
      "Epoch 4/100, Train Loss: 0.0518, Test Loss: -0.0441\n",
      "Epoch 5/100, Train Loss: 0.0250, Test Loss: -0.0067\n",
      "Epoch 6/100, Train Loss: 0.0107, Test Loss: -0.0246\n",
      "Epoch 7/100, Train Loss: -0.0062, Test Loss: -0.0519\n",
      "Epoch 8/100, Train Loss: -0.0154, Test Loss: -0.0532\n",
      "Epoch 9/100, Train Loss: -0.0186, Test Loss: -0.0523\n",
      "Epoch 10/100, Train Loss: -0.0212, Test Loss: -0.0537\n",
      "Epoch 11/100, Train Loss: -0.0228, Test Loss: -0.0529\n",
      "Epoch 12/100, Train Loss: -0.0238, Test Loss: -0.0523\n",
      "Epoch 13/100, Train Loss: -0.0251, Test Loss: -0.0543\n",
      "Epoch 14/100, Train Loss: -0.0264, Test Loss: -0.0564\n",
      "Epoch 15/100, Train Loss: -0.0274, Test Loss: -0.0576\n",
      "Epoch 16/100, Train Loss: -0.0283, Test Loss: -0.0581\n",
      "Epoch 17/100, Train Loss: -0.0289, Test Loss: -0.0588\n",
      "Epoch 18/100, Train Loss: -0.0297, Test Loss: -0.0595\n",
      "Epoch 19/100, Train Loss: -0.0305, Test Loss: -0.0600\n",
      "Epoch 20/100, Train Loss: -0.0311, Test Loss: -0.0606\n",
      "Epoch 21/100, Train Loss: -0.0318, Test Loss: -0.0614\n",
      "Epoch 22/100, Train Loss: -0.0326, Test Loss: -0.0624\n",
      "Epoch 23/100, Train Loss: -0.0332, Test Loss: -0.0633\n",
      "Epoch 24/100, Train Loss: -0.0339, Test Loss: -0.0641\n",
      "Epoch 25/100, Train Loss: -0.0345, Test Loss: -0.0647\n",
      "Epoch 26/100, Train Loss: -0.0352, Test Loss: -0.0654\n",
      "Epoch 27/100, Train Loss: -0.0359, Test Loss: -0.0663\n",
      "Epoch 28/100, Train Loss: -0.0365, Test Loss: -0.0672\n",
      "Epoch 29/100, Train Loss: -0.0371, Test Loss: -0.0680\n",
      "Epoch 30/100, Train Loss: -0.0377, Test Loss: -0.0688\n",
      "Epoch 31/100, Train Loss: -0.0383, Test Loss: -0.0694\n",
      "Epoch 32/100, Train Loss: -0.0389, Test Loss: -0.0702\n",
      "Epoch 33/100, Train Loss: -0.0395, Test Loss: -0.0710\n",
      "Epoch 34/100, Train Loss: -0.0401, Test Loss: -0.0719\n",
      "Epoch 35/100, Train Loss: -0.0407, Test Loss: -0.0727\n",
      "Epoch 36/100, Train Loss: -0.0413, Test Loss: -0.0734\n",
      "Epoch 37/100, Train Loss: -0.0417, Test Loss: -0.0740\n",
      "Epoch 38/100, Train Loss: -0.0423, Test Loss: -0.0747\n",
      "Epoch 39/100, Train Loss: -0.0428, Test Loss: -0.0752\n",
      "Epoch 40/100, Train Loss: -0.0433, Test Loss: -0.0758\n",
      "Epoch 41/100, Train Loss: -0.0439, Test Loss: -0.0763\n",
      "Epoch 42/100, Train Loss: -0.0444, Test Loss: -0.0768\n",
      "Epoch 43/100, Train Loss: -0.0448, Test Loss: -0.0773\n",
      "Epoch 44/100, Train Loss: -0.0453, Test Loss: -0.0779\n",
      "Epoch 45/100, Train Loss: -0.0457, Test Loss: -0.0784\n",
      "Epoch 46/100, Train Loss: -0.0462, Test Loss: -0.0789\n",
      "Epoch 47/100, Train Loss: -0.0466, Test Loss: -0.0792\n",
      "Epoch 48/100, Train Loss: -0.0471, Test Loss: -0.0797\n",
      "Epoch 49/100, Train Loss: -0.0473, Test Loss: -0.0795\n",
      "Epoch 50/100, Train Loss: -0.0479, Test Loss: -0.0792\n",
      "Epoch 51/100, Train Loss: -0.0433, Test Loss: -0.0787\n",
      "Epoch 52/100, Train Loss: -0.0458, Test Loss: -0.0771\n",
      "Epoch 53/100, Train Loss: -0.0445, Test Loss: -0.0597\n",
      "Epoch 54/100, Train Loss: -0.0365, Test Loss: -0.0748\n",
      "Epoch 55/100, Train Loss: -0.0383, Test Loss: -0.0496\n",
      "Epoch 56/100, Train Loss: -0.0343, Test Loss: -0.0700\n",
      "Epoch 57/100, Train Loss: -0.0408, Test Loss: -0.0772\n",
      "Epoch 58/100, Train Loss: -0.0429, Test Loss: -0.0751\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 0.0342, Test Loss: -0.0450\n",
      "Epoch 2/100, Train Loss: 0.0152, Test Loss: -0.0457\n",
      "Epoch 3/100, Train Loss: -0.0220, Test Loss: -0.0544\n",
      "Epoch 4/100, Train Loss: -0.0201, Test Loss: -0.0440\n",
      "Epoch 5/100, Train Loss: -0.0216, Test Loss: -0.0577\n",
      "Epoch 6/100, Train Loss: -0.0296, Test Loss: -0.0604\n",
      "Epoch 7/100, Train Loss: -0.0308, Test Loss: -0.0565\n",
      "Epoch 8/100, Train Loss: -0.0294, Test Loss: -0.0563\n",
      "Epoch 9/100, Train Loss: -0.0301, Test Loss: -0.0584\n",
      "Epoch 10/100, Train Loss: -0.0313, Test Loss: -0.0588\n",
      "Epoch 11/100, Train Loss: -0.0317, Test Loss: -0.0580\n",
      "Epoch 12/100, Train Loss: -0.0316, Test Loss: -0.0584\n",
      "Epoch 13/100, Train Loss: -0.0320, Test Loss: -0.0594\n",
      "Epoch 14/100, Train Loss: -0.0325, Test Loss: -0.0599\n",
      "Epoch 15/100, Train Loss: -0.0327, Test Loss: -0.0603\n",
      "Epoch 16/100, Train Loss: -0.0330, Test Loss: -0.0611\n",
      "Epoch 17/100, Train Loss: -0.0334, Test Loss: -0.0619\n",
      "Epoch 18/100, Train Loss: -0.0337, Test Loss: -0.0625\n",
      "Epoch 19/100, Train Loss: -0.0340, Test Loss: -0.0633\n",
      "Epoch 20/100, Train Loss: -0.0344, Test Loss: -0.0641\n",
      "Epoch 21/100, Train Loss: -0.0348, Test Loss: -0.0649\n",
      "Epoch 22/100, Train Loss: -0.0352, Test Loss: -0.0658\n",
      "Epoch 23/100, Train Loss: -0.0356, Test Loss: -0.0666\n",
      "Epoch 24/100, Train Loss: -0.0359, Test Loss: -0.0674\n",
      "Epoch 25/100, Train Loss: -0.0363, Test Loss: -0.0683\n",
      "Epoch 26/100, Train Loss: -0.0366, Test Loss: -0.0691\n",
      "Epoch 27/100, Train Loss: -0.0370, Test Loss: -0.0699\n",
      "Epoch 28/100, Train Loss: -0.0375, Test Loss: -0.0706\n",
      "Epoch 29/100, Train Loss: -0.0378, Test Loss: -0.0714\n",
      "Epoch 30/100, Train Loss: -0.0382, Test Loss: -0.0703\n",
      "Epoch 31/100, Train Loss: -0.0343, Test Loss: -0.0656\n",
      "Epoch 32/100, Train Loss: -0.0350, Test Loss: -0.0628\n",
      "Epoch 33/100, Train Loss: -0.0355, Test Loss: -0.0655\n",
      "Epoch 34/100, Train Loss: -0.0358, Test Loss: -0.0723\n",
      "Epoch 35/100, Train Loss: -0.0374, Test Loss: -0.0708\n",
      "Epoch 36/100, Train Loss: -0.0383, Test Loss: -0.0730\n",
      "Epoch 37/100, Train Loss: -0.0384, Test Loss: -0.0735\n",
      "Epoch 38/100, Train Loss: -0.0391, Test Loss: -0.0724\n",
      "Epoch 39/100, Train Loss: -0.0392, Test Loss: -0.0741\n",
      "Epoch 40/100, Train Loss: -0.0398, Test Loss: -0.0751\n",
      "Epoch 41/100, Train Loss: -0.0401, Test Loss: -0.0755\n",
      "Epoch 42/100, Train Loss: -0.0405, Test Loss: -0.0762\n",
      "Epoch 43/100, Train Loss: -0.0408, Test Loss: -0.0767\n",
      "Epoch 44/100, Train Loss: -0.0411, Test Loss: -0.0775\n",
      "Epoch 45/100, Train Loss: -0.0415, Test Loss: -0.0781\n",
      "Epoch 46/100, Train Loss: -0.0419, Test Loss: -0.0784\n",
      "Epoch 47/100, Train Loss: -0.0422, Test Loss: -0.0793\n",
      "Epoch 48/100, Train Loss: -0.0425, Test Loss: -0.0799\n",
      "Epoch 49/100, Train Loss: -0.0427, Test Loss: -0.0805\n",
      "Epoch 50/100, Train Loss: -0.0429, Test Loss: -0.0808\n",
      "Epoch 51/100, Train Loss: -0.0432, Test Loss: -0.0809\n",
      "Epoch 52/100, Train Loss: -0.0435, Test Loss: -0.0811\n",
      "Epoch 53/100, Train Loss: -0.0439, Test Loss: -0.0819\n",
      "Epoch 54/100, Train Loss: -0.0442, Test Loss: -0.0820\n",
      "Epoch 55/100, Train Loss: -0.0441, Test Loss: -0.0814\n",
      "Epoch 56/100, Train Loss: -0.0437, Test Loss: -0.0813\n",
      "Epoch 57/100, Train Loss: -0.0386, Test Loss: -0.0625\n",
      "Epoch 58/100, Train Loss: -0.0409, Test Loss: -0.0779\n",
      "Epoch 59/100, Train Loss: -0.0436, Test Loss: -0.0814\n",
      "Epoch 60/100, Train Loss: -0.0445, Test Loss: -0.0829\n",
      "Epoch 61/100, Train Loss: -0.0429, Test Loss: -0.0836\n",
      "Epoch 62/100, Train Loss: -0.0417, Test Loss: -0.0838\n",
      "Epoch 63/100, Train Loss: -0.0418, Test Loss: -0.0833\n",
      "Epoch 64/100, Train Loss: -0.0407, Test Loss: -0.0706\n",
      "Epoch 65/100, Train Loss: -0.0378, Test Loss: -0.0500\n",
      "Epoch 66/100, Train Loss: -0.0357, Test Loss: -0.0781\n",
      "Epoch 67/100, Train Loss: -0.0384, Test Loss: -0.0610\n",
      "Epoch 68/100, Train Loss: -0.0371, Test Loss: -0.0789\n",
      "Epoch 69/100, Train Loss: -0.0431, Test Loss: -0.0718\n",
      "Epoch 70/100, Train Loss: -0.0418, Test Loss: -0.0789\n",
      "Epoch 71/100, Train Loss: -0.0437, Test Loss: -0.0829\n",
      "Epoch 72/100, Train Loss: -0.0451, Test Loss: -0.0809\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -851     |\n",
      "| time/              |          |\n",
      "|    fps             | 351      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -854     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 346      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 11       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.624    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.0075   |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 89       |\n",
      "|    policy_objective       | 0.0134   |\n",
      "|    std                    | 0.788    |\n",
      "|    value_loss             | 298      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -885     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 342      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 17       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.642    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.0053   |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 90       |\n",
      "|    policy_objective       | 0.00938  |\n",
      "|    std                    | 0.797    |\n",
      "|    value_loss             | 341      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -907     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 319      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 25       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.626    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00701  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 91       |\n",
      "|    policy_objective       | 0.0167   |\n",
      "|    std                    | 0.768    |\n",
      "|    value_loss             | 252      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -900     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 317      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 32       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.641    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00743  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 92       |\n",
      "|    policy_objective       | 0.0187   |\n",
      "|    std                    | 0.788    |\n",
      "|    value_loss             | 353      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -899     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 314      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 39       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.367    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00694  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 93       |\n",
      "|    policy_objective       | 0.0116   |\n",
      "|    std                    | 0.788    |\n",
      "|    value_loss             | 265      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -888     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 311      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 46       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.649    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00659  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 94       |\n",
      "|    policy_objective       | 0.0124   |\n",
      "|    std                    | 0.797    |\n",
      "|    value_loss             | 298      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -880     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 312      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 52       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.482    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00565  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 95       |\n",
      "|    policy_objective       | 0.011    |\n",
      "|    std                    | 0.802    |\n",
      "|    value_loss             | 365      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-870.0306994786952\n",
      "------------------------------\n",
      "round: 12\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.0016, Test Loss: 2.9535\n",
      "Epoch 2/100, Train Loss: 0.0139, Test Loss: 1.9279\n",
      "Epoch 3/100, Train Loss: -0.0386, Test Loss: 1.3649\n",
      "Epoch 4/100, Train Loss: -0.0224, Test Loss: 1.0181\n",
      "Epoch 5/100, Train Loss: -0.0359, Test Loss: 0.7895\n",
      "Epoch 6/100, Train Loss: -0.0416, Test Loss: 0.6790\n",
      "Epoch 7/100, Train Loss: -0.0372, Test Loss: 0.6045\n",
      "Epoch 8/100, Train Loss: -0.0405, Test Loss: 0.5518\n",
      "Epoch 9/100, Train Loss: -0.0432, Test Loss: 0.5313\n",
      "Epoch 10/100, Train Loss: -0.0420, Test Loss: 0.5331\n",
      "Epoch 11/100, Train Loss: -0.0432, Test Loss: 0.5501\n",
      "Epoch 12/100, Train Loss: -0.0449, Test Loss: 0.5826\n",
      "Epoch 13/100, Train Loss: -0.0449, Test Loss: 0.6140\n",
      "Epoch 14/100, Train Loss: -0.0460, Test Loss: 0.6476\n",
      "Epoch 15/100, Train Loss: -0.0473, Test Loss: 0.7161\n",
      "Epoch 16/100, Train Loss: -0.0480, Test Loss: 0.8177\n",
      "Epoch 17/100, Train Loss: -0.0492, Test Loss: 0.9434\n",
      "Epoch 18/100, Train Loss: -0.0505, Test Loss: 1.0841\n",
      "Epoch 19/100, Train Loss: -0.0518, Test Loss: 1.2680\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 0.0086, Test Loss: -0.0211\n",
      "Epoch 2/100, Train Loss: -0.0199, Test Loss: -0.0446\n",
      "Epoch 3/100, Train Loss: -0.0230, Test Loss: -0.0622\n",
      "Epoch 4/100, Train Loss: -0.0405, Test Loss: -0.0638\n",
      "Epoch 5/100, Train Loss: -0.0384, Test Loss: -0.0643\n",
      "Epoch 6/100, Train Loss: -0.0406, Test Loss: -0.0615\n",
      "Epoch 7/100, Train Loss: -0.0425, Test Loss: -0.0709\n",
      "Epoch 8/100, Train Loss: -0.0439, Test Loss: -0.0679\n",
      "Epoch 9/100, Train Loss: -0.0435, Test Loss: -0.0703\n",
      "Epoch 10/100, Train Loss: -0.0446, Test Loss: -0.0714\n",
      "Epoch 11/100, Train Loss: -0.0460, Test Loss: -0.0726\n",
      "Epoch 12/100, Train Loss: -0.0461, Test Loss: -0.0742\n",
      "Epoch 13/100, Train Loss: -0.0472, Test Loss: -0.0749\n",
      "Epoch 14/100, Train Loss: -0.0480, Test Loss: -0.0768\n",
      "Epoch 15/100, Train Loss: -0.0488, Test Loss: -0.0781\n",
      "Epoch 16/100, Train Loss: -0.0498, Test Loss: -0.0791\n",
      "Epoch 17/100, Train Loss: -0.0505, Test Loss: -0.0806\n",
      "Epoch 18/100, Train Loss: -0.0512, Test Loss: -0.0816\n",
      "Epoch 19/100, Train Loss: -0.0518, Test Loss: -0.0831\n",
      "Epoch 20/100, Train Loss: -0.0523, Test Loss: -0.0821\n",
      "Epoch 21/100, Train Loss: -0.0529, Test Loss: -0.0845\n",
      "Epoch 22/100, Train Loss: -0.0535, Test Loss: -0.0815\n",
      "Epoch 23/100, Train Loss: -0.0523, Test Loss: -0.0800\n",
      "Epoch 24/100, Train Loss: -0.0522, Test Loss: -0.0782\n",
      "Epoch 25/100, Train Loss: -0.0501, Test Loss: -0.0796\n",
      "Epoch 26/100, Train Loss: -0.0479, Test Loss: -0.0591\n",
      "Epoch 27/100, Train Loss: -0.0458, Test Loss: -0.0626\n",
      "Epoch 28/100, Train Loss: -0.0458, Test Loss: -0.0706\n",
      "Epoch 29/100, Train Loss: -0.0481, Test Loss: -0.0793\n",
      "Epoch 30/100, Train Loss: -0.0508, Test Loss: -0.0829\n",
      "Epoch 31/100, Train Loss: -0.0524, Test Loss: -0.0822\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 1.3570, Test Loss: -0.0285\n",
      "Epoch 2/100, Train Loss: 0.5790, Test Loss: -0.0510\n",
      "Epoch 3/100, Train Loss: 0.2338, Test Loss: -0.0526\n",
      "Epoch 4/100, Train Loss: 0.0914, Test Loss: -0.0587\n",
      "Epoch 5/100, Train Loss: 0.0326, Test Loss: -0.0562\n",
      "Epoch 6/100, Train Loss: 0.0078, Test Loss: -0.0590\n",
      "Epoch 7/100, Train Loss: -0.0066, Test Loss: -0.0581\n",
      "Epoch 8/100, Train Loss: -0.0135, Test Loss: -0.0581\n",
      "Epoch 9/100, Train Loss: -0.0185, Test Loss: -0.0572\n",
      "Epoch 10/100, Train Loss: -0.0210, Test Loss: -0.0572\n",
      "Epoch 11/100, Train Loss: -0.0229, Test Loss: -0.0574\n",
      "Epoch 12/100, Train Loss: -0.0242, Test Loss: -0.0570\n",
      "Epoch 13/100, Train Loss: -0.0253, Test Loss: -0.0573\n",
      "Epoch 14/100, Train Loss: -0.0262, Test Loss: -0.0573\n",
      "Epoch 15/100, Train Loss: -0.0268, Test Loss: -0.0576\n",
      "Epoch 16/100, Train Loss: -0.0275, Test Loss: -0.0579\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -806     |\n",
      "| time/              |          |\n",
      "|    fps             | 354      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -864     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 325      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 12       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.612    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00728  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 97       |\n",
      "|    policy_objective       | 0.0131   |\n",
      "|    std                    | 0.782    |\n",
      "|    value_loss             | 317      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -910     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 318      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 19       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.556    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00469  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 98       |\n",
      "|    policy_objective       | 0.0106   |\n",
      "|    std                    | 0.771    |\n",
      "|    value_loss             | 270      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -916     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 310      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 26       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.753    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00723  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 99       |\n",
      "|    policy_objective       | 0.0136   |\n",
      "|    std                    | 0.765    |\n",
      "|    value_loss             | 311      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -911     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 302      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 33       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.798    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00436  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 100      |\n",
      "|    policy_objective       | 0.0113   |\n",
      "|    std                    | 0.75     |\n",
      "|    value_loss             | 333      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -914     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 298      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 41       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.652    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00587  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 101      |\n",
      "|    policy_objective       | 0.017    |\n",
      "|    std                    | 0.758    |\n",
      "|    value_loss             | 340      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -912     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 289      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 49       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.783    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00864  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 102      |\n",
      "|    policy_objective       | 0.0137   |\n",
      "|    std                    | 0.755    |\n",
      "|    value_loss             | 437      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -904     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 286      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 57       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.82     |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00731  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 103      |\n",
      "|    policy_objective       | 0.0103   |\n",
      "|    std                    | 0.764    |\n",
      "|    value_loss             | 277      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-840.4977051302791\n",
      "------------------------------\n",
      "round: 13\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.0305, Test Loss: -0.0671\n",
      "Epoch 2/100, Train Loss: -0.0058, Test Loss: -0.0479\n",
      "Epoch 3/100, Train Loss: -0.0198, Test Loss: -0.0555\n",
      "Epoch 4/100, Train Loss: -0.0245, Test Loss: -0.0642\n",
      "Epoch 5/100, Train Loss: -0.0297, Test Loss: -0.0598\n",
      "Epoch 6/100, Train Loss: -0.0306, Test Loss: -0.0641\n",
      "Epoch 7/100, Train Loss: -0.0335, Test Loss: -0.0619\n",
      "Epoch 8/100, Train Loss: -0.0334, Test Loss: -0.0630\n",
      "Epoch 9/100, Train Loss: -0.0346, Test Loss: -0.0627\n",
      "Epoch 10/100, Train Loss: -0.0349, Test Loss: -0.0633\n",
      "Epoch 11/100, Train Loss: -0.0358, Test Loss: -0.0638\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 0.0115, Test Loss: 2.5519\n",
      "Epoch 2/100, Train Loss: 0.0230, Test Loss: 1.4228\n",
      "Epoch 3/100, Train Loss: -0.0238, Test Loss: 0.9440\n",
      "Epoch 4/100, Train Loss: -0.0267, Test Loss: 0.6942\n",
      "Epoch 5/100, Train Loss: -0.0329, Test Loss: 0.5449\n",
      "Epoch 6/100, Train Loss: -0.0347, Test Loss: 0.4197\n",
      "Epoch 7/100, Train Loss: -0.0427, Test Loss: 0.3718\n",
      "Epoch 8/100, Train Loss: -0.0399, Test Loss: 0.3441\n",
      "Epoch 9/100, Train Loss: -0.0438, Test Loss: 0.3398\n",
      "Epoch 10/100, Train Loss: -0.0437, Test Loss: 0.3396\n",
      "Epoch 11/100, Train Loss: -0.0454, Test Loss: 0.3379\n",
      "Epoch 12/100, Train Loss: -0.0464, Test Loss: 0.3536\n",
      "Epoch 13/100, Train Loss: -0.0471, Test Loss: 0.3812\n",
      "Epoch 14/100, Train Loss: -0.0487, Test Loss: 0.4371\n",
      "Epoch 15/100, Train Loss: -0.0492, Test Loss: 0.5159\n",
      "Epoch 16/100, Train Loss: -0.0507, Test Loss: 0.6440\n",
      "Epoch 17/100, Train Loss: -0.0518, Test Loss: 0.8560\n",
      "Epoch 18/100, Train Loss: -0.0532, Test Loss: 1.0899\n",
      "Epoch 19/100, Train Loss: -0.0544, Test Loss: 1.4619\n",
      "Epoch 20/100, Train Loss: -0.0558, Test Loss: 1.9204\n",
      "Epoch 21/100, Train Loss: -0.0571, Test Loss: 2.7128\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: -0.0206, Test Loss: -0.0266\n",
      "Epoch 2/100, Train Loss: -0.0203, Test Loss: -0.0497\n",
      "Epoch 3/100, Train Loss: -0.0241, Test Loss: -0.0500\n",
      "Epoch 4/100, Train Loss: -0.0287, Test Loss: -0.0517\n",
      "Epoch 5/100, Train Loss: -0.0272, Test Loss: -0.0541\n",
      "Epoch 6/100, Train Loss: -0.0299, Test Loss: -0.0579\n",
      "Epoch 7/100, Train Loss: -0.0305, Test Loss: -0.0569\n",
      "Epoch 8/100, Train Loss: -0.0312, Test Loss: -0.0599\n",
      "Epoch 9/100, Train Loss: -0.0321, Test Loss: -0.0610\n",
      "Epoch 10/100, Train Loss: -0.0333, Test Loss: -0.0638\n",
      "Epoch 11/100, Train Loss: -0.0339, Test Loss: -0.0651\n",
      "Epoch 12/100, Train Loss: -0.0349, Test Loss: -0.0674\n",
      "Epoch 13/100, Train Loss: -0.0357, Test Loss: -0.0694\n",
      "Epoch 14/100, Train Loss: -0.0367, Test Loss: -0.0708\n",
      "Epoch 15/100, Train Loss: -0.0375, Test Loss: -0.0711\n",
      "Epoch 16/100, Train Loss: -0.0382, Test Loss: -0.0735\n",
      "Epoch 17/100, Train Loss: -0.0385, Test Loss: -0.0711\n",
      "Epoch 18/100, Train Loss: -0.0392, Test Loss: -0.0735\n",
      "Epoch 19/100, Train Loss: -0.0392, Test Loss: -0.0675\n",
      "Epoch 20/100, Train Loss: -0.0387, Test Loss: -0.0678\n",
      "Epoch 21/100, Train Loss: -0.0385, Test Loss: -0.0719\n",
      "Epoch 22/100, Train Loss: -0.0397, Test Loss: -0.0754\n",
      "Epoch 23/100, Train Loss: -0.0412, Test Loss: -0.0755\n",
      "Epoch 24/100, Train Loss: -0.0411, Test Loss: -0.0731\n",
      "Epoch 25/100, Train Loss: -0.0411, Test Loss: -0.0744\n",
      "Epoch 26/100, Train Loss: -0.0418, Test Loss: -0.0784\n",
      "Epoch 27/100, Train Loss: -0.0427, Test Loss: -0.0809\n",
      "Epoch 28/100, Train Loss: -0.0438, Test Loss: -0.0774\n",
      "Epoch 29/100, Train Loss: -0.0435, Test Loss: -0.0756\n",
      "Epoch 30/100, Train Loss: -0.0428, Test Loss: -0.0804\n",
      "Epoch 31/100, Train Loss: -0.0436, Test Loss: -0.0806\n",
      "Epoch 32/100, Train Loss: -0.0442, Test Loss: -0.0706\n",
      "Epoch 33/100, Train Loss: -0.0420, Test Loss: -0.0790\n",
      "Epoch 34/100, Train Loss: -0.0440, Test Loss: -0.0710\n",
      "Epoch 35/100, Train Loss: -0.0424, Test Loss: -0.0746\n",
      "Epoch 36/100, Train Loss: -0.0419, Test Loss: -0.0635\n",
      "Epoch 37/100, Train Loss: -0.0405, Test Loss: -0.0699\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -777     |\n",
      "| time/              |          |\n",
      "|    fps             | 298      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -826     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 284      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 14       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.627    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00775  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 105      |\n",
      "|    policy_objective       | 0.0156   |\n",
      "|    std                    | 0.752    |\n",
      "|    value_loss             | 286      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -825     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 278      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 22       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.594    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00741  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 106      |\n",
      "|    policy_objective       | 0.0181   |\n",
      "|    std                    | 0.749    |\n",
      "|    value_loss             | 305      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -851     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 279      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 29       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.587    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00819  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 107      |\n",
      "|    policy_objective       | 0.0143   |\n",
      "|    std                    | 0.747    |\n",
      "|    value_loss             | 303      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -844     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 274      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 37       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.494    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.007    |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 108      |\n",
      "|    policy_objective       | 0.0119   |\n",
      "|    std                    | 0.703    |\n",
      "|    value_loss             | 375      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -851     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 265      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 46       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.643    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00632  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 109      |\n",
      "|    policy_objective       | 0.0152   |\n",
      "|    std                    | 0.699    |\n",
      "|    value_loss             | 603      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -853     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 263      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 54       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.532    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00798  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 110      |\n",
      "|    policy_objective       | 0.0171   |\n",
      "|    std                    | 0.707    |\n",
      "|    value_loss             | 326      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -856     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 260      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 62       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.675    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00823  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 111      |\n",
      "|    policy_objective       | 0.0164   |\n",
      "|    std                    | 0.7      |\n",
      "|    value_loss             | 365      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-754.8268289189785\n",
      "------------------------------\n",
      "round: 14\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: -0.0007, Test Loss: -0.0451\n",
      "Epoch 2/100, Train Loss: -0.0107, Test Loss: -0.0535\n",
      "Epoch 3/100, Train Loss: -0.0241, Test Loss: -0.0475\n",
      "Epoch 4/100, Train Loss: -0.0238, Test Loss: -0.0481\n",
      "Epoch 5/100, Train Loss: -0.0274, Test Loss: -0.0507\n",
      "Epoch 6/100, Train Loss: -0.0282, Test Loss: -0.0493\n",
      "Epoch 7/100, Train Loss: -0.0286, Test Loss: -0.0504\n",
      "Epoch 8/100, Train Loss: -0.0298, Test Loss: -0.0513\n",
      "Epoch 9/100, Train Loss: -0.0303, Test Loss: -0.0520\n",
      "Epoch 10/100, Train Loss: -0.0313, Test Loss: -0.0534\n",
      "Epoch 11/100, Train Loss: -0.0321, Test Loss: -0.0551\n",
      "Epoch 12/100, Train Loss: -0.0332, Test Loss: -0.0569\n",
      "Epoch 13/100, Train Loss: -0.0342, Test Loss: -0.0589\n",
      "Epoch 14/100, Train Loss: -0.0356, Test Loss: -0.0611\n",
      "Epoch 15/100, Train Loss: -0.0367, Test Loss: -0.0627\n",
      "Epoch 16/100, Train Loss: -0.0376, Test Loss: -0.0625\n",
      "Epoch 17/100, Train Loss: -0.0370, Test Loss: -0.0649\n",
      "Epoch 18/100, Train Loss: -0.0370, Test Loss: -0.0638\n",
      "Epoch 19/100, Train Loss: -0.0390, Test Loss: -0.0687\n",
      "Epoch 20/100, Train Loss: -0.0402, Test Loss: -0.0667\n",
      "Epoch 21/100, Train Loss: -0.0398, Test Loss: -0.0709\n",
      "Epoch 22/100, Train Loss: -0.0414, Test Loss: -0.0696\n",
      "Epoch 23/100, Train Loss: -0.0421, Test Loss: -0.0726\n",
      "Epoch 24/100, Train Loss: -0.0429, Test Loss: -0.0720\n",
      "Epoch 25/100, Train Loss: -0.0428, Test Loss: -0.0735\n",
      "Epoch 26/100, Train Loss: -0.0428, Test Loss: -0.0736\n",
      "Epoch 27/100, Train Loss: -0.0434, Test Loss: -0.0688\n",
      "Epoch 28/100, Train Loss: -0.0424, Test Loss: -0.0663\n",
      "Epoch 29/100, Train Loss: -0.0399, Test Loss: -0.0655\n",
      "Epoch 30/100, Train Loss: -0.0374, Test Loss: -0.0741\n",
      "Epoch 31/100, Train Loss: -0.0414, Test Loss: -0.0629\n",
      "Epoch 32/100, Train Loss: -0.0393, Test Loss: -0.0681\n",
      "Epoch 33/100, Train Loss: -0.0424, Test Loss: -0.0715\n",
      "Epoch 34/100, Train Loss: -0.0439, Test Loss: -0.0743\n",
      "Epoch 35/100, Train Loss: -0.0448, Test Loss: -0.0750\n",
      "Epoch 36/100, Train Loss: -0.0453, Test Loss: -0.0759\n",
      "Epoch 37/100, Train Loss: -0.0458, Test Loss: -0.0770\n",
      "Epoch 38/100, Train Loss: -0.0464, Test Loss: -0.0781\n",
      "Epoch 39/100, Train Loss: -0.0467, Test Loss: -0.0791\n",
      "Epoch 40/100, Train Loss: -0.0473, Test Loss: -0.0795\n",
      "Epoch 41/100, Train Loss: -0.0477, Test Loss: -0.0806\n",
      "Epoch 42/100, Train Loss: -0.0483, Test Loss: -0.0818\n",
      "Epoch 43/100, Train Loss: -0.0489, Test Loss: -0.0814\n",
      "Epoch 44/100, Train Loss: -0.0492, Test Loss: -0.0795\n",
      "Epoch 45/100, Train Loss: -0.0484, Test Loss: -0.0655\n",
      "Epoch 46/100, Train Loss: -0.0364, Test Loss: -0.0575\n",
      "Epoch 47/100, Train Loss: -0.0345, Test Loss: -0.0771\n",
      "Epoch 48/100, Train Loss: -0.0390, Test Loss: -0.0617\n",
      "Epoch 49/100, Train Loss: -0.0404, Test Loss: -0.0721\n",
      "Epoch 50/100, Train Loss: -0.0415, Test Loss: -0.0768\n",
      "Epoch 51/100, Train Loss: -0.0446, Test Loss: -0.0758\n",
      "Epoch 52/100, Train Loss: -0.0461, Test Loss: -0.0801\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 1.0849, Test Loss: 0.5513\n",
      "Epoch 2/100, Train Loss: 0.3106, Test Loss: 0.2609\n",
      "Epoch 3/100, Train Loss: 0.0952, Test Loss: 0.1273\n",
      "Epoch 4/100, Train Loss: 0.0275, Test Loss: 0.0445\n",
      "Epoch 5/100, Train Loss: -0.0068, Test Loss: 0.0255\n",
      "Epoch 6/100, Train Loss: -0.0132, Test Loss: -0.0023\n",
      "Epoch 7/100, Train Loss: -0.0266, Test Loss: -0.0104\n",
      "Epoch 8/100, Train Loss: -0.0276, Test Loss: -0.0173\n",
      "Epoch 9/100, Train Loss: -0.0315, Test Loss: -0.0218\n",
      "Epoch 10/100, Train Loss: -0.0324, Test Loss: -0.0241\n",
      "Epoch 11/100, Train Loss: -0.0342, Test Loss: -0.0285\n",
      "Epoch 12/100, Train Loss: -0.0357, Test Loss: -0.0288\n",
      "Epoch 13/100, Train Loss: -0.0358, Test Loss: -0.0303\n",
      "Epoch 14/100, Train Loss: -0.0369, Test Loss: -0.0300\n",
      "Epoch 15/100, Train Loss: -0.0372, Test Loss: -0.0307\n",
      "Epoch 16/100, Train Loss: -0.0381, Test Loss: -0.0304\n",
      "Epoch 17/100, Train Loss: -0.0384, Test Loss: -0.0298\n",
      "Epoch 18/100, Train Loss: -0.0390, Test Loss: -0.0291\n",
      "Epoch 19/100, Train Loss: -0.0395, Test Loss: -0.0288\n",
      "Epoch 20/100, Train Loss: -0.0401, Test Loss: -0.0277\n",
      "Epoch 21/100, Train Loss: -0.0406, Test Loss: -0.0262\n",
      "Epoch 22/100, Train Loss: -0.0412, Test Loss: -0.0246\n",
      "Epoch 23/100, Train Loss: -0.0417, Test Loss: -0.0230\n",
      "Epoch 24/100, Train Loss: -0.0422, Test Loss: -0.0208\n",
      "Epoch 25/100, Train Loss: -0.0428, Test Loss: -0.0179\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 0.0297, Test Loss: 0.0624\n",
      "Epoch 2/100, Train Loss: 0.0332, Test Loss: -0.0333\n",
      "Epoch 3/100, Train Loss: -0.0270, Test Loss: 0.0383\n",
      "Epoch 4/100, Train Loss: -0.0087, Test Loss: -0.0378\n",
      "Epoch 5/100, Train Loss: -0.0363, Test Loss: -0.0470\n",
      "Epoch 6/100, Train Loss: -0.0285, Test Loss: -0.0357\n",
      "Epoch 7/100, Train Loss: -0.0324, Test Loss: -0.0622\n",
      "Epoch 8/100, Train Loss: -0.0396, Test Loss: -0.0539\n",
      "Epoch 9/100, Train Loss: -0.0358, Test Loss: -0.0521\n",
      "Epoch 10/100, Train Loss: -0.0376, Test Loss: -0.0626\n",
      "Epoch 11/100, Train Loss: -0.0404, Test Loss: -0.0611\n",
      "Epoch 12/100, Train Loss: -0.0394, Test Loss: -0.0602\n",
      "Epoch 13/100, Train Loss: -0.0402, Test Loss: -0.0640\n",
      "Epoch 14/100, Train Loss: -0.0418, Test Loss: -0.0632\n",
      "Epoch 15/100, Train Loss: -0.0418, Test Loss: -0.0623\n",
      "Epoch 16/100, Train Loss: -0.0424, Test Loss: -0.0642\n",
      "Epoch 17/100, Train Loss: -0.0435, Test Loss: -0.0644\n",
      "Epoch 18/100, Train Loss: -0.0439, Test Loss: -0.0639\n",
      "Epoch 19/100, Train Loss: -0.0446, Test Loss: -0.0636\n",
      "Epoch 20/100, Train Loss: -0.0454, Test Loss: -0.0624\n",
      "Epoch 21/100, Train Loss: -0.0460, Test Loss: -0.0613\n",
      "Epoch 22/100, Train Loss: -0.0468, Test Loss: -0.0602\n",
      "Epoch 23/100, Train Loss: -0.0474, Test Loss: -0.0584\n",
      "Epoch 24/100, Train Loss: -0.0479, Test Loss: -0.0552\n",
      "Epoch 25/100, Train Loss: -0.0475, Test Loss: -0.0530\n",
      "Epoch 26/100, Train Loss: -0.0482, Test Loss: -0.0481\n",
      "Epoch 27/100, Train Loss: -0.0479, Test Loss: -0.0465\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -924     |\n",
      "| time/              |          |\n",
      "|    fps             | 258      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -849     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 253      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 16       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.61     |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00889  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 113      |\n",
      "|    policy_objective       | 0.0116   |\n",
      "|    std                    | 0.697    |\n",
      "|    value_loss             | 484      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -858     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 257      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 23       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.676    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00631  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 114      |\n",
      "|    policy_objective       | 0.0117   |\n",
      "|    std                    | 0.67     |\n",
      "|    value_loss             | 348      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -864     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 251      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 32       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.703    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00718  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 115      |\n",
      "|    policy_objective       | 0.0131   |\n",
      "|    std                    | 0.665    |\n",
      "|    value_loss             | 499      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -846     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 254      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 40       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.641    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00862  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 116      |\n",
      "|    policy_objective       | 0.014    |\n",
      "|    std                    | 0.666    |\n",
      "|    value_loss             | 342      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -861     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 250      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 49       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.563    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00831  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 117      |\n",
      "|    policy_objective       | 0.0137   |\n",
      "|    std                    | 0.657    |\n",
      "|    value_loss             | 341      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -842     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 254      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 56       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.653    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00715  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 118      |\n",
      "|    policy_objective       | 0.0129   |\n",
      "|    std                    | 0.659    |\n",
      "|    value_loss             | 471      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -840     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 254      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 64       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.564    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00742  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 119      |\n",
      "|    policy_objective       | 0.0119   |\n",
      "|    std                    | 0.672    |\n",
      "|    value_loss             | 418      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-751.6086850993336\n",
      "------------------------------\n",
      "round: 15\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.1036, Test Loss: 0.7968\n",
      "Epoch 2/100, Train Loss: 0.0184, Test Loss: 0.3812\n",
      "Epoch 3/100, Train Loss: -0.0095, Test Loss: 0.2101\n",
      "Epoch 4/100, Train Loss: -0.0218, Test Loss: 0.1272\n",
      "Epoch 5/100, Train Loss: -0.0288, Test Loss: 0.0852\n",
      "Epoch 6/100, Train Loss: -0.0302, Test Loss: 0.0586\n",
      "Epoch 7/100, Train Loss: -0.0332, Test Loss: 0.0443\n",
      "Epoch 8/100, Train Loss: -0.0341, Test Loss: 0.0342\n",
      "Epoch 9/100, Train Loss: -0.0358, Test Loss: 0.0306\n",
      "Epoch 10/100, Train Loss: -0.0368, Test Loss: 0.0293\n",
      "Epoch 11/100, Train Loss: -0.0378, Test Loss: 0.0295\n",
      "Epoch 12/100, Train Loss: -0.0387, Test Loss: 0.0310\n",
      "Epoch 13/100, Train Loss: -0.0396, Test Loss: 0.0381\n",
      "Epoch 14/100, Train Loss: -0.0406, Test Loss: 0.0516\n",
      "Epoch 15/100, Train Loss: -0.0416, Test Loss: 0.0694\n",
      "Epoch 16/100, Train Loss: -0.0425, Test Loss: 0.0847\n",
      "Epoch 17/100, Train Loss: -0.0432, Test Loss: 0.1016\n",
      "Epoch 18/100, Train Loss: -0.0441, Test Loss: 0.1193\n",
      "Epoch 19/100, Train Loss: -0.0449, Test Loss: 0.1388\n",
      "Epoch 20/100, Train Loss: -0.0455, Test Loss: 0.1609\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: -0.0171, Test Loss: -0.0341\n",
      "Epoch 2/100, Train Loss: -0.0258, Test Loss: -0.0649\n",
      "Epoch 3/100, Train Loss: -0.0330, Test Loss: -0.0661\n",
      "Epoch 4/100, Train Loss: -0.0387, Test Loss: -0.0618\n",
      "Epoch 5/100, Train Loss: -0.0377, Test Loss: -0.0683\n",
      "Epoch 6/100, Train Loss: -0.0406, Test Loss: -0.0677\n",
      "Epoch 7/100, Train Loss: -0.0405, Test Loss: -0.0703\n",
      "Epoch 8/100, Train Loss: -0.0420, Test Loss: -0.0708\n",
      "Epoch 9/100, Train Loss: -0.0425, Test Loss: -0.0730\n",
      "Epoch 10/100, Train Loss: -0.0438, Test Loss: -0.0740\n",
      "Epoch 11/100, Train Loss: -0.0443, Test Loss: -0.0763\n",
      "Epoch 12/100, Train Loss: -0.0453, Test Loss: -0.0776\n",
      "Epoch 13/100, Train Loss: -0.0463, Test Loss: -0.0791\n",
      "Epoch 14/100, Train Loss: -0.0470, Test Loss: -0.0814\n",
      "Epoch 15/100, Train Loss: -0.0478, Test Loss: -0.0829\n",
      "Epoch 16/100, Train Loss: -0.0486, Test Loss: -0.0841\n",
      "Epoch 17/100, Train Loss: -0.0491, Test Loss: -0.0853\n",
      "Epoch 18/100, Train Loss: -0.0495, Test Loss: -0.0864\n",
      "Epoch 19/100, Train Loss: -0.0500, Test Loss: -0.0859\n",
      "Epoch 20/100, Train Loss: -0.0490, Test Loss: -0.0749\n",
      "Epoch 21/100, Train Loss: -0.0456, Test Loss: -0.0876\n",
      "Epoch 22/100, Train Loss: -0.0492, Test Loss: -0.0853\n",
      "Epoch 23/100, Train Loss: -0.0506, Test Loss: -0.0839\n",
      "Epoch 24/100, Train Loss: -0.0512, Test Loss: -0.0837\n",
      "Epoch 25/100, Train Loss: -0.0512, Test Loss: -0.0884\n",
      "Epoch 26/100, Train Loss: -0.0522, Test Loss: -0.0900\n",
      "Epoch 27/100, Train Loss: -0.0529, Test Loss: -0.0904\n",
      "Epoch 28/100, Train Loss: -0.0536, Test Loss: -0.0895\n",
      "Epoch 29/100, Train Loss: -0.0538, Test Loss: -0.0891\n",
      "Epoch 30/100, Train Loss: -0.0540, Test Loss: -0.0902\n",
      "Epoch 31/100, Train Loss: -0.0540, Test Loss: -0.0923\n",
      "Epoch 32/100, Train Loss: -0.0544, Test Loss: -0.0918\n",
      "Epoch 33/100, Train Loss: -0.0541, Test Loss: -0.0847\n",
      "Epoch 34/100, Train Loss: -0.0525, Test Loss: -0.0794\n",
      "Epoch 35/100, Train Loss: -0.0494, Test Loss: -0.0914\n",
      "Epoch 36/100, Train Loss: -0.0521, Test Loss: -0.0808\n",
      "Epoch 37/100, Train Loss: -0.0495, Test Loss: -0.0920\n",
      "Epoch 38/100, Train Loss: -0.0548, Test Loss: -0.0866\n",
      "Epoch 39/100, Train Loss: -0.0539, Test Loss: -0.0877\n",
      "Epoch 40/100, Train Loss: -0.0541, Test Loss: -0.0916\n",
      "Epoch 41/100, Train Loss: -0.0556, Test Loss: -0.0934\n",
      "Epoch 42/100, Train Loss: -0.0565, Test Loss: -0.0934\n",
      "Epoch 43/100, Train Loss: -0.0569, Test Loss: -0.0964\n",
      "Epoch 44/100, Train Loss: -0.0577, Test Loss: -0.0948\n",
      "Epoch 45/100, Train Loss: -0.0580, Test Loss: -0.0946\n",
      "Epoch 46/100, Train Loss: -0.0583, Test Loss: -0.0975\n",
      "Epoch 47/100, Train Loss: -0.0590, Test Loss: -0.0982\n",
      "Epoch 48/100, Train Loss: -0.0593, Test Loss: -0.0988\n",
      "Epoch 49/100, Train Loss: -0.0597, Test Loss: -0.0988\n",
      "Epoch 50/100, Train Loss: -0.0602, Test Loss: -0.1002\n",
      "Epoch 51/100, Train Loss: -0.0605, Test Loss: -0.0990\n",
      "Epoch 52/100, Train Loss: -0.0604, Test Loss: -0.0995\n",
      "Epoch 53/100, Train Loss: -0.0603, Test Loss: -0.0983\n",
      "Epoch 54/100, Train Loss: -0.0598, Test Loss: -0.0944\n",
      "Epoch 55/100, Train Loss: -0.0586, Test Loss: -0.0891\n",
      "Epoch 56/100, Train Loss: -0.0570, Test Loss: -0.0932\n",
      "Epoch 57/100, Train Loss: -0.0574, Test Loss: -0.0990\n",
      "Epoch 58/100, Train Loss: -0.0556, Test Loss: -0.0839\n",
      "Epoch 59/100, Train Loss: -0.0503, Test Loss: -0.0849\n",
      "Epoch 60/100, Train Loss: -0.0509, Test Loss: -0.0820\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 0.0992, Test Loss: -0.0185\n",
      "Epoch 2/100, Train Loss: 0.0336, Test Loss: -0.0326\n",
      "Epoch 3/100, Train Loss: 0.0017, Test Loss: -0.0483\n",
      "Epoch 4/100, Train Loss: -0.0124, Test Loss: -0.0519\n",
      "Epoch 5/100, Train Loss: -0.0194, Test Loss: -0.0530\n",
      "Epoch 6/100, Train Loss: -0.0243, Test Loss: -0.0544\n",
      "Epoch 7/100, Train Loss: -0.0276, Test Loss: -0.0541\n",
      "Epoch 8/100, Train Loss: -0.0289, Test Loss: -0.0537\n",
      "Epoch 9/100, Train Loss: -0.0302, Test Loss: -0.0548\n",
      "Epoch 10/100, Train Loss: -0.0316, Test Loss: -0.0555\n",
      "Epoch 11/100, Train Loss: -0.0323, Test Loss: -0.0560\n",
      "Epoch 12/100, Train Loss: -0.0328, Test Loss: -0.0566\n",
      "Epoch 13/100, Train Loss: -0.0336, Test Loss: -0.0575\n",
      "Epoch 14/100, Train Loss: -0.0344, Test Loss: -0.0581\n",
      "Epoch 15/100, Train Loss: -0.0351, Test Loss: -0.0590\n",
      "Epoch 16/100, Train Loss: -0.0358, Test Loss: -0.0601\n",
      "Epoch 17/100, Train Loss: -0.0366, Test Loss: -0.0612\n",
      "Epoch 18/100, Train Loss: -0.0373, Test Loss: -0.0621\n",
      "Epoch 19/100, Train Loss: -0.0381, Test Loss: -0.0634\n",
      "Epoch 20/100, Train Loss: -0.0389, Test Loss: -0.0644\n",
      "Epoch 21/100, Train Loss: -0.0397, Test Loss: -0.0656\n",
      "Epoch 22/100, Train Loss: -0.0405, Test Loss: -0.0664\n",
      "Epoch 23/100, Train Loss: -0.0412, Test Loss: -0.0675\n",
      "Epoch 24/100, Train Loss: -0.0419, Test Loss: -0.0677\n",
      "Epoch 25/100, Train Loss: -0.0425, Test Loss: -0.0681\n",
      "Epoch 26/100, Train Loss: -0.0432, Test Loss: -0.0705\n",
      "Epoch 27/100, Train Loss: -0.0441, Test Loss: -0.0709\n",
      "Epoch 28/100, Train Loss: -0.0435, Test Loss: -0.0690\n",
      "Epoch 29/100, Train Loss: -0.0444, Test Loss: -0.0723\n",
      "Epoch 30/100, Train Loss: -0.0453, Test Loss: -0.0729\n",
      "Epoch 31/100, Train Loss: -0.0453, Test Loss: -0.0710\n",
      "Epoch 32/100, Train Loss: -0.0457, Test Loss: -0.0687\n",
      "Epoch 33/100, Train Loss: -0.0445, Test Loss: -0.0715\n",
      "Epoch 34/100, Train Loss: -0.0449, Test Loss: -0.0661\n",
      "Epoch 35/100, Train Loss: -0.0432, Test Loss: -0.0640\n",
      "Epoch 36/100, Train Loss: -0.0417, Test Loss: -0.0571\n",
      "Epoch 37/100, Train Loss: -0.0409, Test Loss: -0.0669\n",
      "Epoch 38/100, Train Loss: -0.0429, Test Loss: -0.0668\n",
      "Epoch 39/100, Train Loss: -0.0427, Test Loss: -0.0706\n",
      "Epoch 40/100, Train Loss: -0.0436, Test Loss: -0.0730\n",
      "Epoch 41/100, Train Loss: -0.0444, Test Loss: -0.0721\n",
      "Epoch 42/100, Train Loss: -0.0454, Test Loss: -0.0717\n",
      "Epoch 43/100, Train Loss: -0.0456, Test Loss: -0.0726\n",
      "Epoch 44/100, Train Loss: -0.0464, Test Loss: -0.0726\n",
      "Epoch 45/100, Train Loss: -0.0465, Test Loss: -0.0741\n",
      "Epoch 46/100, Train Loss: -0.0469, Test Loss: -0.0735\n",
      "Epoch 47/100, Train Loss: -0.0465, Test Loss: -0.0732\n",
      "Epoch 48/100, Train Loss: -0.0463, Test Loss: -0.0745\n",
      "Epoch 49/100, Train Loss: -0.0464, Test Loss: -0.0735\n",
      "Epoch 50/100, Train Loss: -0.0463, Test Loss: -0.0735\n",
      "Epoch 51/100, Train Loss: -0.0454, Test Loss: -0.0746\n",
      "Epoch 52/100, Train Loss: -0.0457, Test Loss: -0.0735\n",
      "Epoch 53/100, Train Loss: -0.0446, Test Loss: -0.0714\n",
      "Epoch 54/100, Train Loss: -0.0427, Test Loss: -0.0753\n",
      "Epoch 55/100, Train Loss: -0.0442, Test Loss: -0.0679\n",
      "Epoch 56/100, Train Loss: -0.0440, Test Loss: -0.0728\n",
      "Epoch 57/100, Train Loss: -0.0455, Test Loss: -0.0689\n",
      "Epoch 58/100, Train Loss: -0.0451, Test Loss: -0.0757\n",
      "Epoch 59/100, Train Loss: -0.0470, Test Loss: -0.0690\n",
      "Epoch 60/100, Train Loss: -0.0463, Test Loss: -0.0752\n",
      "Epoch 61/100, Train Loss: -0.0476, Test Loss: -0.0717\n",
      "Epoch 62/100, Train Loss: -0.0473, Test Loss: -0.0759\n",
      "Epoch 63/100, Train Loss: -0.0487, Test Loss: -0.0751\n",
      "Epoch 64/100, Train Loss: -0.0488, Test Loss: -0.0765\n",
      "Epoch 65/100, Train Loss: -0.0491, Test Loss: -0.0775\n",
      "Epoch 66/100, Train Loss: -0.0499, Test Loss: -0.0769\n",
      "Epoch 67/100, Train Loss: -0.0500, Test Loss: -0.0787\n",
      "Epoch 68/100, Train Loss: -0.0505, Test Loss: -0.0780\n",
      "Epoch 69/100, Train Loss: -0.0504, Test Loss: -0.0768\n",
      "Epoch 70/100, Train Loss: -0.0493, Test Loss: -0.0787\n",
      "Epoch 71/100, Train Loss: -0.0501, Test Loss: -0.0792\n",
      "Epoch 72/100, Train Loss: -0.0510, Test Loss: -0.0737\n",
      "Epoch 73/100, Train Loss: -0.0480, Test Loss: -0.0776\n",
      "Epoch 74/100, Train Loss: -0.0490, Test Loss: -0.0771\n",
      "Epoch 75/100, Train Loss: -0.0493, Test Loss: -0.0718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100, Train Loss: -0.0462, Test Loss: -0.0803\n",
      "Epoch 77/100, Train Loss: -0.0502, Test Loss: -0.0707\n",
      "Epoch 78/100, Train Loss: -0.0452, Test Loss: -0.0791\n",
      "Epoch 79/100, Train Loss: -0.0458, Test Loss: -0.0729\n",
      "Epoch 80/100, Train Loss: -0.0458, Test Loss: -0.0773\n",
      "Epoch 81/100, Train Loss: -0.0474, Test Loss: -0.0720\n",
      "Epoch 82/100, Train Loss: -0.0493, Test Loss: -0.0766\n",
      "Epoch 83/100, Train Loss: -0.0499, Test Loss: -0.0742\n",
      "Epoch 84/100, Train Loss: -0.0495, Test Loss: -0.0661\n",
      "Epoch 85/100, Train Loss: -0.0471, Test Loss: -0.0675\n",
      "Epoch 86/100, Train Loss: -0.0458, Test Loss: -0.0732\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -832     |\n",
      "| time/              |          |\n",
      "|    fps             | 305      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -813     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 310      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 13       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.539    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00779  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 121      |\n",
      "|    policy_objective       | 0.0135   |\n",
      "|    std                    | 0.648    |\n",
      "|    value_loss             | 469      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -806     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 315      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 19       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.636    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00751  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 122      |\n",
      "|    policy_objective       | 0.0171   |\n",
      "|    std                    | 0.654    |\n",
      "|    value_loss             | 469      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -803     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 319      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 25       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.754    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00684  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 123      |\n",
      "|    policy_objective       | 0.0168   |\n",
      "|    std                    | 0.65     |\n",
      "|    value_loss             | 317      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -793     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 321      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 31       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.578    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00709  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 124      |\n",
      "|    policy_objective       | 0.0162   |\n",
      "|    std                    | 0.642    |\n",
      "|    value_loss             | 564      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -794     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 321      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 38       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.394    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00748  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 125      |\n",
      "|    policy_objective       | 0.0188   |\n",
      "|    std                    | 0.646    |\n",
      "|    value_loss             | 336      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -786     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 323      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 44       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.561    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00783  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 126      |\n",
      "|    policy_objective       | 0.013    |\n",
      "|    std                    | 0.62     |\n",
      "|    value_loss             | 349      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -774     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 319      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 51       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.56     |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00751  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 127      |\n",
      "|    policy_objective       | 0.0136   |\n",
      "|    std                    | 0.631    |\n",
      "|    value_loss             | 402      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-612.6545992769301\n",
      "------------------------------\n",
      "round: 16\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.1620, Test Loss: -0.0358\n",
      "Epoch 2/100, Train Loss: 0.0826, Test Loss: -0.0436\n",
      "Epoch 3/100, Train Loss: 0.0038, Test Loss: -0.0539\n",
      "Epoch 4/100, Train Loss: -0.0001, Test Loss: -0.0428\n",
      "Epoch 5/100, Train Loss: -0.0114, Test Loss: -0.0580\n",
      "Epoch 6/100, Train Loss: -0.0222, Test Loss: -0.0556\n",
      "Epoch 7/100, Train Loss: -0.0219, Test Loss: -0.0551\n",
      "Epoch 8/100, Train Loss: -0.0248, Test Loss: -0.0591\n",
      "Epoch 9/100, Train Loss: -0.0281, Test Loss: -0.0591\n",
      "Epoch 10/100, Train Loss: -0.0281, Test Loss: -0.0579\n",
      "Epoch 11/100, Train Loss: -0.0281, Test Loss: -0.0589\n",
      "Epoch 12/100, Train Loss: -0.0296, Test Loss: -0.0601\n",
      "Epoch 13/100, Train Loss: -0.0307, Test Loss: -0.0603\n",
      "Epoch 14/100, Train Loss: -0.0311, Test Loss: -0.0608\n",
      "Epoch 15/100, Train Loss: -0.0318, Test Loss: -0.0623\n",
      "Epoch 16/100, Train Loss: -0.0328, Test Loss: -0.0636\n",
      "Epoch 17/100, Train Loss: -0.0335, Test Loss: -0.0648\n",
      "Epoch 18/100, Train Loss: -0.0342, Test Loss: -0.0661\n",
      "Epoch 19/100, Train Loss: -0.0351, Test Loss: -0.0671\n",
      "Epoch 20/100, Train Loss: -0.0358, Test Loss: -0.0683\n",
      "Epoch 21/100, Train Loss: -0.0367, Test Loss: -0.0697\n",
      "Epoch 22/100, Train Loss: -0.0377, Test Loss: -0.0709\n",
      "Epoch 23/100, Train Loss: -0.0385, Test Loss: -0.0722\n",
      "Epoch 24/100, Train Loss: -0.0394, Test Loss: -0.0735\n",
      "Epoch 25/100, Train Loss: -0.0401, Test Loss: -0.0749\n",
      "Epoch 26/100, Train Loss: -0.0407, Test Loss: -0.0758\n",
      "Epoch 27/100, Train Loss: -0.0413, Test Loss: -0.0765\n",
      "Epoch 28/100, Train Loss: -0.0421, Test Loss: -0.0771\n",
      "Epoch 29/100, Train Loss: -0.0427, Test Loss: -0.0780\n",
      "Epoch 30/100, Train Loss: -0.0433, Test Loss: -0.0785\n",
      "Epoch 31/100, Train Loss: -0.0439, Test Loss: -0.0792\n",
      "Epoch 32/100, Train Loss: -0.0445, Test Loss: -0.0800\n",
      "Epoch 33/100, Train Loss: -0.0450, Test Loss: -0.0811\n",
      "Epoch 34/100, Train Loss: -0.0453, Test Loss: -0.0815\n",
      "Epoch 35/100, Train Loss: -0.0447, Test Loss: -0.0771\n",
      "Epoch 36/100, Train Loss: -0.0435, Test Loss: -0.0812\n",
      "Epoch 37/100, Train Loss: -0.0456, Test Loss: -0.0768\n",
      "Epoch 38/100, Train Loss: -0.0443, Test Loss: -0.0828\n",
      "Epoch 39/100, Train Loss: -0.0466, Test Loss: -0.0804\n",
      "Epoch 40/100, Train Loss: -0.0459, Test Loss: -0.0838\n",
      "Epoch 41/100, Train Loss: -0.0472, Test Loss: -0.0816\n",
      "Epoch 42/100, Train Loss: -0.0469, Test Loss: -0.0825\n",
      "Epoch 43/100, Train Loss: -0.0463, Test Loss: -0.0813\n",
      "Epoch 44/100, Train Loss: -0.0473, Test Loss: -0.0829\n",
      "Epoch 45/100, Train Loss: -0.0473, Test Loss: -0.0849\n",
      "Epoch 46/100, Train Loss: -0.0481, Test Loss: -0.0817\n",
      "Epoch 47/100, Train Loss: -0.0475, Test Loss: -0.0864\n",
      "Epoch 48/100, Train Loss: -0.0493, Test Loss: -0.0835\n",
      "Epoch 49/100, Train Loss: -0.0484, Test Loss: -0.0876\n",
      "Epoch 50/100, Train Loss: -0.0495, Test Loss: -0.0829\n",
      "Epoch 51/100, Train Loss: -0.0486, Test Loss: -0.0886\n",
      "Epoch 52/100, Train Loss: -0.0506, Test Loss: -0.0848\n",
      "Epoch 53/100, Train Loss: -0.0495, Test Loss: -0.0886\n",
      "Epoch 54/100, Train Loss: -0.0506, Test Loss: -0.0839\n",
      "Epoch 55/100, Train Loss: -0.0494, Test Loss: -0.0891\n",
      "Epoch 56/100, Train Loss: -0.0516, Test Loss: -0.0871\n",
      "Epoch 57/100, Train Loss: -0.0507, Test Loss: -0.0888\n",
      "Epoch 58/100, Train Loss: -0.0516, Test Loss: -0.0892\n",
      "Epoch 59/100, Train Loss: -0.0507, Test Loss: -0.0828\n",
      "Epoch 60/100, Train Loss: -0.0503, Test Loss: -0.0903\n",
      "Epoch 61/100, Train Loss: -0.0529, Test Loss: -0.0891\n",
      "Epoch 62/100, Train Loss: -0.0523, Test Loss: -0.0897\n",
      "Epoch 63/100, Train Loss: -0.0527, Test Loss: -0.0907\n",
      "Epoch 64/100, Train Loss: -0.0524, Test Loss: -0.0836\n",
      "Epoch 65/100, Train Loss: -0.0503, Test Loss: -0.0843\n",
      "Epoch 66/100, Train Loss: -0.0521, Test Loss: -0.0917\n",
      "Epoch 67/100, Train Loss: -0.0530, Test Loss: -0.0865\n",
      "Epoch 68/100, Train Loss: -0.0517, Test Loss: -0.0936\n",
      "Epoch 69/100, Train Loss: -0.0529, Test Loss: -0.0876\n",
      "Epoch 70/100, Train Loss: -0.0494, Test Loss: -0.0864\n",
      "Epoch 71/100, Train Loss: -0.0432, Test Loss: -0.0464\n",
      "Epoch 72/100, Train Loss: -0.0340, Test Loss: -0.0874\n",
      "Epoch 73/100, Train Loss: -0.0473, Test Loss: -0.0693\n",
      "Epoch 74/100, Train Loss: -0.0449, Test Loss: -0.0803\n",
      "Epoch 75/100, Train Loss: -0.0481, Test Loss: -0.0788\n",
      "Epoch 76/100, Train Loss: -0.0478, Test Loss: -0.0875\n",
      "Epoch 77/100, Train Loss: -0.0500, Test Loss: -0.0900\n",
      "Epoch 78/100, Train Loss: -0.0516, Test Loss: -0.0899\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 0.1949, Test Loss: -0.0062\n",
      "Epoch 2/100, Train Loss: 0.0974, Test Loss: -0.0354\n",
      "Epoch 3/100, Train Loss: 0.0023, Test Loss: -0.0221\n",
      "Epoch 4/100, Train Loss: 0.0058, Test Loss: -0.0538\n",
      "Epoch 5/100, Train Loss: -0.0231, Test Loss: -0.0710\n",
      "Epoch 6/100, Train Loss: -0.0278, Test Loss: -0.0706\n",
      "Epoch 7/100, Train Loss: -0.0345, Test Loss: -0.0811\n",
      "Epoch 8/100, Train Loss: -0.0398, Test Loss: -0.0740\n",
      "Epoch 9/100, Train Loss: -0.0391, Test Loss: -0.0750\n",
      "Epoch 10/100, Train Loss: -0.0417, Test Loss: -0.0810\n",
      "Epoch 11/100, Train Loss: -0.0433, Test Loss: -0.0802\n",
      "Epoch 12/100, Train Loss: -0.0433, Test Loss: -0.0805\n",
      "Epoch 13/100, Train Loss: -0.0441, Test Loss: -0.0816\n",
      "Epoch 14/100, Train Loss: -0.0447, Test Loss: -0.0808\n",
      "Epoch 15/100, Train Loss: -0.0449, Test Loss: -0.0809\n",
      "Epoch 16/100, Train Loss: -0.0452, Test Loss: -0.0816\n",
      "Epoch 17/100, Train Loss: -0.0454, Test Loss: -0.0821\n",
      "Epoch 18/100, Train Loss: -0.0457, Test Loss: -0.0825\n",
      "Epoch 19/100, Train Loss: -0.0460, Test Loss: -0.0824\n",
      "Epoch 20/100, Train Loss: -0.0461, Test Loss: -0.0826\n",
      "Epoch 21/100, Train Loss: -0.0464, Test Loss: -0.0830\n",
      "Epoch 22/100, Train Loss: -0.0466, Test Loss: -0.0834\n",
      "Epoch 23/100, Train Loss: -0.0469, Test Loss: -0.0837\n",
      "Epoch 24/100, Train Loss: -0.0471, Test Loss: -0.0839\n",
      "Epoch 25/100, Train Loss: -0.0473, Test Loss: -0.0842\n",
      "Epoch 26/100, Train Loss: -0.0475, Test Loss: -0.0846\n",
      "Epoch 27/100, Train Loss: -0.0477, Test Loss: -0.0849\n",
      "Epoch 28/100, Train Loss: -0.0479, Test Loss: -0.0851\n",
      "Epoch 29/100, Train Loss: -0.0481, Test Loss: -0.0855\n",
      "Epoch 30/100, Train Loss: -0.0483, Test Loss: -0.0858\n",
      "Epoch 31/100, Train Loss: -0.0485, Test Loss: -0.0861\n",
      "Epoch 32/100, Train Loss: -0.0487, Test Loss: -0.0863\n",
      "Epoch 33/100, Train Loss: -0.0489, Test Loss: -0.0867\n",
      "Epoch 34/100, Train Loss: -0.0492, Test Loss: -0.0870\n",
      "Epoch 35/100, Train Loss: -0.0494, Test Loss: -0.0873\n",
      "Epoch 36/100, Train Loss: -0.0496, Test Loss: -0.0877\n",
      "Epoch 37/100, Train Loss: -0.0498, Test Loss: -0.0880\n",
      "Epoch 38/100, Train Loss: -0.0501, Test Loss: -0.0884\n",
      "Epoch 39/100, Train Loss: -0.0503, Test Loss: -0.0887\n",
      "Epoch 40/100, Train Loss: -0.0506, Test Loss: -0.0890\n",
      "Epoch 41/100, Train Loss: -0.0508, Test Loss: -0.0894\n",
      "Epoch 42/100, Train Loss: -0.0511, Test Loss: -0.0897\n",
      "Epoch 43/100, Train Loss: -0.0513, Test Loss: -0.0900\n",
      "Epoch 44/100, Train Loss: -0.0516, Test Loss: -0.0904\n",
      "Epoch 45/100, Train Loss: -0.0518, Test Loss: -0.0907\n",
      "Epoch 46/100, Train Loss: -0.0521, Test Loss: -0.0911\n",
      "Epoch 47/100, Train Loss: -0.0524, Test Loss: -0.0915\n",
      "Epoch 48/100, Train Loss: -0.0527, Test Loss: -0.0919\n",
      "Epoch 49/100, Train Loss: -0.0530, Test Loss: -0.0923\n",
      "Epoch 50/100, Train Loss: -0.0532, Test Loss: -0.0927\n",
      "Epoch 51/100, Train Loss: -0.0535, Test Loss: -0.0931\n",
      "Epoch 52/100, Train Loss: -0.0538, Test Loss: -0.0935\n",
      "Epoch 53/100, Train Loss: -0.0540, Test Loss: -0.0939\n",
      "Epoch 54/100, Train Loss: -0.0543, Test Loss: -0.0943\n",
      "Epoch 55/100, Train Loss: -0.0546, Test Loss: -0.0947\n",
      "Epoch 56/100, Train Loss: -0.0548, Test Loss: -0.0951\n",
      "Epoch 57/100, Train Loss: -0.0551, Test Loss: -0.0954\n",
      "Epoch 58/100, Train Loss: -0.0554, Test Loss: -0.0957\n",
      "Epoch 59/100, Train Loss: -0.0556, Test Loss: -0.0962\n",
      "Epoch 60/100, Train Loss: -0.0559, Test Loss: -0.0965\n",
      "Epoch 61/100, Train Loss: -0.0561, Test Loss: -0.0968\n",
      "Epoch 62/100, Train Loss: -0.0564, Test Loss: -0.0971\n",
      "Epoch 63/100, Train Loss: -0.0566, Test Loss: -0.0974\n",
      "Epoch 64/100, Train Loss: -0.0569, Test Loss: -0.0976\n",
      "Epoch 65/100, Train Loss: -0.0571, Test Loss: -0.0979\n",
      "Epoch 66/100, Train Loss: -0.0574, Test Loss: -0.0981\n",
      "Epoch 67/100, Train Loss: -0.0576, Test Loss: -0.0984\n",
      "Epoch 68/100, Train Loss: -0.0578, Test Loss: -0.0986\n",
      "Epoch 69/100, Train Loss: -0.0581, Test Loss: -0.0988\n",
      "Epoch 70/100, Train Loss: -0.0583, Test Loss: -0.0991\n",
      "Epoch 71/100, Train Loss: -0.0586, Test Loss: -0.0993\n",
      "Epoch 72/100, Train Loss: -0.0588, Test Loss: -0.0996\n",
      "Epoch 73/100, Train Loss: -0.0590, Test Loss: -0.0999\n",
      "Epoch 74/100, Train Loss: -0.0592, Test Loss: -0.1002\n",
      "Epoch 75/100, Train Loss: -0.0595, Test Loss: -0.1003\n",
      "Epoch 76/100, Train Loss: -0.0597, Test Loss: -0.1007\n",
      "Epoch 77/100, Train Loss: -0.0599, Test Loss: -0.1009\n",
      "Epoch 78/100, Train Loss: -0.0601, Test Loss: -0.1013\n",
      "Epoch 79/100, Train Loss: -0.0602, Test Loss: -0.1013\n",
      "Epoch 80/100, Train Loss: -0.0601, Test Loss: -0.1015\n",
      "Epoch 81/100, Train Loss: -0.0600, Test Loss: -0.1006\n",
      "Epoch 82/100, Train Loss: -0.0599, Test Loss: -0.0952\n",
      "Epoch 83/100, Train Loss: -0.0594, Test Loss: -0.0874\n",
      "Epoch 84/100, Train Loss: -0.0563, Test Loss: -0.0900\n",
      "Epoch 85/100, Train Loss: -0.0535, Test Loss: -0.0886\n",
      "Epoch 86/100, Train Loss: -0.0532, Test Loss: -0.0648\n",
      "Epoch 87/100, Train Loss: -0.0491, Test Loss: -0.0918\n",
      "Epoch 88/100, Train Loss: -0.0526, Test Loss: -0.0800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100, Train Loss: -0.0541, Test Loss: -0.0616\n",
      "Epoch 90/100, Train Loss: -0.0501, Test Loss: -0.0836\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 0.0979, Test Loss: 0.0222\n",
      "Epoch 2/100, Train Loss: 0.0614, Test Loss: -0.0558\n",
      "Epoch 3/100, Train Loss: -0.0101, Test Loss: -0.0275\n",
      "Epoch 4/100, Train Loss: -0.0133, Test Loss: -0.0650\n",
      "Epoch 5/100, Train Loss: -0.0279, Test Loss: -0.0541\n",
      "Epoch 6/100, Train Loss: -0.0278, Test Loss: -0.0604\n",
      "Epoch 7/100, Train Loss: -0.0354, Test Loss: -0.0641\n",
      "Epoch 8/100, Train Loss: -0.0375, Test Loss: -0.0625\n",
      "Epoch 9/100, Train Loss: -0.0375, Test Loss: -0.0633\n",
      "Epoch 10/100, Train Loss: -0.0380, Test Loss: -0.0645\n",
      "Epoch 11/100, Train Loss: -0.0394, Test Loss: -0.0662\n",
      "Epoch 12/100, Train Loss: -0.0406, Test Loss: -0.0669\n",
      "Epoch 13/100, Train Loss: -0.0406, Test Loss: -0.0673\n",
      "Epoch 14/100, Train Loss: -0.0410, Test Loss: -0.0684\n",
      "Epoch 15/100, Train Loss: -0.0417, Test Loss: -0.0690\n",
      "Epoch 16/100, Train Loss: -0.0421, Test Loss: -0.0697\n",
      "Epoch 17/100, Train Loss: -0.0427, Test Loss: -0.0706\n",
      "Epoch 18/100, Train Loss: -0.0432, Test Loss: -0.0713\n",
      "Epoch 19/100, Train Loss: -0.0436, Test Loss: -0.0725\n",
      "Epoch 20/100, Train Loss: -0.0441, Test Loss: -0.0735\n",
      "Epoch 21/100, Train Loss: -0.0445, Test Loss: -0.0745\n",
      "Epoch 22/100, Train Loss: -0.0450, Test Loss: -0.0753\n",
      "Epoch 23/100, Train Loss: -0.0454, Test Loss: -0.0763\n",
      "Epoch 24/100, Train Loss: -0.0459, Test Loss: -0.0771\n",
      "Epoch 25/100, Train Loss: -0.0463, Test Loss: -0.0780\n",
      "Epoch 26/100, Train Loss: -0.0467, Test Loss: -0.0789\n",
      "Epoch 27/100, Train Loss: -0.0471, Test Loss: -0.0797\n",
      "Epoch 28/100, Train Loss: -0.0475, Test Loss: -0.0805\n",
      "Epoch 29/100, Train Loss: -0.0478, Test Loss: -0.0811\n",
      "Epoch 30/100, Train Loss: -0.0481, Test Loss: -0.0818\n",
      "Epoch 31/100, Train Loss: -0.0484, Test Loss: -0.0821\n",
      "Epoch 32/100, Train Loss: -0.0485, Test Loss: -0.0822\n",
      "Epoch 33/100, Train Loss: -0.0486, Test Loss: -0.0829\n",
      "Epoch 34/100, Train Loss: -0.0487, Test Loss: -0.0844\n",
      "Epoch 35/100, Train Loss: -0.0486, Test Loss: -0.0830\n",
      "Epoch 36/100, Train Loss: -0.0479, Test Loss: -0.0830\n",
      "Epoch 37/100, Train Loss: -0.0480, Test Loss: -0.0845\n",
      "Epoch 38/100, Train Loss: -0.0483, Test Loss: -0.0815\n",
      "Epoch 39/100, Train Loss: -0.0475, Test Loss: -0.0846\n",
      "Epoch 40/100, Train Loss: -0.0488, Test Loss: -0.0848\n",
      "Epoch 41/100, Train Loss: -0.0492, Test Loss: -0.0844\n",
      "Epoch 42/100, Train Loss: -0.0493, Test Loss: -0.0859\n",
      "Epoch 43/100, Train Loss: -0.0502, Test Loss: -0.0858\n",
      "Epoch 44/100, Train Loss: -0.0505, Test Loss: -0.0860\n",
      "Epoch 45/100, Train Loss: -0.0505, Test Loss: -0.0870\n",
      "Epoch 46/100, Train Loss: -0.0510, Test Loss: -0.0869\n",
      "Epoch 47/100, Train Loss: -0.0510, Test Loss: -0.0873\n",
      "Epoch 48/100, Train Loss: -0.0512, Test Loss: -0.0877\n",
      "Epoch 49/100, Train Loss: -0.0515, Test Loss: -0.0878\n",
      "Epoch 50/100, Train Loss: -0.0517, Test Loss: -0.0885\n",
      "Epoch 51/100, Train Loss: -0.0519, Test Loss: -0.0888\n",
      "Epoch 52/100, Train Loss: -0.0520, Test Loss: -0.0887\n",
      "Epoch 53/100, Train Loss: -0.0521, Test Loss: -0.0893\n",
      "Epoch 54/100, Train Loss: -0.0524, Test Loss: -0.0894\n",
      "Epoch 55/100, Train Loss: -0.0525, Test Loss: -0.0895\n",
      "Epoch 56/100, Train Loss: -0.0525, Test Loss: -0.0903\n",
      "Epoch 57/100, Train Loss: -0.0525, Test Loss: -0.0892\n",
      "Epoch 58/100, Train Loss: -0.0520, Test Loss: -0.0873\n",
      "Epoch 59/100, Train Loss: -0.0516, Test Loss: -0.0896\n",
      "Epoch 60/100, Train Loss: -0.0515, Test Loss: -0.0897\n",
      "Epoch 61/100, Train Loss: -0.0505, Test Loss: -0.0818\n",
      "Epoch 62/100, Train Loss: -0.0482, Test Loss: -0.0872\n",
      "Epoch 63/100, Train Loss: -0.0490, Test Loss: -0.0844\n",
      "Epoch 64/100, Train Loss: -0.0472, Test Loss: -0.0851\n",
      "Epoch 65/100, Train Loss: -0.0489, Test Loss: -0.0844\n",
      "Epoch 66/100, Train Loss: -0.0494, Test Loss: -0.0895\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -746     |\n",
      "| time/              |          |\n",
      "|    fps             | 302      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -736     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 314      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 13       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.735    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00845  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 129      |\n",
      "|    policy_objective       | 0.0124   |\n",
      "|    std                    | 0.635    |\n",
      "|    value_loss             | 420      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -733     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 319      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 19       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.703    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00802  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 130      |\n",
      "|    policy_objective       | 0.0183   |\n",
      "|    std                    | 0.617    |\n",
      "|    value_loss             | 427      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -741     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 320      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 25       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.535    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00798  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 131      |\n",
      "|    policy_objective       | 0.0206   |\n",
      "|    std                    | 0.614    |\n",
      "|    value_loss             | 391      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -744     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 321      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 31       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.639    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00653  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 132      |\n",
      "|    policy_objective       | 0.0135   |\n",
      "|    std                    | 0.608    |\n",
      "|    value_loss             | 440      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -751     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 322      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 38       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.791    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.0099   |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 133      |\n",
      "|    policy_objective       | 0.0139   |\n",
      "|    std                    | 0.606    |\n",
      "|    value_loss             | 326      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -757     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 323      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 44       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.661    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00675  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 134      |\n",
      "|    policy_objective       | 0.0186   |\n",
      "|    std                    | 0.595    |\n",
      "|    value_loss             | 486      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -758     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 324      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 50       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.662    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00726  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 135      |\n",
      "|    policy_objective       | 0.0129   |\n",
      "|    std                    | 0.593    |\n",
      "|    value_loss             | 435      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-643.8770642060787\n",
      "------------------------------\n",
      "round: 17\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.0736, Test Loss: 8.9175\n",
      "Epoch 2/100, Train Loss: 0.0967, Test Loss: 5.3093\n",
      "Epoch 3/100, Train Loss: -0.0260, Test Loss: 3.5951\n",
      "Epoch 4/100, Train Loss: 0.0118, Test Loss: 2.5720\n",
      "Epoch 5/100, Train Loss: -0.0399, Test Loss: 2.0483\n",
      "Epoch 6/100, Train Loss: -0.0302, Test Loss: 1.6351\n",
      "Epoch 7/100, Train Loss: -0.0326, Test Loss: 1.2450\n",
      "Epoch 8/100, Train Loss: -0.0467, Test Loss: 1.0485\n",
      "Epoch 9/100, Train Loss: -0.0421, Test Loss: 0.9473\n",
      "Epoch 10/100, Train Loss: -0.0441, Test Loss: 0.8847\n",
      "Epoch 11/100, Train Loss: -0.0477, Test Loss: 0.8617\n",
      "Epoch 12/100, Train Loss: -0.0466, Test Loss: 0.8554\n",
      "Epoch 13/100, Train Loss: -0.0480, Test Loss: 0.8626\n",
      "Epoch 14/100, Train Loss: -0.0491, Test Loss: 0.8912\n",
      "Epoch 15/100, Train Loss: -0.0490, Test Loss: 0.9298\n",
      "Epoch 16/100, Train Loss: -0.0501, Test Loss: 0.9746\n",
      "Epoch 17/100, Train Loss: -0.0504, Test Loss: 1.0336\n",
      "Epoch 18/100, Train Loss: -0.0509, Test Loss: 1.1087\n",
      "Epoch 19/100, Train Loss: -0.0514, Test Loss: 1.1868\n",
      "Epoch 20/100, Train Loss: -0.0518, Test Loss: 1.2715\n",
      "Epoch 21/100, Train Loss: -0.0524, Test Loss: 1.3899\n",
      "Epoch 22/100, Train Loss: -0.0529, Test Loss: 1.5284\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 0.0191, Test Loss: 0.0355\n",
      "Epoch 2/100, Train Loss: 0.0190, Test Loss: -0.0360\n",
      "Epoch 3/100, Train Loss: -0.0316, Test Loss: -0.0153\n",
      "Epoch 4/100, Train Loss: -0.0280, Test Loss: -0.0537\n",
      "Epoch 5/100, Train Loss: -0.0383, Test Loss: -0.0549\n",
      "Epoch 6/100, Train Loss: -0.0413, Test Loss: -0.0708\n",
      "Epoch 7/100, Train Loss: -0.0457, Test Loss: -0.0624\n",
      "Epoch 8/100, Train Loss: -0.0434, Test Loss: -0.0697\n",
      "Epoch 9/100, Train Loss: -0.0479, Test Loss: -0.0720\n",
      "Epoch 10/100, Train Loss: -0.0484, Test Loss: -0.0737\n",
      "Epoch 11/100, Train Loss: -0.0496, Test Loss: -0.0728\n",
      "Epoch 12/100, Train Loss: -0.0499, Test Loss: -0.0758\n",
      "Epoch 13/100, Train Loss: -0.0521, Test Loss: -0.0755\n",
      "Epoch 14/100, Train Loss: -0.0522, Test Loss: -0.0768\n",
      "Epoch 15/100, Train Loss: -0.0537, Test Loss: -0.0774\n",
      "Epoch 16/100, Train Loss: -0.0548, Test Loss: -0.0773\n",
      "Epoch 17/100, Train Loss: -0.0558, Test Loss: -0.0765\n",
      "Epoch 18/100, Train Loss: -0.0571, Test Loss: -0.0751\n",
      "Epoch 19/100, Train Loss: -0.0580, Test Loss: -0.0729\n",
      "Epoch 20/100, Train Loss: -0.0591, Test Loss: -0.0698\n",
      "Epoch 21/100, Train Loss: -0.0602, Test Loss: -0.0653\n",
      "Epoch 22/100, Train Loss: -0.0611, Test Loss: -0.0586\n",
      "Epoch 23/100, Train Loss: -0.0620, Test Loss: -0.0510\n",
      "Epoch 24/100, Train Loss: -0.0627, Test Loss: -0.0415\n",
      "Epoch 25/100, Train Loss: -0.0636, Test Loss: -0.0297\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 0.0673, Test Loss: 0.0405\n",
      "Epoch 2/100, Train Loss: 0.0533, Test Loss: -0.0558\n",
      "Epoch 3/100, Train Loss: -0.0077, Test Loss: -0.0181\n",
      "Epoch 4/100, Train Loss: -0.0075, Test Loss: -0.0509\n",
      "Epoch 5/100, Train Loss: -0.0264, Test Loss: -0.0525\n",
      "Epoch 6/100, Train Loss: -0.0289, Test Loss: -0.0569\n",
      "Epoch 7/100, Train Loss: -0.0338, Test Loss: -0.0607\n",
      "Epoch 8/100, Train Loss: -0.0350, Test Loss: -0.0615\n",
      "Epoch 9/100, Train Loss: -0.0370, Test Loss: -0.0631\n",
      "Epoch 10/100, Train Loss: -0.0386, Test Loss: -0.0627\n",
      "Epoch 11/100, Train Loss: -0.0386, Test Loss: -0.0622\n",
      "Epoch 12/100, Train Loss: -0.0393, Test Loss: -0.0631\n",
      "Epoch 13/100, Train Loss: -0.0404, Test Loss: -0.0636\n",
      "Epoch 14/100, Train Loss: -0.0409, Test Loss: -0.0637\n",
      "Epoch 15/100, Train Loss: -0.0412, Test Loss: -0.0640\n",
      "Epoch 16/100, Train Loss: -0.0418, Test Loss: -0.0646\n",
      "Epoch 17/100, Train Loss: -0.0425, Test Loss: -0.0653\n",
      "Epoch 18/100, Train Loss: -0.0431, Test Loss: -0.0653\n",
      "Epoch 19/100, Train Loss: -0.0436, Test Loss: -0.0658\n",
      "Epoch 20/100, Train Loss: -0.0442, Test Loss: -0.0661\n",
      "Epoch 21/100, Train Loss: -0.0449, Test Loss: -0.0662\n",
      "Epoch 22/100, Train Loss: -0.0456, Test Loss: -0.0663\n",
      "Epoch 23/100, Train Loss: -0.0462, Test Loss: -0.0665\n",
      "Epoch 24/100, Train Loss: -0.0469, Test Loss: -0.0666\n",
      "Epoch 25/100, Train Loss: -0.0475, Test Loss: -0.0666\n",
      "Epoch 26/100, Train Loss: -0.0482, Test Loss: -0.0663\n",
      "Epoch 27/100, Train Loss: -0.0489, Test Loss: -0.0661\n",
      "Epoch 28/100, Train Loss: -0.0495, Test Loss: -0.0657\n",
      "Epoch 29/100, Train Loss: -0.0502, Test Loss: -0.0651\n",
      "Epoch 30/100, Train Loss: -0.0509, Test Loss: -0.0643\n",
      "Epoch 31/100, Train Loss: -0.0514, Test Loss: -0.0634\n",
      "Epoch 32/100, Train Loss: -0.0521, Test Loss: -0.0623\n",
      "Epoch 33/100, Train Loss: -0.0528, Test Loss: -0.0608\n",
      "Epoch 34/100, Train Loss: -0.0535, Test Loss: -0.0593\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -787     |\n",
      "| time/              |          |\n",
      "|    fps             | 351      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -806     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 341      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 12       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.588    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00708  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 137      |\n",
      "|    policy_objective       | 0.0178   |\n",
      "|    std                    | 0.614    |\n",
      "|    value_loss             | 523      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -777     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 332      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 18       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.618    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00787  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 138      |\n",
      "|    policy_objective       | 0.0173   |\n",
      "|    std                    | 0.608    |\n",
      "|    value_loss             | 436      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -771     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 331      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 24       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.658    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00816  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 139      |\n",
      "|    policy_objective       | 0.0132   |\n",
      "|    std                    | 0.612    |\n",
      "|    value_loss             | 418      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -736     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 332      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 30       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.523    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00718  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 140      |\n",
      "|    policy_objective       | 0.0179   |\n",
      "|    std                    | 0.614    |\n",
      "|    value_loss             | 391      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -713     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 333      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 36       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.673    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00833  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 141      |\n",
      "|    policy_objective       | 0.0171   |\n",
      "|    std                    | 0.618    |\n",
      "|    value_loss             | 455      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -699     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 332      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 43       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.709    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00817  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 142      |\n",
      "|    policy_objective       | 0.0172   |\n",
      "|    std                    | 0.611    |\n",
      "|    value_loss             | 484      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -695     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 331      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 49       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.677    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00832  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 143      |\n",
      "|    policy_objective       | 0.0136   |\n",
      "|    std                    | 0.6      |\n",
      "|    value_loss             | 491      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-350.6331199895329\n",
      "------------------------------\n",
      "round: 18\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: -0.0049, Test Loss: -0.0418\n",
      "Epoch 2/100, Train Loss: 0.0020, Test Loss: -0.0632\n",
      "Epoch 3/100, Train Loss: -0.0522, Test Loss: -0.0576\n",
      "Epoch 4/100, Train Loss: -0.0329, Test Loss: -0.0652\n",
      "Epoch 5/100, Train Loss: -0.0538, Test Loss: -0.0843\n",
      "Epoch 6/100, Train Loss: -0.0548, Test Loss: -0.0722\n",
      "Epoch 7/100, Train Loss: -0.0509, Test Loss: -0.0828\n",
      "Epoch 8/100, Train Loss: -0.0597, Test Loss: -0.0846\n",
      "Epoch 9/100, Train Loss: -0.0582, Test Loss: -0.0805\n",
      "Epoch 10/100, Train Loss: -0.0577, Test Loss: -0.0858\n",
      "Epoch 11/100, Train Loss: -0.0613, Test Loss: -0.0871\n",
      "Epoch 12/100, Train Loss: -0.0608, Test Loss: -0.0863\n",
      "Epoch 13/100, Train Loss: -0.0613, Test Loss: -0.0884\n",
      "Epoch 14/100, Train Loss: -0.0629, Test Loss: -0.0886\n",
      "Epoch 15/100, Train Loss: -0.0629, Test Loss: -0.0888\n",
      "Epoch 16/100, Train Loss: -0.0639, Test Loss: -0.0898\n",
      "Epoch 17/100, Train Loss: -0.0646, Test Loss: -0.0893\n",
      "Epoch 18/100, Train Loss: -0.0648, Test Loss: -0.0889\n",
      "Epoch 19/100, Train Loss: -0.0653, Test Loss: -0.0881\n",
      "Epoch 20/100, Train Loss: -0.0653, Test Loss: -0.0889\n",
      "Epoch 21/100, Train Loss: -0.0664, Test Loss: -0.0892\n",
      "Epoch 22/100, Train Loss: -0.0673, Test Loss: -0.0876\n",
      "Epoch 23/100, Train Loss: -0.0678, Test Loss: -0.0853\n",
      "Epoch 24/100, Train Loss: -0.0678, Test Loss: -0.0827\n",
      "Epoch 25/100, Train Loss: -0.0675, Test Loss: -0.0813\n",
      "Epoch 26/100, Train Loss: -0.0674, Test Loss: -0.0818\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 0.2835, Test Loss: 0.0592\n",
      "Epoch 2/100, Train Loss: 0.2645, Test Loss: -0.0491\n",
      "Epoch 3/100, Train Loss: 0.1479, Test Loss: 0.0078\n",
      "Epoch 4/100, Train Loss: 0.1210, Test Loss: -0.0758\n",
      "Epoch 5/100, Train Loss: 0.0734, Test Loss: -0.0499\n",
      "Epoch 6/100, Train Loss: 0.0568, Test Loss: -0.0860\n",
      "Epoch 7/100, Train Loss: 0.0213, Test Loss: -0.0727\n",
      "Epoch 8/100, Train Loss: 0.0141, Test Loss: -0.0726\n",
      "Epoch 9/100, Train Loss: -0.0023, Test Loss: -0.0881\n",
      "Epoch 10/100, Train Loss: -0.0139, Test Loss: -0.0819\n",
      "Epoch 11/100, Train Loss: -0.0181, Test Loss: -0.0834\n",
      "Epoch 12/100, Train Loss: -0.0247, Test Loss: -0.0833\n",
      "Epoch 13/100, Train Loss: -0.0279, Test Loss: -0.0803\n",
      "Epoch 14/100, Train Loss: -0.0303, Test Loss: -0.0833\n",
      "Epoch 15/100, Train Loss: -0.0335, Test Loss: -0.0839\n",
      "Epoch 16/100, Train Loss: -0.0347, Test Loss: -0.0825\n",
      "Epoch 17/100, Train Loss: -0.0359, Test Loss: -0.0827\n",
      "Epoch 18/100, Train Loss: -0.0372, Test Loss: -0.0820\n",
      "Epoch 19/100, Train Loss: -0.0379, Test Loss: -0.0818\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 1.7330, Test Loss: -0.0027\n",
      "Epoch 2/100, Train Loss: 0.4034, Test Loss: 0.0215\n",
      "Epoch 3/100, Train Loss: 0.1039, Test Loss: -0.0624\n",
      "Epoch 4/100, Train Loss: 0.0306, Test Loss: -0.0157\n",
      "Epoch 5/100, Train Loss: 0.0120, Test Loss: -0.0582\n",
      "Epoch 6/100, Train Loss: -0.0215, Test Loss: -0.0774\n",
      "Epoch 7/100, Train Loss: -0.0267, Test Loss: -0.0643\n",
      "Epoch 8/100, Train Loss: -0.0252, Test Loss: -0.0674\n",
      "Epoch 9/100, Train Loss: -0.0323, Test Loss: -0.0764\n",
      "Epoch 10/100, Train Loss: -0.0369, Test Loss: -0.0753\n",
      "Epoch 11/100, Train Loss: -0.0362, Test Loss: -0.0724\n",
      "Epoch 12/100, Train Loss: -0.0366, Test Loss: -0.0744\n",
      "Epoch 13/100, Train Loss: -0.0388, Test Loss: -0.0762\n",
      "Epoch 14/100, Train Loss: -0.0396, Test Loss: -0.0752\n",
      "Epoch 15/100, Train Loss: -0.0394, Test Loss: -0.0749\n",
      "Epoch 16/100, Train Loss: -0.0399, Test Loss: -0.0762\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -715     |\n",
      "| time/              |          |\n",
      "|    fps             | 356      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -657     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 340      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 12       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.663    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00636  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 145      |\n",
      "|    policy_objective       | 0.015    |\n",
      "|    std                    | 0.596    |\n",
      "|    value_loss             | 535      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -674     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 336      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 18       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.719    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00724  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 146      |\n",
      "|    policy_objective       | 0.015    |\n",
      "|    std                    | 0.579    |\n",
      "|    value_loss             | 536      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -684     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 333      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 24       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.721    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00837  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 147      |\n",
      "|    policy_objective       | 0.0228   |\n",
      "|    std                    | 0.576    |\n",
      "|    value_loss             | 768      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -677     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 332      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 30       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.755    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00832  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 148      |\n",
      "|    policy_objective       | 0.0199   |\n",
      "|    std                    | 0.567    |\n",
      "|    value_loss             | 755      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -696     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 332      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 37       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.642    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00875  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 149      |\n",
      "|    policy_objective       | 0.0223   |\n",
      "|    std                    | 0.562    |\n",
      "|    value_loss             | 757      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -691     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 331      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 43       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.575    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00711  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 150      |\n",
      "|    policy_objective       | 0.0204   |\n",
      "|    std                    | 0.552    |\n",
      "|    value_loss             | 861      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -691     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 330      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 49       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.72     |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00644  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 151      |\n",
      "|    policy_objective       | 0.017    |\n",
      "|    std                    | 0.535    |\n",
      "|    value_loss             | 901      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-189.95389851189685\n",
      "------------------------------\n",
      "round: 19\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.2753, Test Loss: -0.0486\n",
      "Epoch 2/100, Train Loss: 0.1514, Test Loss: -0.0398\n",
      "Epoch 3/100, Train Loss: 0.0964, Test Loss: -0.0878\n",
      "Epoch 4/100, Train Loss: 0.0503, Test Loss: -0.0781\n",
      "Epoch 5/100, Train Loss: 0.0191, Test Loss: -0.0877\n",
      "Epoch 6/100, Train Loss: -0.0007, Test Loss: -0.0863\n",
      "Epoch 7/100, Train Loss: -0.0129, Test Loss: -0.0836\n",
      "Epoch 8/100, Train Loss: -0.0207, Test Loss: -0.0856\n",
      "Epoch 9/100, Train Loss: -0.0278, Test Loss: -0.0838\n",
      "Epoch 10/100, Train Loss: -0.0300, Test Loss: -0.0836\n",
      "Epoch 11/100, Train Loss: -0.0337, Test Loss: -0.0829\n",
      "Epoch 12/100, Train Loss: -0.0355, Test Loss: -0.0817\n",
      "Epoch 13/100, Train Loss: -0.0367, Test Loss: -0.0817\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 0.0104, Test Loss: -0.0746\n",
      "Epoch 2/100, Train Loss: -0.0102, Test Loss: -0.0748\n",
      "Epoch 3/100, Train Loss: -0.0226, Test Loss: -0.0723\n",
      "Epoch 4/100, Train Loss: -0.0288, Test Loss: -0.0728\n",
      "Epoch 5/100, Train Loss: -0.0334, Test Loss: -0.0709\n",
      "Epoch 6/100, Train Loss: -0.0351, Test Loss: -0.0706\n",
      "Epoch 7/100, Train Loss: -0.0375, Test Loss: -0.0700\n",
      "Epoch 8/100, Train Loss: -0.0384, Test Loss: -0.0702\n",
      "Epoch 9/100, Train Loss: -0.0399, Test Loss: -0.0706\n",
      "Epoch 10/100, Train Loss: -0.0409, Test Loss: -0.0716\n",
      "Epoch 11/100, Train Loss: -0.0422, Test Loss: -0.0724\n",
      "Epoch 12/100, Train Loss: -0.0433, Test Loss: -0.0737\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: -0.0019, Test Loss: -0.0679\n",
      "Epoch 2/100, Train Loss: -0.0123, Test Loss: -0.0674\n",
      "Epoch 3/100, Train Loss: -0.0251, Test Loss: -0.0648\n",
      "Epoch 4/100, Train Loss: -0.0273, Test Loss: -0.0624\n",
      "Epoch 5/100, Train Loss: -0.0297, Test Loss: -0.0624\n",
      "Epoch 6/100, Train Loss: -0.0320, Test Loss: -0.0623\n",
      "Epoch 7/100, Train Loss: -0.0330, Test Loss: -0.0611\n",
      "Epoch 8/100, Train Loss: -0.0335, Test Loss: -0.0612\n",
      "Epoch 9/100, Train Loss: -0.0342, Test Loss: -0.0616\n",
      "Epoch 10/100, Train Loss: -0.0348, Test Loss: -0.0624\n",
      "Epoch 11/100, Train Loss: -0.0356, Test Loss: -0.0632\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -745     |\n",
      "| time/              |          |\n",
      "|    fps             | 349      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -727     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 337      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 12       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.831    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.0073   |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 153      |\n",
      "|    policy_objective       | 0.0213   |\n",
      "|    std                    | 0.503    |\n",
      "|    value_loss             | 732      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -675     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 332      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 18       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.658    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00804  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 154      |\n",
      "|    policy_objective       | 0.017    |\n",
      "|    std                    | 0.495    |\n",
      "|    value_loss             | 605      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -669     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 331      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 24       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.848    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00891  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 155      |\n",
      "|    policy_objective       | 0.0195   |\n",
      "|    std                    | 0.488    |\n",
      "|    value_loss             | 671      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -682     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 328      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 31       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.733    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00754  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 156      |\n",
      "|    policy_objective       | 0.0103   |\n",
      "|    std                    | 0.486    |\n",
      "|    value_loss             | 1.1e+03  |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -648     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 328      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 37       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.604    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00778  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 157      |\n",
      "|    policy_objective       | 0.0141   |\n",
      "|    std                    | 0.488    |\n",
      "|    value_loss             | 1.16e+03 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -654     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 327      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 43       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.72     |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00837  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 158      |\n",
      "|    policy_objective       | 0.0179   |\n",
      "|    std                    | 0.5      |\n",
      "|    value_loss             | 769      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -657     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 327      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 50       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.47     |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00914  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 159      |\n",
      "|    policy_objective       | 0.0184   |\n",
      "|    std                    | 0.493    |\n",
      "|    value_loss             | 917      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-190.92159692987335\n",
      "------------------------------\n",
      "round: 20\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.0031, Test Loss: -0.0203\n",
      "Epoch 2/100, Train Loss: -0.0108, Test Loss: -0.0413\n",
      "Epoch 3/100, Train Loss: -0.0171, Test Loss: -0.0557\n",
      "Epoch 4/100, Train Loss: -0.0314, Test Loss: -0.0642\n",
      "Epoch 5/100, Train Loss: -0.0328, Test Loss: -0.0582\n",
      "Epoch 6/100, Train Loss: -0.0341, Test Loss: -0.0628\n",
      "Epoch 7/100, Train Loss: -0.0360, Test Loss: -0.0654\n",
      "Epoch 8/100, Train Loss: -0.0374, Test Loss: -0.0669\n",
      "Epoch 9/100, Train Loss: -0.0386, Test Loss: -0.0674\n",
      "Epoch 10/100, Train Loss: -0.0389, Test Loss: -0.0672\n",
      "Epoch 11/100, Train Loss: -0.0391, Test Loss: -0.0677\n",
      "Epoch 12/100, Train Loss: -0.0396, Test Loss: -0.0684\n",
      "Epoch 13/100, Train Loss: -0.0402, Test Loss: -0.0684\n",
      "Epoch 14/100, Train Loss: -0.0405, Test Loss: -0.0694\n",
      "Epoch 15/100, Train Loss: -0.0410, Test Loss: -0.0698\n",
      "Epoch 16/100, Train Loss: -0.0414, Test Loss: -0.0699\n",
      "Epoch 17/100, Train Loss: -0.0418, Test Loss: -0.0709\n",
      "Epoch 18/100, Train Loss: -0.0422, Test Loss: -0.0712\n",
      "Epoch 19/100, Train Loss: -0.0426, Test Loss: -0.0727\n",
      "Epoch 20/100, Train Loss: -0.0427, Test Loss: -0.0697\n",
      "Epoch 21/100, Train Loss: -0.0422, Test Loss: -0.0734\n",
      "Epoch 22/100, Train Loss: -0.0420, Test Loss: -0.0737\n",
      "Epoch 23/100, Train Loss: -0.0434, Test Loss: -0.0731\n",
      "Epoch 24/100, Train Loss: -0.0438, Test Loss: -0.0737\n",
      "Epoch 25/100, Train Loss: -0.0441, Test Loss: -0.0754\n",
      "Epoch 26/100, Train Loss: -0.0442, Test Loss: -0.0763\n",
      "Epoch 27/100, Train Loss: -0.0448, Test Loss: -0.0764\n",
      "Epoch 28/100, Train Loss: -0.0454, Test Loss: -0.0768\n",
      "Epoch 29/100, Train Loss: -0.0458, Test Loss: -0.0775\n",
      "Epoch 30/100, Train Loss: -0.0460, Test Loss: -0.0783\n",
      "Epoch 31/100, Train Loss: -0.0465, Test Loss: -0.0780\n",
      "Epoch 32/100, Train Loss: -0.0468, Test Loss: -0.0795\n",
      "Epoch 33/100, Train Loss: -0.0470, Test Loss: -0.0778\n",
      "Epoch 34/100, Train Loss: -0.0471, Test Loss: -0.0805\n",
      "Epoch 35/100, Train Loss: -0.0465, Test Loss: -0.0756\n",
      "Epoch 36/100, Train Loss: -0.0459, Test Loss: -0.0789\n",
      "Epoch 37/100, Train Loss: -0.0460, Test Loss: -0.0801\n",
      "Epoch 38/100, Train Loss: -0.0476, Test Loss: -0.0776\n",
      "Epoch 39/100, Train Loss: -0.0472, Test Loss: -0.0771\n",
      "Epoch 40/100, Train Loss: -0.0472, Test Loss: -0.0789\n",
      "Epoch 41/100, Train Loss: -0.0472, Test Loss: -0.0734\n",
      "Epoch 42/100, Train Loss: -0.0470, Test Loss: -0.0767\n",
      "Epoch 43/100, Train Loss: -0.0437, Test Loss: -0.0805\n",
      "Epoch 44/100, Train Loss: -0.0452, Test Loss: -0.0746\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: -0.0442, Test Loss: 0.0036\n",
      "Epoch 2/100, Train Loss: -0.0440, Test Loss: 0.0081\n",
      "Epoch 3/100, Train Loss: -0.0498, Test Loss: 0.0140\n",
      "Epoch 4/100, Train Loss: -0.0508, Test Loss: 0.0321\n",
      "Epoch 5/100, Train Loss: -0.0529, Test Loss: 0.0562\n",
      "Epoch 6/100, Train Loss: -0.0544, Test Loss: 0.0900\n",
      "Epoch 7/100, Train Loss: -0.0559, Test Loss: 0.1325\n",
      "Epoch 8/100, Train Loss: -0.0573, Test Loss: 0.1797\n",
      "Epoch 9/100, Train Loss: -0.0582, Test Loss: 0.2342\n",
      "Epoch 10/100, Train Loss: -0.0594, Test Loss: 0.2985\n",
      "Epoch 11/100, Train Loss: -0.0604, Test Loss: 0.3653\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: -0.0369, Test Loss: -0.0609\n",
      "Epoch 2/100, Train Loss: -0.0366, Test Loss: -0.0658\n",
      "Epoch 3/100, Train Loss: -0.0397, Test Loss: -0.0662\n",
      "Epoch 4/100, Train Loss: -0.0411, Test Loss: -0.0678\n",
      "Epoch 5/100, Train Loss: -0.0421, Test Loss: -0.0716\n",
      "Epoch 6/100, Train Loss: -0.0434, Test Loss: -0.0715\n",
      "Epoch 7/100, Train Loss: -0.0442, Test Loss: -0.0741\n",
      "Epoch 8/100, Train Loss: -0.0442, Test Loss: -0.0757\n",
      "Epoch 9/100, Train Loss: -0.0450, Test Loss: -0.0766\n",
      "Epoch 10/100, Train Loss: -0.0453, Test Loss: -0.0794\n",
      "Epoch 11/100, Train Loss: -0.0463, Test Loss: -0.0776\n",
      "Epoch 12/100, Train Loss: -0.0460, Test Loss: -0.0747\n",
      "Epoch 13/100, Train Loss: -0.0443, Test Loss: -0.0746\n",
      "Epoch 14/100, Train Loss: -0.0447, Test Loss: -0.0778\n",
      "Epoch 15/100, Train Loss: -0.0459, Test Loss: -0.0805\n",
      "Epoch 16/100, Train Loss: -0.0469, Test Loss: -0.0805\n",
      "Epoch 17/100, Train Loss: -0.0478, Test Loss: -0.0816\n",
      "Epoch 18/100, Train Loss: -0.0485, Test Loss: -0.0818\n",
      "Epoch 19/100, Train Loss: -0.0490, Test Loss: -0.0833\n",
      "Epoch 20/100, Train Loss: -0.0493, Test Loss: -0.0848\n",
      "Epoch 21/100, Train Loss: -0.0497, Test Loss: -0.0857\n",
      "Epoch 22/100, Train Loss: -0.0503, Test Loss: -0.0860\n",
      "Epoch 23/100, Train Loss: -0.0506, Test Loss: -0.0868\n",
      "Epoch 24/100, Train Loss: -0.0511, Test Loss: -0.0875\n",
      "Epoch 25/100, Train Loss: -0.0515, Test Loss: -0.0880\n",
      "Epoch 26/100, Train Loss: -0.0516, Test Loss: -0.0863\n",
      "Epoch 27/100, Train Loss: -0.0516, Test Loss: -0.0888\n",
      "Epoch 28/100, Train Loss: -0.0515, Test Loss: -0.0854\n",
      "Epoch 29/100, Train Loss: -0.0511, Test Loss: -0.0861\n",
      "Epoch 30/100, Train Loss: -0.0506, Test Loss: -0.0866\n",
      "Epoch 31/100, Train Loss: -0.0508, Test Loss: -0.0823\n",
      "Epoch 32/100, Train Loss: -0.0481, Test Loss: -0.0829\n",
      "Epoch 33/100, Train Loss: -0.0478, Test Loss: -0.0797\n",
      "Epoch 34/100, Train Loss: -0.0471, Test Loss: -0.0788\n",
      "Epoch 35/100, Train Loss: -0.0466, Test Loss: -0.0797\n",
      "Epoch 36/100, Train Loss: -0.0497, Test Loss: -0.0862\n",
      "Epoch 37/100, Train Loss: -0.0503, Test Loss: -0.0847\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -784     |\n",
      "| time/              |          |\n",
      "|    fps             | 359      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -622     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 334      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 12       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.755    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.0088   |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 161      |\n",
      "|    policy_objective       | 0.0208   |\n",
      "|    std                    | 0.498    |\n",
      "|    value_loss             | 722      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -658     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 332      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 18       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.8      |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00781  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 162      |\n",
      "|    policy_objective       | 0.0154   |\n",
      "|    std                    | 0.483    |\n",
      "|    value_loss             | 757      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -673     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 332      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 24       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.683    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00852  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 163      |\n",
      "|    policy_objective       | 0.0168   |\n",
      "|    std                    | 0.469    |\n",
      "|    value_loss             | 793      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -656     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 330      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 30       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.671    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00677  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 164      |\n",
      "|    policy_objective       | 0.0245   |\n",
      "|    std                    | 0.462    |\n",
      "|    value_loss             | 769      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -660     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 331      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 37       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.746    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00823  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 165      |\n",
      "|    policy_objective       | 0.0193   |\n",
      "|    std                    | 0.469    |\n",
      "|    value_loss             | 795      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -660     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 330      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 43       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.58     |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00819  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 166      |\n",
      "|    policy_objective       | 0.0207   |\n",
      "|    std                    | 0.462    |\n",
      "|    value_loss             | 638      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -662     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 330      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 49       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.532    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00852  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 167      |\n",
      "|    policy_objective       | 0.0209   |\n",
      "|    std                    | 0.463    |\n",
      "|    value_loss             | 715      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-125.54477715431713\n",
      "------------------------------\n",
      "round: 21\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: -0.0019, Test Loss: -0.0666\n",
      "Epoch 2/100, Train Loss: -0.0189, Test Loss: -0.0487\n",
      "Epoch 3/100, Train Loss: -0.0318, Test Loss: -0.0591\n",
      "Epoch 4/100, Train Loss: -0.0312, Test Loss: -0.0601\n",
      "Epoch 5/100, Train Loss: -0.0377, Test Loss: -0.0690\n",
      "Epoch 6/100, Train Loss: -0.0361, Test Loss: -0.0628\n",
      "Epoch 7/100, Train Loss: -0.0381, Test Loss: -0.0715\n",
      "Epoch 8/100, Train Loss: -0.0399, Test Loss: -0.0662\n",
      "Epoch 9/100, Train Loss: -0.0390, Test Loss: -0.0705\n",
      "Epoch 10/100, Train Loss: -0.0406, Test Loss: -0.0700\n",
      "Epoch 11/100, Train Loss: -0.0399, Test Loss: -0.0707\n",
      "Epoch 12/100, Train Loss: -0.0410, Test Loss: -0.0718\n",
      "Epoch 13/100, Train Loss: -0.0411, Test Loss: -0.0719\n",
      "Epoch 14/100, Train Loss: -0.0416, Test Loss: -0.0734\n",
      "Epoch 15/100, Train Loss: -0.0419, Test Loss: -0.0735\n",
      "Epoch 16/100, Train Loss: -0.0422, Test Loss: -0.0748\n",
      "Epoch 17/100, Train Loss: -0.0426, Test Loss: -0.0754\n",
      "Epoch 18/100, Train Loss: -0.0430, Test Loss: -0.0763\n",
      "Epoch 19/100, Train Loss: -0.0433, Test Loss: -0.0769\n",
      "Epoch 20/100, Train Loss: -0.0436, Test Loss: -0.0777\n",
      "Epoch 21/100, Train Loss: -0.0439, Test Loss: -0.0784\n",
      "Epoch 22/100, Train Loss: -0.0442, Test Loss: -0.0788\n",
      "Epoch 23/100, Train Loss: -0.0445, Test Loss: -0.0794\n",
      "Epoch 24/100, Train Loss: -0.0448, Test Loss: -0.0800\n",
      "Epoch 25/100, Train Loss: -0.0451, Test Loss: -0.0803\n",
      "Epoch 26/100, Train Loss: -0.0454, Test Loss: -0.0808\n",
      "Epoch 27/100, Train Loss: -0.0457, Test Loss: -0.0812\n",
      "Epoch 28/100, Train Loss: -0.0460, Test Loss: -0.0816\n",
      "Epoch 29/100, Train Loss: -0.0463, Test Loss: -0.0822\n",
      "Epoch 30/100, Train Loss: -0.0466, Test Loss: -0.0825\n",
      "Epoch 31/100, Train Loss: -0.0469, Test Loss: -0.0828\n",
      "Epoch 32/100, Train Loss: -0.0472, Test Loss: -0.0832\n",
      "Epoch 33/100, Train Loss: -0.0475, Test Loss: -0.0835\n",
      "Epoch 34/100, Train Loss: -0.0478, Test Loss: -0.0839\n",
      "Epoch 35/100, Train Loss: -0.0478, Test Loss: -0.0824\n",
      "Epoch 36/100, Train Loss: -0.0468, Test Loss: -0.0782\n",
      "Epoch 37/100, Train Loss: -0.0456, Test Loss: -0.0831\n",
      "Epoch 38/100, Train Loss: -0.0453, Test Loss: -0.0782\n",
      "Epoch 39/100, Train Loss: -0.0466, Test Loss: -0.0783\n",
      "Epoch 40/100, Train Loss: -0.0457, Test Loss: -0.0815\n",
      "Epoch 41/100, Train Loss: -0.0459, Test Loss: -0.0819\n",
      "Epoch 42/100, Train Loss: -0.0452, Test Loss: -0.0806\n",
      "Epoch 43/100, Train Loss: -0.0447, Test Loss: -0.0839\n",
      "Epoch 44/100, Train Loss: -0.0446, Test Loss: -0.0825\n",
      "Epoch 45/100, Train Loss: -0.0460, Test Loss: -0.0794\n",
      "Epoch 46/100, Train Loss: -0.0464, Test Loss: -0.0762\n",
      "Epoch 47/100, Train Loss: -0.0471, Test Loss: -0.0765\n",
      "Epoch 48/100, Train Loss: -0.0468, Test Loss: -0.0794\n",
      "Epoch 49/100, Train Loss: -0.0479, Test Loss: -0.0830\n",
      "Epoch 50/100, Train Loss: -0.0487, Test Loss: -0.0843\n",
      "Epoch 51/100, Train Loss: -0.0488, Test Loss: -0.0838\n",
      "Epoch 52/100, Train Loss: -0.0494, Test Loss: -0.0836\n",
      "Epoch 53/100, Train Loss: -0.0495, Test Loss: -0.0851\n",
      "Epoch 54/100, Train Loss: -0.0497, Test Loss: -0.0860\n",
      "Epoch 55/100, Train Loss: -0.0501, Test Loss: -0.0867\n",
      "Epoch 56/100, Train Loss: -0.0503, Test Loss: -0.0865\n",
      "Epoch 57/100, Train Loss: -0.0505, Test Loss: -0.0880\n",
      "Epoch 58/100, Train Loss: -0.0508, Test Loss: -0.0875\n",
      "Epoch 59/100, Train Loss: -0.0510, Test Loss: -0.0881\n",
      "Epoch 60/100, Train Loss: -0.0513, Test Loss: -0.0887\n",
      "Epoch 61/100, Train Loss: -0.0512, Test Loss: -0.0884\n",
      "Epoch 62/100, Train Loss: -0.0512, Test Loss: -0.0885\n",
      "Epoch 63/100, Train Loss: -0.0514, Test Loss: -0.0885\n",
      "Epoch 64/100, Train Loss: -0.0506, Test Loss: -0.0876\n",
      "Epoch 65/100, Train Loss: -0.0505, Test Loss: -0.0869\n",
      "Epoch 66/100, Train Loss: -0.0505, Test Loss: -0.0886\n",
      "Epoch 67/100, Train Loss: -0.0499, Test Loss: -0.0851\n",
      "Epoch 68/100, Train Loss: -0.0493, Test Loss: -0.0852\n",
      "Epoch 69/100, Train Loss: -0.0498, Test Loss: -0.0869\n",
      "Epoch 70/100, Train Loss: -0.0511, Test Loss: -0.0855\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: -0.0166, Test Loss: -0.0647\n",
      "Epoch 2/100, Train Loss: -0.0281, Test Loss: -0.0692\n",
      "Epoch 3/100, Train Loss: -0.0491, Test Loss: -0.0827\n",
      "Epoch 4/100, Train Loss: -0.0469, Test Loss: -0.0762\n",
      "Epoch 5/100, Train Loss: -0.0498, Test Loss: -0.0921\n",
      "Epoch 6/100, Train Loss: -0.0553, Test Loss: -0.0849\n",
      "Epoch 7/100, Train Loss: -0.0520, Test Loss: -0.0862\n",
      "Epoch 8/100, Train Loss: -0.0548, Test Loss: -0.0914\n",
      "Epoch 9/100, Train Loss: -0.0560, Test Loss: -0.0896\n",
      "Epoch 10/100, Train Loss: -0.0550, Test Loss: -0.0906\n",
      "Epoch 11/100, Train Loss: -0.0563, Test Loss: -0.0924\n",
      "Epoch 12/100, Train Loss: -0.0568, Test Loss: -0.0921\n",
      "Epoch 13/100, Train Loss: -0.0569, Test Loss: -0.0932\n",
      "Epoch 14/100, Train Loss: -0.0578, Test Loss: -0.0947\n",
      "Epoch 15/100, Train Loss: -0.0580, Test Loss: -0.0954\n",
      "Epoch 16/100, Train Loss: -0.0585, Test Loss: -0.0965\n",
      "Epoch 17/100, Train Loss: -0.0590, Test Loss: -0.0972\n",
      "Epoch 18/100, Train Loss: -0.0594, Test Loss: -0.0985\n",
      "Epoch 19/100, Train Loss: -0.0599, Test Loss: -0.0995\n",
      "Epoch 20/100, Train Loss: -0.0603, Test Loss: -0.1005\n",
      "Epoch 21/100, Train Loss: -0.0607, Test Loss: -0.1013\n",
      "Epoch 22/100, Train Loss: -0.0611, Test Loss: -0.1023\n",
      "Epoch 23/100, Train Loss: -0.0614, Test Loss: -0.1032\n",
      "Epoch 24/100, Train Loss: -0.0618, Test Loss: -0.1039\n",
      "Epoch 25/100, Train Loss: -0.0621, Test Loss: -0.1047\n",
      "Epoch 26/100, Train Loss: -0.0624, Test Loss: -0.1055\n",
      "Epoch 27/100, Train Loss: -0.0626, Test Loss: -0.1061\n",
      "Epoch 28/100, Train Loss: -0.0629, Test Loss: -0.1068\n",
      "Epoch 29/100, Train Loss: -0.0628, Test Loss: -0.1067\n",
      "Epoch 30/100, Train Loss: -0.0617, Test Loss: -0.1032\n",
      "Epoch 31/100, Train Loss: -0.0598, Test Loss: -0.1011\n",
      "Epoch 32/100, Train Loss: -0.0588, Test Loss: -0.1006\n",
      "Epoch 33/100, Train Loss: -0.0598, Test Loss: -0.1042\n",
      "Epoch 34/100, Train Loss: -0.0607, Test Loss: -0.0992\n",
      "Epoch 35/100, Train Loss: -0.0609, Test Loss: -0.1034\n",
      "Epoch 36/100, Train Loss: -0.0619, Test Loss: -0.1050\n",
      "Epoch 37/100, Train Loss: -0.0621, Test Loss: -0.1050\n",
      "Epoch 38/100, Train Loss: -0.0628, Test Loss: -0.1038\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: -0.0192, Test Loss: -0.0643\n",
      "Epoch 2/100, Train Loss: -0.0187, Test Loss: -0.0622\n",
      "Epoch 3/100, Train Loss: -0.0352, Test Loss: -0.0711\n",
      "Epoch 4/100, Train Loss: -0.0354, Test Loss: -0.0658\n",
      "Epoch 5/100, Train Loss: -0.0367, Test Loss: -0.0702\n",
      "Epoch 6/100, Train Loss: -0.0405, Test Loss: -0.0706\n",
      "Epoch 7/100, Train Loss: -0.0403, Test Loss: -0.0690\n",
      "Epoch 8/100, Train Loss: -0.0396, Test Loss: -0.0695\n",
      "Epoch 9/100, Train Loss: -0.0406, Test Loss: -0.0706\n",
      "Epoch 10/100, Train Loss: -0.0417, Test Loss: -0.0703\n",
      "Epoch 11/100, Train Loss: -0.0415, Test Loss: -0.0705\n",
      "Epoch 12/100, Train Loss: -0.0418, Test Loss: -0.0718\n",
      "Epoch 13/100, Train Loss: -0.0425, Test Loss: -0.0729\n",
      "Epoch 14/100, Train Loss: -0.0429, Test Loss: -0.0735\n",
      "Epoch 15/100, Train Loss: -0.0432, Test Loss: -0.0746\n",
      "Epoch 16/100, Train Loss: -0.0437, Test Loss: -0.0757\n",
      "Epoch 17/100, Train Loss: -0.0441, Test Loss: -0.0767\n",
      "Epoch 18/100, Train Loss: -0.0444, Test Loss: -0.0778\n",
      "Epoch 19/100, Train Loss: -0.0448, Test Loss: -0.0787\n",
      "Epoch 20/100, Train Loss: -0.0452, Test Loss: -0.0798\n",
      "Epoch 21/100, Train Loss: -0.0455, Test Loss: -0.0806\n",
      "Epoch 22/100, Train Loss: -0.0458, Test Loss: -0.0813\n",
      "Epoch 23/100, Train Loss: -0.0461, Test Loss: -0.0820\n",
      "Epoch 24/100, Train Loss: -0.0463, Test Loss: -0.0829\n",
      "Epoch 25/100, Train Loss: -0.0466, Test Loss: -0.0832\n",
      "Epoch 26/100, Train Loss: -0.0469, Test Loss: -0.0839\n",
      "Epoch 27/100, Train Loss: -0.0469, Test Loss: -0.0841\n",
      "Epoch 28/100, Train Loss: -0.0472, Test Loss: -0.0842\n",
      "Epoch 29/100, Train Loss: -0.0471, Test Loss: -0.0842\n",
      "Epoch 30/100, Train Loss: -0.0475, Test Loss: -0.0854\n",
      "Epoch 31/100, Train Loss: -0.0478, Test Loss: -0.0850\n",
      "Epoch 32/100, Train Loss: -0.0479, Test Loss: -0.0858\n",
      "Epoch 33/100, Train Loss: -0.0482, Test Loss: -0.0840\n",
      "Epoch 34/100, Train Loss: -0.0476, Test Loss: -0.0844\n",
      "Epoch 35/100, Train Loss: -0.0481, Test Loss: -0.0850\n",
      "Epoch 36/100, Train Loss: -0.0474, Test Loss: -0.0840\n",
      "Epoch 37/100, Train Loss: -0.0480, Test Loss: -0.0816\n",
      "Epoch 38/100, Train Loss: -0.0473, Test Loss: -0.0853\n",
      "Epoch 39/100, Train Loss: -0.0469, Test Loss: -0.0833\n",
      "Epoch 40/100, Train Loss: -0.0462, Test Loss: -0.0760\n",
      "Epoch 41/100, Train Loss: -0.0438, Test Loss: -0.0789\n",
      "Epoch 42/100, Train Loss: -0.0424, Test Loss: -0.0834\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -568     |\n",
      "| time/              |          |\n",
      "|    fps             | 348      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -566     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 339      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 12       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.783    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.0072   |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 169      |\n",
      "|    policy_objective       | 0.0159   |\n",
      "|    std                    | 0.453    |\n",
      "|    value_loss             | 1.14e+03 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -660     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 333      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 18       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.82     |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00785  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 170      |\n",
      "|    policy_objective       | 0.0218   |\n",
      "|    std                    | 0.448    |\n",
      "|    value_loss             | 851      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -689     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 333      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 24       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.76     |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00889  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 171      |\n",
      "|    policy_objective       | 0.0288   |\n",
      "|    std                    | 0.44     |\n",
      "|    value_loss             | 755      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -644     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 333      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 30       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.784    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00802  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 172      |\n",
      "|    policy_objective       | 0.0231   |\n",
      "|    std                    | 0.441    |\n",
      "|    value_loss             | 897      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -647     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 331      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 37       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.847    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.0071   |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 173      |\n",
      "|    policy_objective       | 0.0208   |\n",
      "|    std                    | 0.442    |\n",
      "|    value_loss             | 1.09e+03 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -631     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 330      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 43       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.743    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00702  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 174      |\n",
      "|    policy_objective       | 0.0163   |\n",
      "|    std                    | 0.442    |\n",
      "|    value_loss             | 1.02e+03 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -619     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 330      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 49       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.788    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00716  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 175      |\n",
      "|    policy_objective       | 0.0159   |\n",
      "|    std                    | 0.435    |\n",
      "|    value_loss             | 1.02e+03 |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-191.4934958951082\n",
      "------------------------------\n",
      "round: 22\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.0203, Test Loss: 0.0064\n",
      "Epoch 2/100, Train Loss: 0.0372, Test Loss: -0.0521\n",
      "Epoch 3/100, Train Loss: -0.0169, Test Loss: -0.0034\n",
      "Epoch 4/100, Train Loss: -0.0138, Test Loss: -0.0705\n",
      "Epoch 5/100, Train Loss: -0.0336, Test Loss: -0.0462\n",
      "Epoch 6/100, Train Loss: -0.0293, Test Loss: -0.0743\n",
      "Epoch 7/100, Train Loss: -0.0415, Test Loss: -0.0693\n",
      "Epoch 8/100, Train Loss: -0.0370, Test Loss: -0.0702\n",
      "Epoch 9/100, Train Loss: -0.0415, Test Loss: -0.0759\n",
      "Epoch 10/100, Train Loss: -0.0432, Test Loss: -0.0728\n",
      "Epoch 11/100, Train Loss: -0.0437, Test Loss: -0.0784\n",
      "Epoch 12/100, Train Loss: -0.0444, Test Loss: -0.0777\n",
      "Epoch 13/100, Train Loss: -0.0447, Test Loss: -0.0792\n",
      "Epoch 14/100, Train Loss: -0.0459, Test Loss: -0.0792\n",
      "Epoch 15/100, Train Loss: -0.0459, Test Loss: -0.0801\n",
      "Epoch 16/100, Train Loss: -0.0464, Test Loss: -0.0810\n",
      "Epoch 17/100, Train Loss: -0.0466, Test Loss: -0.0815\n",
      "Epoch 18/100, Train Loss: -0.0470, Test Loss: -0.0821\n",
      "Epoch 19/100, Train Loss: -0.0474, Test Loss: -0.0821\n",
      "Epoch 20/100, Train Loss: -0.0476, Test Loss: -0.0831\n",
      "Epoch 21/100, Train Loss: -0.0479, Test Loss: -0.0837\n",
      "Epoch 22/100, Train Loss: -0.0482, Test Loss: -0.0842\n",
      "Epoch 23/100, Train Loss: -0.0484, Test Loss: -0.0845\n",
      "Epoch 24/100, Train Loss: -0.0487, Test Loss: -0.0853\n",
      "Epoch 25/100, Train Loss: -0.0490, Test Loss: -0.0859\n",
      "Epoch 26/100, Train Loss: -0.0492, Test Loss: -0.0863\n",
      "Epoch 27/100, Train Loss: -0.0495, Test Loss: -0.0868\n",
      "Epoch 28/100, Train Loss: -0.0497, Test Loss: -0.0873\n",
      "Epoch 29/100, Train Loss: -0.0500, Test Loss: -0.0878\n",
      "Epoch 30/100, Train Loss: -0.0502, Test Loss: -0.0883\n",
      "Epoch 31/100, Train Loss: -0.0504, Test Loss: -0.0887\n",
      "Epoch 32/100, Train Loss: -0.0507, Test Loss: -0.0892\n",
      "Epoch 33/100, Train Loss: -0.0509, Test Loss: -0.0896\n",
      "Epoch 34/100, Train Loss: -0.0511, Test Loss: -0.0900\n",
      "Epoch 35/100, Train Loss: -0.0513, Test Loss: -0.0904\n",
      "Epoch 36/100, Train Loss: -0.0515, Test Loss: -0.0908\n",
      "Epoch 37/100, Train Loss: -0.0517, Test Loss: -0.0912\n",
      "Epoch 38/100, Train Loss: -0.0519, Test Loss: -0.0915\n",
      "Epoch 39/100, Train Loss: -0.0522, Test Loss: -0.0918\n",
      "Epoch 40/100, Train Loss: -0.0524, Test Loss: -0.0921\n",
      "Epoch 41/100, Train Loss: -0.0526, Test Loss: -0.0925\n",
      "Epoch 42/100, Train Loss: -0.0528, Test Loss: -0.0928\n",
      "Epoch 43/100, Train Loss: -0.0531, Test Loss: -0.0931\n",
      "Epoch 44/100, Train Loss: -0.0533, Test Loss: -0.0935\n",
      "Epoch 45/100, Train Loss: -0.0535, Test Loss: -0.0938\n",
      "Epoch 46/100, Train Loss: -0.0537, Test Loss: -0.0941\n",
      "Epoch 47/100, Train Loss: -0.0540, Test Loss: -0.0943\n",
      "Epoch 48/100, Train Loss: -0.0542, Test Loss: -0.0946\n",
      "Epoch 49/100, Train Loss: -0.0544, Test Loss: -0.0949\n",
      "Epoch 50/100, Train Loss: -0.0546, Test Loss: -0.0951\n",
      "Epoch 51/100, Train Loss: -0.0548, Test Loss: -0.0953\n",
      "Epoch 52/100, Train Loss: -0.0550, Test Loss: -0.0955\n",
      "Epoch 53/100, Train Loss: -0.0552, Test Loss: -0.0958\n",
      "Epoch 54/100, Train Loss: -0.0554, Test Loss: -0.0960\n",
      "Epoch 55/100, Train Loss: -0.0556, Test Loss: -0.0962\n",
      "Epoch 56/100, Train Loss: -0.0558, Test Loss: -0.0965\n",
      "Epoch 57/100, Train Loss: -0.0561, Test Loss: -0.0967\n",
      "Epoch 58/100, Train Loss: -0.0562, Test Loss: -0.0970\n",
      "Epoch 59/100, Train Loss: -0.0564, Test Loss: -0.0972\n",
      "Epoch 60/100, Train Loss: -0.0566, Test Loss: -0.0974\n",
      "Epoch 61/100, Train Loss: -0.0568, Test Loss: -0.0976\n",
      "Epoch 62/100, Train Loss: -0.0569, Test Loss: -0.0979\n",
      "Epoch 63/100, Train Loss: -0.0571, Test Loss: -0.0981\n",
      "Epoch 64/100, Train Loss: -0.0572, Test Loss: -0.0982\n",
      "Epoch 65/100, Train Loss: -0.0573, Test Loss: -0.0984\n",
      "Epoch 66/100, Train Loss: -0.0571, Test Loss: -0.0984\n",
      "Epoch 67/100, Train Loss: -0.0571, Test Loss: -0.0983\n",
      "Epoch 68/100, Train Loss: -0.0561, Test Loss: -0.0984\n",
      "Epoch 69/100, Train Loss: -0.0558, Test Loss: -0.0980\n",
      "Epoch 70/100, Train Loss: -0.0541, Test Loss: -0.0981\n",
      "Epoch 71/100, Train Loss: -0.0551, Test Loss: -0.0968\n",
      "Epoch 72/100, Train Loss: -0.0555, Test Loss: -0.0974\n",
      "Epoch 73/100, Train Loss: -0.0571, Test Loss: -0.0976\n",
      "Epoch 74/100, Train Loss: -0.0569, Test Loss: -0.0970\n",
      "Epoch 75/100, Train Loss: -0.0570, Test Loss: -0.0968\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 0.3405, Test Loss: 0.4624\n",
      "Epoch 2/100, Train Loss: 0.2308, Test Loss: 0.2960\n",
      "Epoch 3/100, Train Loss: 0.1204, Test Loss: 0.1904\n",
      "Epoch 4/100, Train Loss: 0.0624, Test Loss: 0.0933\n",
      "Epoch 5/100, Train Loss: 0.0254, Test Loss: 0.0384\n",
      "Epoch 6/100, Train Loss: -0.0002, Test Loss: -0.0058\n",
      "Epoch 7/100, Train Loss: -0.0140, Test Loss: -0.0182\n",
      "Epoch 8/100, Train Loss: -0.0195, Test Loss: -0.0324\n",
      "Epoch 9/100, Train Loss: -0.0271, Test Loss: -0.0448\n",
      "Epoch 10/100, Train Loss: -0.0297, Test Loss: -0.0459\n",
      "Epoch 11/100, Train Loss: -0.0311, Test Loss: -0.0507\n",
      "Epoch 12/100, Train Loss: -0.0336, Test Loss: -0.0539\n",
      "Epoch 13/100, Train Loss: -0.0344, Test Loss: -0.0543\n",
      "Epoch 14/100, Train Loss: -0.0352, Test Loss: -0.0560\n",
      "Epoch 15/100, Train Loss: -0.0360, Test Loss: -0.0572\n",
      "Epoch 16/100, Train Loss: -0.0366, Test Loss: -0.0584\n",
      "Epoch 17/100, Train Loss: -0.0371, Test Loss: -0.0590\n",
      "Epoch 18/100, Train Loss: -0.0375, Test Loss: -0.0598\n",
      "Epoch 19/100, Train Loss: -0.0381, Test Loss: -0.0605\n",
      "Epoch 20/100, Train Loss: -0.0384, Test Loss: -0.0610\n",
      "Epoch 21/100, Train Loss: -0.0389, Test Loss: -0.0618\n",
      "Epoch 22/100, Train Loss: -0.0393, Test Loss: -0.0623\n",
      "Epoch 23/100, Train Loss: -0.0396, Test Loss: -0.0629\n",
      "Epoch 24/100, Train Loss: -0.0401, Test Loss: -0.0634\n",
      "Epoch 25/100, Train Loss: -0.0405, Test Loss: -0.0639\n",
      "Epoch 26/100, Train Loss: -0.0408, Test Loss: -0.0645\n",
      "Epoch 27/100, Train Loss: -0.0413, Test Loss: -0.0650\n",
      "Epoch 28/100, Train Loss: -0.0417, Test Loss: -0.0656\n",
      "Epoch 29/100, Train Loss: -0.0421, Test Loss: -0.0660\n",
      "Epoch 30/100, Train Loss: -0.0425, Test Loss: -0.0665\n",
      "Epoch 31/100, Train Loss: -0.0429, Test Loss: -0.0669\n",
      "Epoch 32/100, Train Loss: -0.0433, Test Loss: -0.0674\n",
      "Epoch 33/100, Train Loss: -0.0437, Test Loss: -0.0679\n",
      "Epoch 34/100, Train Loss: -0.0441, Test Loss: -0.0685\n",
      "Epoch 35/100, Train Loss: -0.0445, Test Loss: -0.0689\n",
      "Epoch 36/100, Train Loss: -0.0449, Test Loss: -0.0693\n",
      "Epoch 37/100, Train Loss: -0.0454, Test Loss: -0.0697\n",
      "Epoch 38/100, Train Loss: -0.0458, Test Loss: -0.0697\n",
      "Epoch 39/100, Train Loss: -0.0460, Test Loss: -0.0679\n",
      "Epoch 40/100, Train Loss: -0.0453, Test Loss: -0.0626\n",
      "Epoch 41/100, Train Loss: -0.0427, Test Loss: -0.0583\n",
      "Epoch 42/100, Train Loss: -0.0411, Test Loss: -0.0640\n",
      "Epoch 43/100, Train Loss: -0.0446, Test Loss: -0.0623\n",
      "Epoch 44/100, Train Loss: -0.0429, Test Loss: -0.0690\n",
      "Epoch 45/100, Train Loss: -0.0428, Test Loss: -0.0687\n",
      "Epoch 46/100, Train Loss: -0.0424, Test Loss: -0.0667\n",
      "Epoch 47/100, Train Loss: -0.0434, Test Loss: -0.0679\n",
      "Epoch 48/100, Train Loss: -0.0442, Test Loss: -0.0698\n",
      "Epoch 49/100, Train Loss: -0.0453, Test Loss: -0.0699\n",
      "Epoch 50/100, Train Loss: -0.0453, Test Loss: -0.0692\n",
      "Epoch 51/100, Train Loss: -0.0469, Test Loss: -0.0690\n",
      "Epoch 52/100, Train Loss: -0.0477, Test Loss: -0.0703\n",
      "Epoch 53/100, Train Loss: -0.0483, Test Loss: -0.0704\n",
      "Epoch 54/100, Train Loss: -0.0486, Test Loss: -0.0715\n",
      "Epoch 55/100, Train Loss: -0.0488, Test Loss: -0.0716\n",
      "Epoch 56/100, Train Loss: -0.0491, Test Loss: -0.0718\n",
      "Epoch 57/100, Train Loss: -0.0493, Test Loss: -0.0717\n",
      "Epoch 58/100, Train Loss: -0.0497, Test Loss: -0.0716\n",
      "Epoch 59/100, Train Loss: -0.0498, Test Loss: -0.0702\n",
      "Epoch 60/100, Train Loss: -0.0493, Test Loss: -0.0664\n",
      "Epoch 61/100, Train Loss: -0.0476, Test Loss: -0.0652\n",
      "Epoch 62/100, Train Loss: -0.0471, Test Loss: -0.0667\n",
      "Epoch 63/100, Train Loss: -0.0487, Test Loss: -0.0707\n",
      "Epoch 64/100, Train Loss: -0.0501, Test Loss: -0.0684\n",
      "Epoch 65/100, Train Loss: -0.0496, Test Loss: -0.0703\n",
      "Epoch 66/100, Train Loss: -0.0507, Test Loss: -0.0713\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: -0.0211, Test Loss: -0.0304\n",
      "Epoch 2/100, Train Loss: -0.0248, Test Loss: -0.0556\n",
      "Epoch 3/100, Train Loss: -0.0370, Test Loss: -0.0599\n",
      "Epoch 4/100, Train Loss: -0.0419, Test Loss: -0.0588\n",
      "Epoch 5/100, Train Loss: -0.0409, Test Loss: -0.0583\n",
      "Epoch 6/100, Train Loss: -0.0426, Test Loss: -0.0640\n",
      "Epoch 7/100, Train Loss: -0.0444, Test Loss: -0.0648\n",
      "Epoch 8/100, Train Loss: -0.0439, Test Loss: -0.0645\n",
      "Epoch 9/100, Train Loss: -0.0444, Test Loss: -0.0658\n",
      "Epoch 10/100, Train Loss: -0.0454, Test Loss: -0.0661\n",
      "Epoch 11/100, Train Loss: -0.0456, Test Loss: -0.0665\n",
      "Epoch 12/100, Train Loss: -0.0462, Test Loss: -0.0674\n",
      "Epoch 13/100, Train Loss: -0.0467, Test Loss: -0.0679\n",
      "Epoch 14/100, Train Loss: -0.0471, Test Loss: -0.0686\n",
      "Epoch 15/100, Train Loss: -0.0478, Test Loss: -0.0687\n",
      "Epoch 16/100, Train Loss: -0.0482, Test Loss: -0.0692\n",
      "Epoch 17/100, Train Loss: -0.0488, Test Loss: -0.0694\n",
      "Epoch 18/100, Train Loss: -0.0493, Test Loss: -0.0694\n",
      "Epoch 19/100, Train Loss: -0.0498, Test Loss: -0.0692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100, Train Loss: -0.0503, Test Loss: -0.0695\n",
      "Epoch 21/100, Train Loss: -0.0507, Test Loss: -0.0694\n",
      "Epoch 22/100, Train Loss: -0.0512, Test Loss: -0.0685\n",
      "Epoch 23/100, Train Loss: -0.0516, Test Loss: -0.0693\n",
      "Epoch 24/100, Train Loss: -0.0520, Test Loss: -0.0691\n",
      "Epoch 25/100, Train Loss: -0.0523, Test Loss: -0.0692\n",
      "Epoch 26/100, Train Loss: -0.0527, Test Loss: -0.0696\n",
      "Epoch 27/100, Train Loss: -0.0531, Test Loss: -0.0677\n",
      "Epoch 28/100, Train Loss: -0.0532, Test Loss: -0.0686\n",
      "Epoch 29/100, Train Loss: -0.0537, Test Loss: -0.0668\n",
      "Epoch 30/100, Train Loss: -0.0534, Test Loss: -0.0628\n",
      "Epoch 31/100, Train Loss: -0.0524, Test Loss: -0.0552\n",
      "Epoch 32/100, Train Loss: -0.0488, Test Loss: -0.0595\n",
      "Epoch 33/100, Train Loss: -0.0447, Test Loss: -0.0422\n",
      "Epoch 34/100, Train Loss: -0.0372, Test Loss: -0.0602\n",
      "Epoch 35/100, Train Loss: -0.0409, Test Loss: -0.0427\n",
      "Epoch 36/100, Train Loss: -0.0410, Test Loss: -0.0386\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -639     |\n",
      "| time/              |          |\n",
      "|    fps             | 358      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -691     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 343      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 11       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.589    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00802  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 177      |\n",
      "|    policy_objective       | 0.0211   |\n",
      "|    std                    | 0.452    |\n",
      "|    value_loss             | 977      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -624     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 338      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 18       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.772    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00752  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 178      |\n",
      "|    policy_objective       | 0.0191   |\n",
      "|    std                    | 0.452    |\n",
      "|    value_loss             | 908      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -598     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 331      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 24       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.817    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00778  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 179      |\n",
      "|    policy_objective       | 0.0239   |\n",
      "|    std                    | 0.451    |\n",
      "|    value_loss             | 884      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -596     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 331      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 30       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.685    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00912  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 180      |\n",
      "|    policy_objective       | 0.0251   |\n",
      "|    std                    | 0.447    |\n",
      "|    value_loss             | 1.17e+03 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -592     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 329      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 37       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.708    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00903  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 181      |\n",
      "|    policy_objective       | 0.0217   |\n",
      "|    std                    | 0.436    |\n",
      "|    value_loss             | 619      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -602     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 329      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 43       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.73     |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00803  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 182      |\n",
      "|    policy_objective       | 0.0186   |\n",
      "|    std                    | 0.446    |\n",
      "|    value_loss             | 969      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -612     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 329      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 49       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.721    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00793  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 183      |\n",
      "|    policy_objective       | 0.0175   |\n",
      "|    std                    | 0.445    |\n",
      "|    value_loss             | 1.26e+03 |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-127.65472502240445\n",
      "------------------------------\n",
      "round: 23\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.0307, Test Loss: 0.0772\n",
      "Epoch 2/100, Train Loss: 0.0179, Test Loss: -0.0325\n",
      "Epoch 3/100, Train Loss: 0.0042, Test Loss: -0.0423\n",
      "Epoch 4/100, Train Loss: -0.0373, Test Loss: -0.0456\n",
      "Epoch 5/100, Train Loss: -0.0220, Test Loss: -0.0655\n",
      "Epoch 6/100, Train Loss: -0.0418, Test Loss: -0.0580\n",
      "Epoch 7/100, Train Loss: -0.0369, Test Loss: -0.0736\n",
      "Epoch 8/100, Train Loss: -0.0474, Test Loss: -0.0673\n",
      "Epoch 9/100, Train Loss: -0.0422, Test Loss: -0.0736\n",
      "Epoch 10/100, Train Loss: -0.0485, Test Loss: -0.0744\n",
      "Epoch 11/100, Train Loss: -0.0467, Test Loss: -0.0764\n",
      "Epoch 12/100, Train Loss: -0.0496, Test Loss: -0.0773\n",
      "Epoch 13/100, Train Loss: -0.0487, Test Loss: -0.0775\n",
      "Epoch 14/100, Train Loss: -0.0501, Test Loss: -0.0776\n",
      "Epoch 15/100, Train Loss: -0.0501, Test Loss: -0.0780\n",
      "Epoch 16/100, Train Loss: -0.0509, Test Loss: -0.0790\n",
      "Epoch 17/100, Train Loss: -0.0508, Test Loss: -0.0793\n",
      "Epoch 18/100, Train Loss: -0.0514, Test Loss: -0.0796\n",
      "Epoch 19/100, Train Loss: -0.0516, Test Loss: -0.0798\n",
      "Epoch 20/100, Train Loss: -0.0520, Test Loss: -0.0805\n",
      "Epoch 21/100, Train Loss: -0.0521, Test Loss: -0.0807\n",
      "Epoch 22/100, Train Loss: -0.0524, Test Loss: -0.0807\n",
      "Epoch 23/100, Train Loss: -0.0526, Test Loss: -0.0810\n",
      "Epoch 24/100, Train Loss: -0.0529, Test Loss: -0.0813\n",
      "Epoch 25/100, Train Loss: -0.0531, Test Loss: -0.0815\n",
      "Epoch 26/100, Train Loss: -0.0534, Test Loss: -0.0816\n",
      "Epoch 27/100, Train Loss: -0.0536, Test Loss: -0.0818\n",
      "Epoch 28/100, Train Loss: -0.0539, Test Loss: -0.0819\n",
      "Epoch 29/100, Train Loss: -0.0540, Test Loss: -0.0821\n",
      "Epoch 30/100, Train Loss: -0.0542, Test Loss: -0.0823\n",
      "Epoch 31/100, Train Loss: -0.0544, Test Loss: -0.0825\n",
      "Epoch 32/100, Train Loss: -0.0546, Test Loss: -0.0826\n",
      "Epoch 33/100, Train Loss: -0.0548, Test Loss: -0.0827\n",
      "Epoch 34/100, Train Loss: -0.0550, Test Loss: -0.0829\n",
      "Epoch 35/100, Train Loss: -0.0552, Test Loss: -0.0831\n",
      "Epoch 36/100, Train Loss: -0.0553, Test Loss: -0.0832\n",
      "Epoch 37/100, Train Loss: -0.0555, Test Loss: -0.0833\n",
      "Epoch 38/100, Train Loss: -0.0557, Test Loss: -0.0834\n",
      "Epoch 39/100, Train Loss: -0.0558, Test Loss: -0.0835\n",
      "Epoch 40/100, Train Loss: -0.0560, Test Loss: -0.0837\n",
      "Epoch 41/100, Train Loss: -0.0562, Test Loss: -0.0838\n",
      "Epoch 42/100, Train Loss: -0.0563, Test Loss: -0.0839\n",
      "Epoch 43/100, Train Loss: -0.0564, Test Loss: -0.0838\n",
      "Epoch 44/100, Train Loss: -0.0565, Test Loss: -0.0834\n",
      "Epoch 45/100, Train Loss: -0.0566, Test Loss: -0.0823\n",
      "Epoch 46/100, Train Loss: -0.0565, Test Loss: -0.0807\n",
      "Epoch 47/100, Train Loss: -0.0565, Test Loss: -0.0812\n",
      "Epoch 48/100, Train Loss: -0.0568, Test Loss: -0.0842\n",
      "Epoch 49/100, Train Loss: -0.0566, Test Loss: -0.0743\n",
      "Epoch 50/100, Train Loss: -0.0527, Test Loss: -0.0651\n",
      "Epoch 51/100, Train Loss: -0.0529, Test Loss: -0.0799\n",
      "Epoch 52/100, Train Loss: -0.0511, Test Loss: -0.0684\n",
      "Epoch 53/100, Train Loss: -0.0529, Test Loss: -0.0649\n",
      "Epoch 54/100, Train Loss: -0.0507, Test Loss: -0.0834\n",
      "Epoch 55/100, Train Loss: -0.0516, Test Loss: -0.0736\n",
      "Epoch 56/100, Train Loss: -0.0531, Test Loss: -0.0749\n",
      "Epoch 57/100, Train Loss: -0.0528, Test Loss: -0.0751\n",
      "Epoch 58/100, Train Loss: -0.0517, Test Loss: -0.0797\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: -0.0210, Test Loss: 0.0116\n",
      "Epoch 2/100, Train Loss: -0.0089, Test Loss: -0.0443\n",
      "Epoch 3/100, Train Loss: -0.0419, Test Loss: -0.0411\n",
      "Epoch 4/100, Train Loss: -0.0352, Test Loss: -0.0579\n",
      "Epoch 5/100, Train Loss: -0.0452, Test Loss: -0.0462\n",
      "Epoch 6/100, Train Loss: -0.0448, Test Loss: -0.0653\n",
      "Epoch 7/100, Train Loss: -0.0504, Test Loss: -0.0594\n",
      "Epoch 8/100, Train Loss: -0.0478, Test Loss: -0.0631\n",
      "Epoch 9/100, Train Loss: -0.0521, Test Loss: -0.0621\n",
      "Epoch 10/100, Train Loss: -0.0518, Test Loss: -0.0600\n",
      "Epoch 11/100, Train Loss: -0.0536, Test Loss: -0.0597\n",
      "Epoch 12/100, Train Loss: -0.0537, Test Loss: -0.0568\n",
      "Epoch 13/100, Train Loss: -0.0552, Test Loss: -0.0528\n",
      "Epoch 14/100, Train Loss: -0.0555, Test Loss: -0.0480\n",
      "Epoch 15/100, Train Loss: -0.0565, Test Loss: -0.0437\n",
      "Epoch 16/100, Train Loss: -0.0570, Test Loss: -0.0376\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 0.0564, Test Loss: -0.0261\n",
      "Epoch 2/100, Train Loss: 0.0573, Test Loss: -0.0527\n",
      "Epoch 3/100, Train Loss: -0.0137, Test Loss: -0.0324\n",
      "Epoch 4/100, Train Loss: -0.0018, Test Loss: -0.0498\n",
      "Epoch 5/100, Train Loss: -0.0224, Test Loss: -0.0662\n",
      "Epoch 6/100, Train Loss: -0.0279, Test Loss: -0.0496\n",
      "Epoch 7/100, Train Loss: -0.0269, Test Loss: -0.0604\n",
      "Epoch 8/100, Train Loss: -0.0327, Test Loss: -0.0702\n",
      "Epoch 9/100, Train Loss: -0.0349, Test Loss: -0.0680\n",
      "Epoch 10/100, Train Loss: -0.0341, Test Loss: -0.0667\n",
      "Epoch 11/100, Train Loss: -0.0349, Test Loss: -0.0687\n",
      "Epoch 12/100, Train Loss: -0.0365, Test Loss: -0.0688\n",
      "Epoch 13/100, Train Loss: -0.0371, Test Loss: -0.0670\n",
      "Epoch 14/100, Train Loss: -0.0371, Test Loss: -0.0664\n",
      "Epoch 15/100, Train Loss: -0.0375, Test Loss: -0.0676\n",
      "Epoch 16/100, Train Loss: -0.0382, Test Loss: -0.0690\n",
      "Epoch 17/100, Train Loss: -0.0387, Test Loss: -0.0698\n",
      "Epoch 18/100, Train Loss: -0.0390, Test Loss: -0.0704\n",
      "Epoch 19/100, Train Loss: -0.0394, Test Loss: -0.0709\n",
      "Epoch 20/100, Train Loss: -0.0399, Test Loss: -0.0713\n",
      "Epoch 21/100, Train Loss: -0.0403, Test Loss: -0.0718\n",
      "Epoch 22/100, Train Loss: -0.0408, Test Loss: -0.0723\n",
      "Epoch 23/100, Train Loss: -0.0413, Test Loss: -0.0728\n",
      "Epoch 24/100, Train Loss: -0.0417, Test Loss: -0.0735\n",
      "Epoch 25/100, Train Loss: -0.0422, Test Loss: -0.0742\n",
      "Epoch 26/100, Train Loss: -0.0427, Test Loss: -0.0749\n",
      "Epoch 27/100, Train Loss: -0.0432, Test Loss: -0.0756\n",
      "Epoch 28/100, Train Loss: -0.0437, Test Loss: -0.0763\n",
      "Epoch 29/100, Train Loss: -0.0442, Test Loss: -0.0770\n",
      "Epoch 30/100, Train Loss: -0.0447, Test Loss: -0.0776\n",
      "Epoch 31/100, Train Loss: -0.0452, Test Loss: -0.0783\n",
      "Epoch 32/100, Train Loss: -0.0457, Test Loss: -0.0789\n",
      "Epoch 33/100, Train Loss: -0.0462, Test Loss: -0.0796\n",
      "Epoch 34/100, Train Loss: -0.0467, Test Loss: -0.0802\n",
      "Epoch 35/100, Train Loss: -0.0472, Test Loss: -0.0808\n",
      "Epoch 36/100, Train Loss: -0.0477, Test Loss: -0.0814\n",
      "Epoch 37/100, Train Loss: -0.0482, Test Loss: -0.0821\n",
      "Epoch 38/100, Train Loss: -0.0486, Test Loss: -0.0827\n",
      "Epoch 39/100, Train Loss: -0.0491, Test Loss: -0.0833\n",
      "Epoch 40/100, Train Loss: -0.0496, Test Loss: -0.0839\n",
      "Epoch 41/100, Train Loss: -0.0500, Test Loss: -0.0845\n",
      "Epoch 42/100, Train Loss: -0.0504, Test Loss: -0.0850\n",
      "Epoch 43/100, Train Loss: -0.0507, Test Loss: -0.0856\n",
      "Epoch 44/100, Train Loss: -0.0510, Test Loss: -0.0860\n",
      "Epoch 45/100, Train Loss: -0.0513, Test Loss: -0.0863\n",
      "Epoch 46/100, Train Loss: -0.0516, Test Loss: -0.0868\n",
      "Epoch 47/100, Train Loss: -0.0519, Test Loss: -0.0870\n",
      "Epoch 48/100, Train Loss: -0.0516, Test Loss: -0.0875\n",
      "Epoch 49/100, Train Loss: -0.0519, Test Loss: -0.0837\n",
      "Epoch 50/100, Train Loss: -0.0495, Test Loss: -0.0833\n",
      "Epoch 51/100, Train Loss: -0.0443, Test Loss: -0.0742\n",
      "Epoch 52/100, Train Loss: -0.0362, Test Loss: -0.0793\n",
      "Epoch 53/100, Train Loss: -0.0353, Test Loss: -0.0739\n",
      "Epoch 54/100, Train Loss: -0.0463, Test Loss: -0.0753\n",
      "Epoch 55/100, Train Loss: -0.0470, Test Loss: -0.0764\n",
      "Epoch 56/100, Train Loss: -0.0445, Test Loss: -0.0825\n",
      "Epoch 57/100, Train Loss: -0.0511, Test Loss: -0.0782\n",
      "Epoch 58/100, Train Loss: -0.0499, Test Loss: -0.0794\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -683     |\n",
      "| time/              |          |\n",
      "|    fps             | 352      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -598     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 340      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 12       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.745    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00733  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 185      |\n",
      "|    policy_objective       | 0.0219   |\n",
      "|    std                    | 0.456    |\n",
      "|    value_loss             | 1.05e+03 |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -578     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 332      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 18       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.76     |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.0088   |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 186      |\n",
      "|    policy_objective       | 0.0196   |\n",
      "|    std                    | 0.459    |\n",
      "|    value_loss             | 1.09e+03 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -571     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 329      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 24       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.795    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00851  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 187      |\n",
      "|    policy_objective       | 0.0258   |\n",
      "|    std                    | 0.468    |\n",
      "|    value_loss             | 1.09e+03 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -576     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 330      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 31       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.757    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00836  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 188      |\n",
      "|    policy_objective       | 0.0201   |\n",
      "|    std                    | 0.454    |\n",
      "|    value_loss             | 1.16e+03 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -574     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 330      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 37       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.621    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00637  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 189      |\n",
      "|    policy_objective       | 0.0147   |\n",
      "|    std                    | 0.447    |\n",
      "|    value_loss             | 1.07e+03 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -570     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 330      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 43       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.768    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00905  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 190      |\n",
      "|    policy_objective       | 0.0197   |\n",
      "|    std                    | 0.44     |\n",
      "|    value_loss             | 865      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -562     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 330      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 49       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.799    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00966  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 191      |\n",
      "|    policy_objective       | 0.0198   |\n",
      "|    std                    | 0.439    |\n",
      "|    value_loss             | 848      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-155.7382440895075\n",
      "------------------------------\n",
      "round: 24\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.0889, Test Loss: 0.0550\n",
      "Epoch 2/100, Train Loss: 0.0730, Test Loss: -0.0655\n",
      "Epoch 3/100, Train Loss: 0.0070, Test Loss: 0.0180\n",
      "Epoch 4/100, Train Loss: 0.0010, Test Loss: -0.0740\n",
      "Epoch 5/100, Train Loss: -0.0142, Test Loss: -0.0598\n",
      "Epoch 6/100, Train Loss: -0.0272, Test Loss: -0.0676\n",
      "Epoch 7/100, Train Loss: -0.0287, Test Loss: -0.0666\n",
      "Epoch 8/100, Train Loss: -0.0372, Test Loss: -0.0787\n",
      "Epoch 9/100, Train Loss: -0.0362, Test Loss: -0.0802\n",
      "Epoch 10/100, Train Loss: -0.0412, Test Loss: -0.0809\n",
      "Epoch 11/100, Train Loss: -0.0413, Test Loss: -0.0778\n",
      "Epoch 12/100, Train Loss: -0.0434, Test Loss: -0.0841\n",
      "Epoch 13/100, Train Loss: -0.0440, Test Loss: -0.0829\n",
      "Epoch 14/100, Train Loss: -0.0448, Test Loss: -0.0834\n",
      "Epoch 15/100, Train Loss: -0.0454, Test Loss: -0.0825\n",
      "Epoch 16/100, Train Loss: -0.0460, Test Loss: -0.0846\n",
      "Epoch 17/100, Train Loss: -0.0462, Test Loss: -0.0846\n",
      "Epoch 18/100, Train Loss: -0.0468, Test Loss: -0.0842\n",
      "Epoch 19/100, Train Loss: -0.0469, Test Loss: -0.0848\n",
      "Epoch 20/100, Train Loss: -0.0474, Test Loss: -0.0855\n",
      "Epoch 21/100, Train Loss: -0.0475, Test Loss: -0.0858\n",
      "Epoch 22/100, Train Loss: -0.0479, Test Loss: -0.0860\n",
      "Epoch 23/100, Train Loss: -0.0481, Test Loss: -0.0863\n",
      "Epoch 24/100, Train Loss: -0.0483, Test Loss: -0.0866\n",
      "Epoch 25/100, Train Loss: -0.0485, Test Loss: -0.0868\n",
      "Epoch 26/100, Train Loss: -0.0488, Test Loss: -0.0869\n",
      "Epoch 27/100, Train Loss: -0.0489, Test Loss: -0.0873\n",
      "Epoch 28/100, Train Loss: -0.0491, Test Loss: -0.0875\n",
      "Epoch 29/100, Train Loss: -0.0493, Test Loss: -0.0877\n",
      "Epoch 30/100, Train Loss: -0.0495, Test Loss: -0.0880\n",
      "Epoch 31/100, Train Loss: -0.0497, Test Loss: -0.0882\n",
      "Epoch 32/100, Train Loss: -0.0499, Test Loss: -0.0883\n",
      "Epoch 33/100, Train Loss: -0.0501, Test Loss: -0.0885\n",
      "Epoch 34/100, Train Loss: -0.0502, Test Loss: -0.0887\n",
      "Epoch 35/100, Train Loss: -0.0504, Test Loss: -0.0888\n",
      "Epoch 36/100, Train Loss: -0.0506, Test Loss: -0.0890\n",
      "Epoch 37/100, Train Loss: -0.0507, Test Loss: -0.0891\n",
      "Epoch 38/100, Train Loss: -0.0509, Test Loss: -0.0892\n",
      "Epoch 39/100, Train Loss: -0.0510, Test Loss: -0.0894\n",
      "Epoch 40/100, Train Loss: -0.0512, Test Loss: -0.0895\n",
      "Epoch 41/100, Train Loss: -0.0513, Test Loss: -0.0896\n",
      "Epoch 42/100, Train Loss: -0.0514, Test Loss: -0.0898\n",
      "Epoch 43/100, Train Loss: -0.0516, Test Loss: -0.0900\n",
      "Epoch 44/100, Train Loss: -0.0517, Test Loss: -0.0901\n",
      "Epoch 45/100, Train Loss: -0.0518, Test Loss: -0.0902\n",
      "Epoch 46/100, Train Loss: -0.0519, Test Loss: -0.0904\n",
      "Epoch 47/100, Train Loss: -0.0521, Test Loss: -0.0905\n",
      "Epoch 48/100, Train Loss: -0.0522, Test Loss: -0.0906\n",
      "Epoch 49/100, Train Loss: -0.0523, Test Loss: -0.0908\n",
      "Epoch 50/100, Train Loss: -0.0524, Test Loss: -0.0910\n",
      "Epoch 51/100, Train Loss: -0.0525, Test Loss: -0.0912\n",
      "Epoch 52/100, Train Loss: -0.0526, Test Loss: -0.0914\n",
      "Epoch 53/100, Train Loss: -0.0527, Test Loss: -0.0915\n",
      "Epoch 54/100, Train Loss: -0.0528, Test Loss: -0.0917\n",
      "Epoch 55/100, Train Loss: -0.0528, Test Loss: -0.0917\n",
      "Epoch 56/100, Train Loss: -0.0528, Test Loss: -0.0924\n",
      "Epoch 57/100, Train Loss: -0.0529, Test Loss: -0.0913\n",
      "Epoch 58/100, Train Loss: -0.0525, Test Loss: -0.0883\n",
      "Epoch 59/100, Train Loss: -0.0517, Test Loss: -0.0903\n",
      "Epoch 60/100, Train Loss: -0.0520, Test Loss: -0.0868\n",
      "Epoch 61/100, Train Loss: -0.0501, Test Loss: -0.0764\n",
      "Epoch 62/100, Train Loss: -0.0464, Test Loss: -0.0873\n",
      "Epoch 63/100, Train Loss: -0.0453, Test Loss: -0.0531\n",
      "Epoch 64/100, Train Loss: -0.0397, Test Loss: -0.0544\n",
      "Epoch 65/100, Train Loss: -0.0342, Test Loss: -0.0878\n",
      "Epoch 66/100, Train Loss: -0.0443, Test Loss: -0.0745\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: -0.0163, Test Loss: -0.0816\n",
      "Epoch 2/100, Train Loss: -0.0318, Test Loss: -0.0814\n",
      "Epoch 3/100, Train Loss: -0.0405, Test Loss: -0.0737\n",
      "Epoch 4/100, Train Loss: -0.0407, Test Loss: -0.0778\n",
      "Epoch 5/100, Train Loss: -0.0438, Test Loss: -0.0754\n",
      "Epoch 6/100, Train Loss: -0.0438, Test Loss: -0.0764\n",
      "Epoch 7/100, Train Loss: -0.0452, Test Loss: -0.0752\n",
      "Epoch 8/100, Train Loss: -0.0453, Test Loss: -0.0770\n",
      "Epoch 9/100, Train Loss: -0.0466, Test Loss: -0.0772\n",
      "Epoch 10/100, Train Loss: -0.0469, Test Loss: -0.0783\n",
      "Epoch 11/100, Train Loss: -0.0477, Test Loss: -0.0790\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 0.0611, Test Loss: -0.0596\n",
      "Epoch 2/100, Train Loss: 0.0556, Test Loss: -0.0610\n",
      "Epoch 3/100, Train Loss: 0.0051, Test Loss: -0.0431\n",
      "Epoch 4/100, Train Loss: 0.0078, Test Loss: -0.0591\n",
      "Epoch 5/100, Train Loss: -0.0180, Test Loss: -0.0646\n",
      "Epoch 6/100, Train Loss: -0.0180, Test Loss: -0.0581\n",
      "Epoch 7/100, Train Loss: -0.0228, Test Loss: -0.0651\n",
      "Epoch 8/100, Train Loss: -0.0297, Test Loss: -0.0634\n",
      "Epoch 9/100, Train Loss: -0.0302, Test Loss: -0.0621\n",
      "Epoch 10/100, Train Loss: -0.0320, Test Loss: -0.0641\n",
      "Epoch 11/100, Train Loss: -0.0344, Test Loss: -0.0645\n",
      "Epoch 12/100, Train Loss: -0.0353, Test Loss: -0.0655\n",
      "Epoch 13/100, Train Loss: -0.0361, Test Loss: -0.0667\n",
      "Epoch 14/100, Train Loss: -0.0372, Test Loss: -0.0680\n",
      "Epoch 15/100, Train Loss: -0.0382, Test Loss: -0.0690\n",
      "Epoch 16/100, Train Loss: -0.0392, Test Loss: -0.0703\n",
      "Epoch 17/100, Train Loss: -0.0399, Test Loss: -0.0711\n",
      "Epoch 18/100, Train Loss: -0.0406, Test Loss: -0.0726\n",
      "Epoch 19/100, Train Loss: -0.0412, Test Loss: -0.0741\n",
      "Epoch 20/100, Train Loss: -0.0421, Test Loss: -0.0736\n",
      "Epoch 21/100, Train Loss: -0.0414, Test Loss: -0.0745\n",
      "Epoch 22/100, Train Loss: -0.0425, Test Loss: -0.0731\n",
      "Epoch 23/100, Train Loss: -0.0412, Test Loss: -0.0719\n",
      "Epoch 24/100, Train Loss: -0.0419, Test Loss: -0.0668\n",
      "Epoch 25/100, Train Loss: -0.0413, Test Loss: -0.0693\n",
      "Epoch 26/100, Train Loss: -0.0428, Test Loss: -0.0773\n",
      "Epoch 27/100, Train Loss: -0.0443, Test Loss: -0.0785\n",
      "Epoch 28/100, Train Loss: -0.0451, Test Loss: -0.0777\n",
      "Epoch 29/100, Train Loss: -0.0454, Test Loss: -0.0793\n",
      "Epoch 30/100, Train Loss: -0.0459, Test Loss: -0.0814\n",
      "Epoch 31/100, Train Loss: -0.0465, Test Loss: -0.0788\n",
      "Epoch 32/100, Train Loss: -0.0467, Test Loss: -0.0830\n",
      "Epoch 33/100, Train Loss: -0.0469, Test Loss: -0.0772\n",
      "Epoch 34/100, Train Loss: -0.0471, Test Loss: -0.0833\n",
      "Epoch 35/100, Train Loss: -0.0471, Test Loss: -0.0774\n",
      "Epoch 36/100, Train Loss: -0.0471, Test Loss: -0.0836\n",
      "Epoch 37/100, Train Loss: -0.0475, Test Loss: -0.0852\n",
      "Epoch 38/100, Train Loss: -0.0486, Test Loss: -0.0856\n",
      "Epoch 39/100, Train Loss: -0.0488, Test Loss: -0.0872\n",
      "Epoch 40/100, Train Loss: -0.0491, Test Loss: -0.0854\n",
      "Epoch 41/100, Train Loss: -0.0490, Test Loss: -0.0854\n",
      "Epoch 42/100, Train Loss: -0.0488, Test Loss: -0.0880\n",
      "Epoch 43/100, Train Loss: -0.0495, Test Loss: -0.0846\n",
      "Epoch 44/100, Train Loss: -0.0474, Test Loss: -0.0879\n",
      "Epoch 45/100, Train Loss: -0.0491, Test Loss: -0.0827\n",
      "Epoch 46/100, Train Loss: -0.0467, Test Loss: -0.0870\n",
      "Epoch 47/100, Train Loss: -0.0472, Test Loss: -0.0840\n",
      "Epoch 48/100, Train Loss: -0.0481, Test Loss: -0.0764\n",
      "Epoch 49/100, Train Loss: -0.0471, Test Loss: -0.0802\n",
      "Epoch 50/100, Train Loss: -0.0475, Test Loss: -0.0799\n",
      "Epoch 51/100, Train Loss: -0.0483, Test Loss: -0.0852\n",
      "Epoch 52/100, Train Loss: -0.0489, Test Loss: -0.0852\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -551     |\n",
      "| time/              |          |\n",
      "|    fps             | 295      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -617     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 296      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 13       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.713    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00708  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 193      |\n",
      "|    policy_objective       | 0.0159   |\n",
      "|    std                    | 0.433    |\n",
      "|    value_loss             | 638      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -534     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 291      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 21       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.812    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00972  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 194      |\n",
      "|    policy_objective       | 0.0194   |\n",
      "|    std                    | 0.43     |\n",
      "|    value_loss             | 501      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -530     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 291      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 28       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.781    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00931  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 195      |\n",
      "|    policy_objective       | 0.0199   |\n",
      "|    std                    | 0.427    |\n",
      "|    value_loss             | 929      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -530     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 299      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 34       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.822    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00838  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 196      |\n",
      "|    policy_objective       | 0.0211   |\n",
      "|    std                    | 0.427    |\n",
      "|    value_loss             | 808      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -513     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 298      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 41       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.713    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.008    |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 197      |\n",
      "|    policy_objective       | 0.0223   |\n",
      "|    std                    | 0.429    |\n",
      "|    value_loss             | 1.12e+03 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -511     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 291      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 49       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.748    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00923  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 198      |\n",
      "|    policy_objective       | 0.0207   |\n",
      "|    std                    | 0.42     |\n",
      "|    value_loss             | 989      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -518     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 291      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 56       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.862    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00843  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 199      |\n",
      "|    policy_objective       | 0.0228   |\n",
      "|    std                    | 0.417    |\n",
      "|    value_loss             | 765      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-234.8515504431911\n",
      "------------------------------\n",
      "round: 25\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: -0.0023, Test Loss: 0.0032\n",
      "Epoch 2/100, Train Loss: -0.0056, Test Loss: -0.0299\n",
      "Epoch 3/100, Train Loss: -0.0410, Test Loss: -0.0162\n",
      "Epoch 4/100, Train Loss: -0.0279, Test Loss: -0.0672\n",
      "Epoch 5/100, Train Loss: -0.0448, Test Loss: -0.0487\n",
      "Epoch 6/100, Train Loss: -0.0389, Test Loss: -0.0637\n",
      "Epoch 7/100, Train Loss: -0.0468, Test Loss: -0.0740\n",
      "Epoch 8/100, Train Loss: -0.0456, Test Loss: -0.0750\n",
      "Epoch 9/100, Train Loss: -0.0485, Test Loss: -0.0781\n",
      "Epoch 10/100, Train Loss: -0.0486, Test Loss: -0.0783\n",
      "Epoch 11/100, Train Loss: -0.0509, Test Loss: -0.0826\n",
      "Epoch 12/100, Train Loss: -0.0509, Test Loss: -0.0845\n",
      "Epoch 13/100, Train Loss: -0.0528, Test Loss: -0.0842\n",
      "Epoch 14/100, Train Loss: -0.0529, Test Loss: -0.0882\n",
      "Epoch 15/100, Train Loss: -0.0544, Test Loss: -0.0894\n",
      "Epoch 16/100, Train Loss: -0.0548, Test Loss: -0.0916\n",
      "Epoch 17/100, Train Loss: -0.0557, Test Loss: -0.0926\n",
      "Epoch 18/100, Train Loss: -0.0565, Test Loss: -0.0936\n",
      "Epoch 19/100, Train Loss: -0.0569, Test Loss: -0.0955\n",
      "Epoch 20/100, Train Loss: -0.0575, Test Loss: -0.0961\n",
      "Epoch 21/100, Train Loss: -0.0580, Test Loss: -0.0975\n",
      "Epoch 22/100, Train Loss: -0.0584, Test Loss: -0.0978\n",
      "Epoch 23/100, Train Loss: -0.0587, Test Loss: -0.0988\n",
      "Epoch 24/100, Train Loss: -0.0588, Test Loss: -0.0981\n",
      "Epoch 25/100, Train Loss: -0.0589, Test Loss: -0.0987\n",
      "Epoch 26/100, Train Loss: -0.0590, Test Loss: -0.0989\n",
      "Epoch 27/100, Train Loss: -0.0591, Test Loss: -0.0994\n",
      "Epoch 28/100, Train Loss: -0.0593, Test Loss: -0.0999\n",
      "Epoch 29/100, Train Loss: -0.0595, Test Loss: -0.1013\n",
      "Epoch 30/100, Train Loss: -0.0599, Test Loss: -0.1020\n",
      "Epoch 31/100, Train Loss: -0.0602, Test Loss: -0.1027\n",
      "Epoch 32/100, Train Loss: -0.0606, Test Loss: -0.1029\n",
      "Epoch 33/100, Train Loss: -0.0608, Test Loss: -0.1035\n",
      "Epoch 34/100, Train Loss: -0.0610, Test Loss: -0.1034\n",
      "Epoch 35/100, Train Loss: -0.0612, Test Loss: -0.1035\n",
      "Epoch 36/100, Train Loss: -0.0613, Test Loss: -0.1039\n",
      "Epoch 37/100, Train Loss: -0.0615, Test Loss: -0.1039\n",
      "Epoch 38/100, Train Loss: -0.0617, Test Loss: -0.1036\n",
      "Epoch 39/100, Train Loss: -0.0617, Test Loss: -0.1034\n",
      "Epoch 40/100, Train Loss: -0.0617, Test Loss: -0.1044\n",
      "Epoch 41/100, Train Loss: -0.0614, Test Loss: -0.1020\n",
      "Epoch 42/100, Train Loss: -0.0608, Test Loss: -0.0997\n",
      "Epoch 43/100, Train Loss: -0.0601, Test Loss: -0.1043\n",
      "Epoch 44/100, Train Loss: -0.0599, Test Loss: -0.0866\n",
      "Epoch 45/100, Train Loss: -0.0562, Test Loss: -0.1029\n",
      "Epoch 46/100, Train Loss: -0.0572, Test Loss: -0.0887\n",
      "Epoch 47/100, Train Loss: -0.0557, Test Loss: -0.0874\n",
      "Epoch 48/100, Train Loss: -0.0580, Test Loss: -0.1024\n",
      "Epoch 49/100, Train Loss: -0.0608, Test Loss: -0.0999\n",
      "Epoch 50/100, Train Loss: -0.0608, Test Loss: -0.0849\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: -0.0471, Test Loss: 0.0570\n",
      "Epoch 2/100, Train Loss: -0.0520, Test Loss: 0.1069\n",
      "Epoch 3/100, Train Loss: -0.0561, Test Loss: 0.1712\n",
      "Epoch 4/100, Train Loss: -0.0588, Test Loss: 0.2719\n",
      "Epoch 5/100, Train Loss: -0.0626, Test Loss: 0.4247\n",
      "Epoch 6/100, Train Loss: -0.0644, Test Loss: 0.6188\n",
      "Epoch 7/100, Train Loss: -0.0675, Test Loss: 0.8672\n",
      "Epoch 8/100, Train Loss: -0.0694, Test Loss: 1.1387\n",
      "Epoch 9/100, Train Loss: -0.0712, Test Loss: 1.4282\n",
      "Epoch 10/100, Train Loss: -0.0731, Test Loss: 1.7214\n",
      "Epoch 11/100, Train Loss: -0.0740, Test Loss: 2.0022\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: -0.0107, Test Loss: 0.0363\n",
      "Epoch 2/100, Train Loss: -0.0096, Test Loss: 0.0180\n",
      "Epoch 3/100, Train Loss: -0.0453, Test Loss: 0.0313\n",
      "Epoch 4/100, Train Loss: -0.0345, Test Loss: -0.0041\n",
      "Epoch 5/100, Train Loss: -0.0520, Test Loss: -0.0083\n",
      "Epoch 6/100, Train Loss: -0.0469, Test Loss: -0.0079\n",
      "Epoch 7/100, Train Loss: -0.0529, Test Loss: -0.0131\n",
      "Epoch 8/100, Train Loss: -0.0539, Test Loss: -0.0029\n",
      "Epoch 9/100, Train Loss: -0.0541, Test Loss: -0.0050\n",
      "Epoch 10/100, Train Loss: -0.0568, Test Loss: 0.0047\n",
      "Epoch 11/100, Train Loss: -0.0566, Test Loss: 0.0130\n",
      "Epoch 12/100, Train Loss: -0.0585, Test Loss: 0.0273\n",
      "Epoch 13/100, Train Loss: -0.0592, Test Loss: 0.0453\n",
      "Epoch 14/100, Train Loss: -0.0605, Test Loss: 0.0663\n",
      "Epoch 15/100, Train Loss: -0.0616, Test Loss: 0.0955\n",
      "Epoch 16/100, Train Loss: -0.0627, Test Loss: 0.1332\n",
      "Epoch 17/100, Train Loss: -0.0640, Test Loss: 0.1783\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -382     |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -537     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 336      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 12       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.742    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00752  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 201      |\n",
      "|    policy_objective       | 0.0163   |\n",
      "|    std                    | 0.415    |\n",
      "|    value_loss             | 813      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -491     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 334      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 18       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.69     |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00872  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 202      |\n",
      "|    policy_objective       | 0.0185   |\n",
      "|    std                    | 0.414    |\n",
      "|    value_loss             | 1.04e+03 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -496     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 334      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 24       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.759    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00918  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 203      |\n",
      "|    policy_objective       | 0.0202   |\n",
      "|    std                    | 0.407    |\n",
      "|    value_loss             | 847      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -496     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 337      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 30       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.749    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.0078   |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 204      |\n",
      "|    policy_objective       | 0.0162   |\n",
      "|    std                    | 0.403    |\n",
      "|    value_loss             | 770      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -506     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 338      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 36       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.813    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00835  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 205      |\n",
      "|    policy_objective       | 0.0207   |\n",
      "|    std                    | 0.401    |\n",
      "|    value_loss             | 616      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -497     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 332      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 43       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.719    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00905  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 206      |\n",
      "|    policy_objective       | 0.0177   |\n",
      "|    std                    | 0.403    |\n",
      "|    value_loss             | 1.02e+03 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -488     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 331      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 49       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.795    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00765  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 207      |\n",
      "|    policy_objective       | 0.0185   |\n",
      "|    std                    | 0.4      |\n",
      "|    value_loss             | 740      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-152.1133960939478\n",
      "------------------------------\n",
      "round: 26\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.1926, Test Loss: 1.4406\n",
      "Epoch 2/100, Train Loss: 0.2028, Test Loss: 0.9684\n",
      "Epoch 3/100, Train Loss: 0.0560, Test Loss: 0.8137\n",
      "Epoch 4/100, Train Loss: 0.0593, Test Loss: 0.5516\n",
      "Epoch 5/100, Train Loss: 0.0149, Test Loss: 0.4485\n",
      "Epoch 6/100, Train Loss: -0.0031, Test Loss: 0.3205\n",
      "Epoch 7/100, Train Loss: -0.0100, Test Loss: 0.2551\n",
      "Epoch 8/100, Train Loss: -0.0272, Test Loss: 0.2020\n",
      "Epoch 9/100, Train Loss: -0.0255, Test Loss: 0.1716\n",
      "Epoch 10/100, Train Loss: -0.0351, Test Loss: 0.1311\n",
      "Epoch 11/100, Train Loss: -0.0372, Test Loss: 0.1126\n",
      "Epoch 12/100, Train Loss: -0.0394, Test Loss: 0.0953\n",
      "Epoch 13/100, Train Loss: -0.0404, Test Loss: 0.0815\n",
      "Epoch 14/100, Train Loss: -0.0440, Test Loss: 0.0698\n",
      "Epoch 15/100, Train Loss: -0.0434, Test Loss: 0.0608\n",
      "Epoch 16/100, Train Loss: -0.0450, Test Loss: 0.0533\n",
      "Epoch 17/100, Train Loss: -0.0457, Test Loss: 0.0465\n",
      "Epoch 18/100, Train Loss: -0.0467, Test Loss: 0.0415\n",
      "Epoch 19/100, Train Loss: -0.0469, Test Loss: 0.0363\n",
      "Epoch 20/100, Train Loss: -0.0477, Test Loss: 0.0331\n",
      "Epoch 21/100, Train Loss: -0.0482, Test Loss: 0.0297\n",
      "Epoch 22/100, Train Loss: -0.0486, Test Loss: 0.0261\n",
      "Epoch 23/100, Train Loss: -0.0490, Test Loss: 0.0231\n",
      "Epoch 24/100, Train Loss: -0.0494, Test Loss: 0.0208\n",
      "Epoch 25/100, Train Loss: -0.0498, Test Loss: 0.0187\n",
      "Epoch 26/100, Train Loss: -0.0501, Test Loss: 0.0165\n",
      "Epoch 27/100, Train Loss: -0.0505, Test Loss: 0.0144\n",
      "Epoch 28/100, Train Loss: -0.0508, Test Loss: 0.0124\n",
      "Epoch 29/100, Train Loss: -0.0511, Test Loss: 0.0105\n",
      "Epoch 30/100, Train Loss: -0.0515, Test Loss: 0.0091\n",
      "Epoch 31/100, Train Loss: -0.0518, Test Loss: 0.0076\n",
      "Epoch 32/100, Train Loss: -0.0521, Test Loss: 0.0062\n",
      "Epoch 33/100, Train Loss: -0.0524, Test Loss: 0.0049\n",
      "Epoch 34/100, Train Loss: -0.0528, Test Loss: 0.0037\n",
      "Epoch 35/100, Train Loss: -0.0531, Test Loss: 0.0025\n",
      "Epoch 36/100, Train Loss: -0.0533, Test Loss: 0.0014\n",
      "Epoch 37/100, Train Loss: -0.0536, Test Loss: 0.0005\n",
      "Epoch 38/100, Train Loss: -0.0539, Test Loss: -0.0004\n",
      "Epoch 39/100, Train Loss: -0.0542, Test Loss: -0.0012\n",
      "Epoch 40/100, Train Loss: -0.0545, Test Loss: -0.0020\n",
      "Epoch 41/100, Train Loss: -0.0547, Test Loss: -0.0026\n",
      "Epoch 42/100, Train Loss: -0.0550, Test Loss: -0.0032\n",
      "Epoch 43/100, Train Loss: -0.0553, Test Loss: -0.0038\n",
      "Epoch 44/100, Train Loss: -0.0555, Test Loss: -0.0042\n",
      "Epoch 45/100, Train Loss: -0.0558, Test Loss: -0.0045\n",
      "Epoch 46/100, Train Loss: -0.0560, Test Loss: -0.0048\n",
      "Epoch 47/100, Train Loss: -0.0563, Test Loss: -0.0051\n",
      "Epoch 48/100, Train Loss: -0.0565, Test Loss: -0.0053\n",
      "Epoch 49/100, Train Loss: -0.0568, Test Loss: -0.0055\n",
      "Epoch 50/100, Train Loss: -0.0570, Test Loss: -0.0056\n",
      "Epoch 51/100, Train Loss: -0.0573, Test Loss: -0.0057\n",
      "Epoch 52/100, Train Loss: -0.0575, Test Loss: -0.0058\n",
      "Epoch 53/100, Train Loss: -0.0578, Test Loss: -0.0058\n",
      "Epoch 54/100, Train Loss: -0.0580, Test Loss: -0.0059\n",
      "Epoch 55/100, Train Loss: -0.0583, Test Loss: -0.0059\n",
      "Epoch 56/100, Train Loss: -0.0585, Test Loss: -0.0060\n",
      "Epoch 57/100, Train Loss: -0.0587, Test Loss: -0.0061\n",
      "Epoch 58/100, Train Loss: -0.0589, Test Loss: -0.0062\n",
      "Epoch 59/100, Train Loss: -0.0592, Test Loss: -0.0063\n",
      "Epoch 60/100, Train Loss: -0.0594, Test Loss: -0.0064\n",
      "Epoch 61/100, Train Loss: -0.0596, Test Loss: -0.0063\n",
      "Epoch 62/100, Train Loss: -0.0598, Test Loss: -0.0062\n",
      "Epoch 63/100, Train Loss: -0.0601, Test Loss: -0.0060\n",
      "Epoch 64/100, Train Loss: -0.0603, Test Loss: -0.0058\n",
      "Epoch 65/100, Train Loss: -0.0605, Test Loss: -0.0055\n",
      "Epoch 66/100, Train Loss: -0.0607, Test Loss: -0.0053\n",
      "Epoch 67/100, Train Loss: -0.0609, Test Loss: -0.0050\n",
      "Epoch 68/100, Train Loss: -0.0611, Test Loss: -0.0047\n",
      "Epoch 69/100, Train Loss: -0.0613, Test Loss: -0.0044\n",
      "Epoch 70/100, Train Loss: -0.0615, Test Loss: -0.0040\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: -0.0106, Test Loss: 0.2963\n",
      "Epoch 2/100, Train Loss: -0.0213, Test Loss: 0.2636\n",
      "Epoch 3/100, Train Loss: -0.0599, Test Loss: 0.2273\n",
      "Epoch 4/100, Train Loss: -0.0466, Test Loss: 0.1686\n",
      "Epoch 5/100, Train Loss: -0.0670, Test Loss: 0.1705\n",
      "Epoch 6/100, Train Loss: -0.0591, Test Loss: 0.1506\n",
      "Epoch 7/100, Train Loss: -0.0700, Test Loss: 0.1443\n",
      "Epoch 8/100, Train Loss: -0.0662, Test Loss: 0.1450\n",
      "Epoch 9/100, Train Loss: -0.0700, Test Loss: 0.1426\n",
      "Epoch 10/100, Train Loss: -0.0706, Test Loss: 0.1524\n",
      "Epoch 11/100, Train Loss: -0.0712, Test Loss: 0.1552\n",
      "Epoch 12/100, Train Loss: -0.0725, Test Loss: 0.1677\n",
      "Epoch 13/100, Train Loss: -0.0727, Test Loss: 0.1772\n",
      "Epoch 14/100, Train Loss: -0.0737, Test Loss: 0.1929\n",
      "Epoch 15/100, Train Loss: -0.0741, Test Loss: 0.2057\n",
      "Epoch 16/100, Train Loss: -0.0748, Test Loss: 0.2221\n",
      "Epoch 17/100, Train Loss: -0.0755, Test Loss: 0.2406\n",
      "Epoch 18/100, Train Loss: -0.0758, Test Loss: 0.2600\n",
      "Epoch 19/100, Train Loss: -0.0765, Test Loss: 0.2796\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 0.0127, Test Loss: -0.0713\n",
      "Epoch 2/100, Train Loss: -0.0167, Test Loss: -0.0808\n",
      "Epoch 3/100, Train Loss: -0.0331, Test Loss: -0.0793\n",
      "Epoch 4/100, Train Loss: -0.0404, Test Loss: -0.0777\n",
      "Epoch 5/100, Train Loss: -0.0421, Test Loss: -0.0797\n",
      "Epoch 6/100, Train Loss: -0.0454, Test Loss: -0.0784\n",
      "Epoch 7/100, Train Loss: -0.0463, Test Loss: -0.0784\n",
      "Epoch 8/100, Train Loss: -0.0475, Test Loss: -0.0793\n",
      "Epoch 9/100, Train Loss: -0.0485, Test Loss: -0.0799\n",
      "Epoch 10/100, Train Loss: -0.0493, Test Loss: -0.0808\n",
      "Epoch 11/100, Train Loss: -0.0504, Test Loss: -0.0810\n",
      "Epoch 12/100, Train Loss: -0.0510, Test Loss: -0.0821\n",
      "Epoch 13/100, Train Loss: -0.0520, Test Loss: -0.0823\n",
      "Epoch 14/100, Train Loss: -0.0527, Test Loss: -0.0834\n",
      "Epoch 15/100, Train Loss: -0.0536, Test Loss: -0.0838\n",
      "Epoch 16/100, Train Loss: -0.0544, Test Loss: -0.0842\n",
      "Epoch 17/100, Train Loss: -0.0553, Test Loss: -0.0844\n",
      "Epoch 18/100, Train Loss: -0.0562, Test Loss: -0.0844\n",
      "Epoch 19/100, Train Loss: -0.0569, Test Loss: -0.0840\n",
      "Epoch 20/100, Train Loss: -0.0577, Test Loss: -0.0839\n",
      "Epoch 21/100, Train Loss: -0.0585, Test Loss: -0.0831\n",
      "Epoch 22/100, Train Loss: -0.0592, Test Loss: -0.0827\n",
      "Epoch 23/100, Train Loss: -0.0598, Test Loss: -0.0813\n",
      "Epoch 24/100, Train Loss: -0.0600, Test Loss: -0.0784\n",
      "Epoch 25/100, Train Loss: -0.0601, Test Loss: -0.0742\n",
      "Epoch 26/100, Train Loss: -0.0600, Test Loss: -0.0738\n",
      "Epoch 27/100, Train Loss: -0.0608, Test Loss: -0.0750\n",
      "Epoch 28/100, Train Loss: -0.0601, Test Loss: -0.0638\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -508     |\n",
      "| time/              |          |\n",
      "|    fps             | 346      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -558     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 330      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 12       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.809    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00896  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 209      |\n",
      "|    policy_objective       | 0.0244   |\n",
      "|    std                    | 0.398    |\n",
      "|    value_loss             | 633      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -516     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 329      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 18       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.804    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00901  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 210      |\n",
      "|    policy_objective       | 0.0213   |\n",
      "|    std                    | 0.394    |\n",
      "|    value_loss             | 906      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -491     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 328      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 24       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.732    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00826  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 211      |\n",
      "|    policy_objective       | 0.0214   |\n",
      "|    std                    | 0.394    |\n",
      "|    value_loss             | 1e+03    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -487     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 329      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 31       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.813    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00949  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 212      |\n",
      "|    policy_objective       | 0.0193   |\n",
      "|    std                    | 0.393    |\n",
      "|    value_loss             | 690      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -487     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 328      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 37       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.766    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.0083   |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 213      |\n",
      "|    policy_objective       | 0.0166   |\n",
      "|    std                    | 0.391    |\n",
      "|    value_loss             | 870      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -473     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 326      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 43       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.765    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00851  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 214      |\n",
      "|    policy_objective       | 0.0231   |\n",
      "|    std                    | 0.39     |\n",
      "|    value_loss             | 1.03e+03 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -494     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 327      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 50       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.808    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00949  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 215      |\n",
      "|    policy_objective       | 0.0197   |\n",
      "|    std                    | 0.384    |\n",
      "|    value_loss             | 864      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-424.91970788645557\n",
      "------------------------------\n",
      "round: 27\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.0375, Test Loss: 0.9607\n",
      "Epoch 2/100, Train Loss: 0.0275, Test Loss: 0.6954\n",
      "Epoch 3/100, Train Loss: -0.0240, Test Loss: 0.7185\n",
      "Epoch 4/100, Train Loss: -0.0184, Test Loss: 0.6005\n",
      "Epoch 5/100, Train Loss: -0.0368, Test Loss: 0.5801\n",
      "Epoch 6/100, Train Loss: -0.0373, Test Loss: 0.5031\n",
      "Epoch 7/100, Train Loss: -0.0425, Test Loss: 0.4756\n",
      "Epoch 8/100, Train Loss: -0.0462, Test Loss: 0.4590\n",
      "Epoch 9/100, Train Loss: -0.0463, Test Loss: 0.4426\n",
      "Epoch 10/100, Train Loss: -0.0491, Test Loss: 0.4281\n",
      "Epoch 11/100, Train Loss: -0.0486, Test Loss: 0.4182\n",
      "Epoch 12/100, Train Loss: -0.0511, Test Loss: 0.4203\n",
      "Epoch 13/100, Train Loss: -0.0505, Test Loss: 0.4162\n",
      "Epoch 14/100, Train Loss: -0.0514, Test Loss: 0.4192\n",
      "Epoch 15/100, Train Loss: -0.0520, Test Loss: 0.4252\n",
      "Epoch 16/100, Train Loss: -0.0524, Test Loss: 0.4298\n",
      "Epoch 17/100, Train Loss: -0.0526, Test Loss: 0.4351\n",
      "Epoch 18/100, Train Loss: -0.0530, Test Loss: 0.4431\n",
      "Epoch 19/100, Train Loss: -0.0534, Test Loss: 0.4534\n",
      "Epoch 20/100, Train Loss: -0.0535, Test Loss: 0.4625\n",
      "Epoch 21/100, Train Loss: -0.0538, Test Loss: 0.4714\n",
      "Epoch 22/100, Train Loss: -0.0542, Test Loss: 0.4815\n",
      "Epoch 23/100, Train Loss: -0.0543, Test Loss: 0.4913\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 0.4080, Test Loss: 0.1656\n",
      "Epoch 2/100, Train Loss: 0.1978, Test Loss: 0.0641\n",
      "Epoch 3/100, Train Loss: 0.0761, Test Loss: 0.0190\n",
      "Epoch 4/100, Train Loss: 0.0078, Test Loss: -0.0066\n",
      "Epoch 5/100, Train Loss: -0.0195, Test Loss: -0.0254\n",
      "Epoch 6/100, Train Loss: -0.0348, Test Loss: -0.0407\n",
      "Epoch 7/100, Train Loss: -0.0430, Test Loss: -0.0464\n",
      "Epoch 8/100, Train Loss: -0.0457, Test Loss: -0.0505\n",
      "Epoch 9/100, Train Loss: -0.0493, Test Loss: -0.0546\n",
      "Epoch 10/100, Train Loss: -0.0502, Test Loss: -0.0548\n",
      "Epoch 11/100, Train Loss: -0.0513, Test Loss: -0.0570\n",
      "Epoch 12/100, Train Loss: -0.0521, Test Loss: -0.0564\n",
      "Epoch 13/100, Train Loss: -0.0528, Test Loss: -0.0572\n",
      "Epoch 14/100, Train Loss: -0.0534, Test Loss: -0.0560\n",
      "Epoch 15/100, Train Loss: -0.0539, Test Loss: -0.0550\n",
      "Epoch 16/100, Train Loss: -0.0545, Test Loss: -0.0537\n",
      "Epoch 17/100, Train Loss: -0.0551, Test Loss: -0.0522\n",
      "Epoch 18/100, Train Loss: -0.0556, Test Loss: -0.0502\n",
      "Epoch 19/100, Train Loss: -0.0562, Test Loss: -0.0477\n",
      "Epoch 20/100, Train Loss: -0.0567, Test Loss: -0.0451\n",
      "Epoch 21/100, Train Loss: -0.0573, Test Loss: -0.0420\n",
      "Epoch 22/100, Train Loss: -0.0578, Test Loss: -0.0386\n",
      "Epoch 23/100, Train Loss: -0.0584, Test Loss: -0.0350\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 0.0034, Test Loss: 0.3536\n",
      "Epoch 2/100, Train Loss: 0.0084, Test Loss: 0.2850\n",
      "Epoch 3/100, Train Loss: -0.0371, Test Loss: 0.3414\n",
      "Epoch 4/100, Train Loss: -0.0279, Test Loss: 0.2569\n",
      "Epoch 5/100, Train Loss: -0.0494, Test Loss: 0.2468\n",
      "Epoch 6/100, Train Loss: -0.0429, Test Loss: 0.2194\n",
      "Epoch 7/100, Train Loss: -0.0546, Test Loss: 0.2377\n",
      "Epoch 8/100, Train Loss: -0.0507, Test Loss: 0.2241\n",
      "Epoch 9/100, Train Loss: -0.0565, Test Loss: 0.2079\n",
      "Epoch 10/100, Train Loss: -0.0557, Test Loss: 0.2065\n",
      "Epoch 11/100, Train Loss: -0.0574, Test Loss: 0.2133\n",
      "Epoch 12/100, Train Loss: -0.0587, Test Loss: 0.2204\n",
      "Epoch 13/100, Train Loss: -0.0594, Test Loss: 0.2163\n",
      "Epoch 14/100, Train Loss: -0.0603, Test Loss: 0.2185\n",
      "Epoch 15/100, Train Loss: -0.0612, Test Loss: 0.2282\n",
      "Epoch 16/100, Train Loss: -0.0619, Test Loss: 0.2342\n",
      "Epoch 17/100, Train Loss: -0.0628, Test Loss: 0.2357\n",
      "Epoch 18/100, Train Loss: -0.0634, Test Loss: 0.2414\n",
      "Epoch 19/100, Train Loss: -0.0643, Test Loss: 0.2506\n",
      "Epoch 20/100, Train Loss: -0.0649, Test Loss: 0.2555\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -577     |\n",
      "| time/              |          |\n",
      "|    fps             | 320      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -508     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 293      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 13       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.602    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00878  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 217      |\n",
      "|    policy_objective       | 0.0281   |\n",
      "|    std                    | 0.378    |\n",
      "|    value_loss             | 1.17e+03 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -528     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 307      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 19       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.868    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00719  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 218      |\n",
      "|    policy_objective       | 0.0169   |\n",
      "|    std                    | 0.378    |\n",
      "|    value_loss             | 630      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -522     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 310      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 26       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.828    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00848  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 219      |\n",
      "|    policy_objective       | 0.0223   |\n",
      "|    std                    | 0.379    |\n",
      "|    value_loss             | 1.15e+03 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -500     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 313      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 32       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.785    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00769  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 220      |\n",
      "|    policy_objective       | 0.0167   |\n",
      "|    std                    | 0.379    |\n",
      "|    value_loss             | 860      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -483     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 291      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 42       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.782    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00857  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 221      |\n",
      "|    policy_objective       | 0.0181   |\n",
      "|    std                    | 0.38     |\n",
      "|    value_loss             | 856      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -495     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 287      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 49       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.838    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00856  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 222      |\n",
      "|    policy_objective       | 0.0218   |\n",
      "|    std                    | 0.381    |\n",
      "|    value_loss             | 830      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -500     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 290      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 56       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.793    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00865  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 223      |\n",
      "|    policy_objective       | 0.0195   |\n",
      "|    std                    | 0.392    |\n",
      "|    value_loss             | 1.04e+03 |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-448.77869549114257\n",
      "------------------------------\n",
      "round: 28\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.0505, Test Loss: 0.1248\n",
      "Epoch 2/100, Train Loss: 0.0605, Test Loss: -0.0266\n",
      "Epoch 3/100, Train Loss: -0.0105, Test Loss: 0.0222\n",
      "Epoch 4/100, Train Loss: -0.0220, Test Loss: -0.0523\n",
      "Epoch 5/100, Train Loss: -0.0306, Test Loss: -0.0434\n",
      "Epoch 6/100, Train Loss: -0.0429, Test Loss: -0.0747\n",
      "Epoch 7/100, Train Loss: -0.0474, Test Loss: -0.0566\n",
      "Epoch 8/100, Train Loss: -0.0488, Test Loss: -0.0759\n",
      "Epoch 9/100, Train Loss: -0.0555, Test Loss: -0.0723\n",
      "Epoch 10/100, Train Loss: -0.0546, Test Loss: -0.0786\n",
      "Epoch 11/100, Train Loss: -0.0572, Test Loss: -0.0776\n",
      "Epoch 12/100, Train Loss: -0.0580, Test Loss: -0.0815\n",
      "Epoch 13/100, Train Loss: -0.0594, Test Loss: -0.0800\n",
      "Epoch 14/100, Train Loss: -0.0595, Test Loss: -0.0824\n",
      "Epoch 15/100, Train Loss: -0.0610, Test Loss: -0.0799\n",
      "Epoch 16/100, Train Loss: -0.0609, Test Loss: -0.0809\n",
      "Epoch 17/100, Train Loss: -0.0621, Test Loss: -0.0810\n",
      "Epoch 18/100, Train Loss: -0.0623, Test Loss: -0.0808\n",
      "Epoch 19/100, Train Loss: -0.0629, Test Loss: -0.0796\n",
      "Epoch 20/100, Train Loss: -0.0634, Test Loss: -0.0781\n",
      "Epoch 21/100, Train Loss: -0.0639, Test Loss: -0.0773\n",
      "Epoch 22/100, Train Loss: -0.0644, Test Loss: -0.0756\n",
      "Epoch 23/100, Train Loss: -0.0649, Test Loss: -0.0740\n",
      "Epoch 24/100, Train Loss: -0.0654, Test Loss: -0.0721\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 0.0122, Test Loss: -0.0829\n",
      "Epoch 2/100, Train Loss: -0.0241, Test Loss: -0.0775\n",
      "Epoch 3/100, Train Loss: -0.0317, Test Loss: -0.0799\n",
      "Epoch 4/100, Train Loss: -0.0376, Test Loss: -0.0760\n",
      "Epoch 5/100, Train Loss: -0.0388, Test Loss: -0.0768\n",
      "Epoch 6/100, Train Loss: -0.0420, Test Loss: -0.0754\n",
      "Epoch 7/100, Train Loss: -0.0421, Test Loss: -0.0751\n",
      "Epoch 8/100, Train Loss: -0.0425, Test Loss: -0.0752\n",
      "Epoch 9/100, Train Loss: -0.0434, Test Loss: -0.0748\n",
      "Epoch 10/100, Train Loss: -0.0436, Test Loss: -0.0753\n",
      "Epoch 11/100, Train Loss: -0.0440, Test Loss: -0.0755\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 0.6288, Test Loss: 0.0334\n",
      "Epoch 2/100, Train Loss: 0.5601, Test Loss: -0.0224\n",
      "Epoch 3/100, Train Loss: 0.4320, Test Loss: -0.0187\n",
      "Epoch 4/100, Train Loss: 0.3896, Test Loss: -0.0663\n",
      "Epoch 5/100, Train Loss: 0.2873, Test Loss: -0.0724\n",
      "Epoch 6/100, Train Loss: 0.2230, Test Loss: -0.0608\n",
      "Epoch 7/100, Train Loss: 0.1565, Test Loss: -0.0769\n",
      "Epoch 8/100, Train Loss: 0.0978, Test Loss: -0.0695\n",
      "Epoch 9/100, Train Loss: 0.0626, Test Loss: -0.0674\n",
      "Epoch 10/100, Train Loss: 0.0343, Test Loss: -0.0727\n",
      "Epoch 11/100, Train Loss: 0.0134, Test Loss: -0.0727\n",
      "Epoch 12/100, Train Loss: 0.0011, Test Loss: -0.0705\n",
      "Epoch 13/100, Train Loss: -0.0068, Test Loss: -0.0699\n",
      "Epoch 14/100, Train Loss: -0.0132, Test Loss: -0.0699\n",
      "Epoch 15/100, Train Loss: -0.0179, Test Loss: -0.0692\n",
      "Epoch 16/100, Train Loss: -0.0210, Test Loss: -0.0683\n",
      "Epoch 17/100, Train Loss: -0.0231, Test Loss: -0.0680\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -448     |\n",
      "| time/              |          |\n",
      "|    fps             | 330      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -493     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 320      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 12       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.857    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00772  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 225      |\n",
      "|    policy_objective       | 0.021    |\n",
      "|    std                    | 0.393    |\n",
      "|    value_loss             | 720      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -520     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 319      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 19       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.793    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00811  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 226      |\n",
      "|    policy_objective       | 0.0178   |\n",
      "|    std                    | 0.385    |\n",
      "|    value_loss             | 988      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -520     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 319      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 25       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.808    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00873  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 227      |\n",
      "|    policy_objective       | 0.0181   |\n",
      "|    std                    | 0.386    |\n",
      "|    value_loss             | 1.04e+03 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -551     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 318      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 32       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.767    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00883  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 228      |\n",
      "|    policy_objective       | 0.0209   |\n",
      "|    std                    | 0.395    |\n",
      "|    value_loss             | 1.01e+03 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -559     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 318      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 38       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.587    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00967  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 229      |\n",
      "|    policy_objective       | 0.0208   |\n",
      "|    std                    | 0.39     |\n",
      "|    value_loss             | 997      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -570     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 318      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 45       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.808    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00776  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 230      |\n",
      "|    policy_objective       | 0.0194   |\n",
      "|    std                    | 0.391    |\n",
      "|    value_loss             | 721      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -545     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 316      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 51       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.809    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00925  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 231      |\n",
      "|    policy_objective       | 0.0238   |\n",
      "|    std                    | 0.385    |\n",
      "|    value_loss             | 975      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-199.05985711975956\n",
      "------------------------------\n",
      "round: 29\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.0707, Test Loss: 0.0255\n",
      "Epoch 2/100, Train Loss: 0.0473, Test Loss: -0.0653\n",
      "Epoch 3/100, Train Loss: -0.0145, Test Loss: -0.0024\n",
      "Epoch 4/100, Train Loss: -0.0116, Test Loss: -0.0746\n",
      "Epoch 5/100, Train Loss: -0.0372, Test Loss: -0.0501\n",
      "Epoch 6/100, Train Loss: -0.0322, Test Loss: -0.0710\n",
      "Epoch 7/100, Train Loss: -0.0422, Test Loss: -0.0756\n",
      "Epoch 8/100, Train Loss: -0.0417, Test Loss: -0.0721\n",
      "Epoch 9/100, Train Loss: -0.0440, Test Loss: -0.0759\n",
      "Epoch 10/100, Train Loss: -0.0463, Test Loss: -0.0761\n",
      "Epoch 11/100, Train Loss: -0.0456, Test Loss: -0.0766\n",
      "Epoch 12/100, Train Loss: -0.0475, Test Loss: -0.0775\n",
      "Epoch 13/100, Train Loss: -0.0475, Test Loss: -0.0766\n",
      "Epoch 14/100, Train Loss: -0.0478, Test Loss: -0.0782\n",
      "Epoch 15/100, Train Loss: -0.0488, Test Loss: -0.0778\n",
      "Epoch 16/100, Train Loss: -0.0486, Test Loss: -0.0786\n",
      "Epoch 17/100, Train Loss: -0.0493, Test Loss: -0.0789\n",
      "Epoch 18/100, Train Loss: -0.0493, Test Loss: -0.0793\n",
      "Epoch 19/100, Train Loss: -0.0498, Test Loss: -0.0799\n",
      "Epoch 20/100, Train Loss: -0.0500, Test Loss: -0.0803\n",
      "Epoch 21/100, Train Loss: -0.0503, Test Loss: -0.0808\n",
      "Epoch 22/100, Train Loss: -0.0506, Test Loss: -0.0813\n",
      "Epoch 23/100, Train Loss: -0.0509, Test Loss: -0.0818\n",
      "Epoch 24/100, Train Loss: -0.0512, Test Loss: -0.0822\n",
      "Epoch 25/100, Train Loss: -0.0514, Test Loss: -0.0828\n",
      "Epoch 26/100, Train Loss: -0.0517, Test Loss: -0.0833\n",
      "Epoch 27/100, Train Loss: -0.0519, Test Loss: -0.0837\n",
      "Epoch 28/100, Train Loss: -0.0522, Test Loss: -0.0842\n",
      "Epoch 29/100, Train Loss: -0.0524, Test Loss: -0.0847\n",
      "Epoch 30/100, Train Loss: -0.0526, Test Loss: -0.0851\n",
      "Epoch 31/100, Train Loss: -0.0529, Test Loss: -0.0855\n",
      "Epoch 32/100, Train Loss: -0.0531, Test Loss: -0.0859\n",
      "Epoch 33/100, Train Loss: -0.0533, Test Loss: -0.0863\n",
      "Epoch 34/100, Train Loss: -0.0535, Test Loss: -0.0867\n",
      "Epoch 35/100, Train Loss: -0.0537, Test Loss: -0.0871\n",
      "Epoch 36/100, Train Loss: -0.0539, Test Loss: -0.0875\n",
      "Epoch 37/100, Train Loss: -0.0541, Test Loss: -0.0879\n",
      "Epoch 38/100, Train Loss: -0.0543, Test Loss: -0.0882\n",
      "Epoch 39/100, Train Loss: -0.0545, Test Loss: -0.0885\n",
      "Epoch 40/100, Train Loss: -0.0547, Test Loss: -0.0888\n",
      "Epoch 41/100, Train Loss: -0.0549, Test Loss: -0.0891\n",
      "Epoch 42/100, Train Loss: -0.0551, Test Loss: -0.0894\n",
      "Epoch 43/100, Train Loss: -0.0552, Test Loss: -0.0896\n",
      "Epoch 44/100, Train Loss: -0.0554, Test Loss: -0.0899\n",
      "Epoch 45/100, Train Loss: -0.0556, Test Loss: -0.0901\n",
      "Epoch 46/100, Train Loss: -0.0557, Test Loss: -0.0904\n",
      "Epoch 47/100, Train Loss: -0.0559, Test Loss: -0.0906\n",
      "Epoch 48/100, Train Loss: -0.0560, Test Loss: -0.0909\n",
      "Epoch 49/100, Train Loss: -0.0562, Test Loss: -0.0911\n",
      "Epoch 50/100, Train Loss: -0.0563, Test Loss: -0.0913\n",
      "Epoch 51/100, Train Loss: -0.0564, Test Loss: -0.0915\n",
      "Epoch 52/100, Train Loss: -0.0565, Test Loss: -0.0917\n",
      "Epoch 53/100, Train Loss: -0.0567, Test Loss: -0.0918\n",
      "Epoch 54/100, Train Loss: -0.0568, Test Loss: -0.0920\n",
      "Epoch 55/100, Train Loss: -0.0569, Test Loss: -0.0922\n",
      "Epoch 56/100, Train Loss: -0.0570, Test Loss: -0.0924\n",
      "Epoch 57/100, Train Loss: -0.0572, Test Loss: -0.0925\n",
      "Epoch 58/100, Train Loss: -0.0572, Test Loss: -0.0926\n",
      "Epoch 59/100, Train Loss: -0.0573, Test Loss: -0.0927\n",
      "Epoch 60/100, Train Loss: -0.0574, Test Loss: -0.0928\n",
      "Epoch 61/100, Train Loss: -0.0575, Test Loss: -0.0928\n",
      "Epoch 62/100, Train Loss: -0.0576, Test Loss: -0.0927\n",
      "Epoch 63/100, Train Loss: -0.0577, Test Loss: -0.0927\n",
      "Epoch 64/100, Train Loss: -0.0577, Test Loss: -0.0928\n",
      "Epoch 65/100, Train Loss: -0.0578, Test Loss: -0.0931\n",
      "Epoch 66/100, Train Loss: -0.0580, Test Loss: -0.0934\n",
      "Epoch 67/100, Train Loss: -0.0581, Test Loss: -0.0930\n",
      "Epoch 68/100, Train Loss: -0.0578, Test Loss: -0.0914\n",
      "Epoch 69/100, Train Loss: -0.0569, Test Loss: -0.0884\n",
      "Epoch 70/100, Train Loss: -0.0549, Test Loss: -0.0875\n",
      "Epoch 71/100, Train Loss: -0.0515, Test Loss: -0.0924\n",
      "Epoch 72/100, Train Loss: -0.0513, Test Loss: -0.0595\n",
      "Epoch 73/100, Train Loss: -0.0428, Test Loss: -0.0859\n",
      "Epoch 74/100, Train Loss: -0.0448, Test Loss: -0.0647\n",
      "Epoch 75/100, Train Loss: -0.0469, Test Loss: -0.0775\n",
      "Epoch 76/100, Train Loss: -0.0462, Test Loss: -0.0903\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: -0.0330, Test Loss: -0.0639\n",
      "Epoch 2/100, Train Loss: -0.0341, Test Loss: -0.0647\n",
      "Epoch 3/100, Train Loss: -0.0369, Test Loss: -0.0663\n",
      "Epoch 4/100, Train Loss: -0.0390, Test Loss: -0.0678\n",
      "Epoch 5/100, Train Loss: -0.0408, Test Loss: -0.0699\n",
      "Epoch 6/100, Train Loss: -0.0422, Test Loss: -0.0720\n",
      "Epoch 7/100, Train Loss: -0.0428, Test Loss: -0.0741\n",
      "Epoch 8/100, Train Loss: -0.0438, Test Loss: -0.0759\n",
      "Epoch 9/100, Train Loss: -0.0446, Test Loss: -0.0773\n",
      "Epoch 10/100, Train Loss: -0.0449, Test Loss: -0.0786\n",
      "Epoch 11/100, Train Loss: -0.0454, Test Loss: -0.0793\n",
      "Epoch 12/100, Train Loss: -0.0457, Test Loss: -0.0796\n",
      "Epoch 13/100, Train Loss: -0.0458, Test Loss: -0.0801\n",
      "Epoch 14/100, Train Loss: -0.0461, Test Loss: -0.0800\n",
      "Epoch 15/100, Train Loss: -0.0464, Test Loss: -0.0801\n",
      "Epoch 16/100, Train Loss: -0.0468, Test Loss: -0.0802\n",
      "Epoch 17/100, Train Loss: -0.0469, Test Loss: -0.0802\n",
      "Epoch 18/100, Train Loss: -0.0470, Test Loss: -0.0800\n",
      "Epoch 19/100, Train Loss: -0.0470, Test Loss: -0.0802\n",
      "Epoch 20/100, Train Loss: -0.0469, Test Loss: -0.0800\n",
      "Epoch 21/100, Train Loss: -0.0459, Test Loss: -0.0799\n",
      "Epoch 22/100, Train Loss: -0.0467, Test Loss: -0.0803\n",
      "Epoch 23/100, Train Loss: -0.0471, Test Loss: -0.0788\n",
      "Epoch 24/100, Train Loss: -0.0470, Test Loss: -0.0797\n",
      "Epoch 25/100, Train Loss: -0.0474, Test Loss: -0.0795\n",
      "Epoch 26/100, Train Loss: -0.0473, Test Loss: -0.0796\n",
      "Epoch 27/100, Train Loss: -0.0475, Test Loss: -0.0802\n",
      "Epoch 28/100, Train Loss: -0.0473, Test Loss: -0.0798\n",
      "Epoch 29/100, Train Loss: -0.0472, Test Loss: -0.0803\n",
      "Epoch 30/100, Train Loss: -0.0471, Test Loss: -0.0800\n",
      "Epoch 31/100, Train Loss: -0.0474, Test Loss: -0.0800\n",
      "Epoch 32/100, Train Loss: -0.0478, Test Loss: -0.0803\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: -0.0243, Test Loss: -0.0543\n",
      "Epoch 2/100, Train Loss: -0.0303, Test Loss: -0.0656\n",
      "Epoch 3/100, Train Loss: -0.0414, Test Loss: -0.0591\n",
      "Epoch 4/100, Train Loss: -0.0446, Test Loss: -0.0734\n",
      "Epoch 5/100, Train Loss: -0.0480, Test Loss: -0.0745\n",
      "Epoch 6/100, Train Loss: -0.0526, Test Loss: -0.0762\n",
      "Epoch 7/100, Train Loss: -0.0546, Test Loss: -0.0757\n",
      "Epoch 8/100, Train Loss: -0.0571, Test Loss: -0.0781\n",
      "Epoch 9/100, Train Loss: -0.0591, Test Loss: -0.0770\n",
      "Epoch 10/100, Train Loss: -0.0622, Test Loss: -0.0737\n",
      "Epoch 11/100, Train Loss: -0.0639, Test Loss: -0.0680\n",
      "Epoch 12/100, Train Loss: -0.0663, Test Loss: -0.0577\n",
      "Epoch 13/100, Train Loss: -0.0683, Test Loss: -0.0433\n",
      "Epoch 14/100, Train Loss: -0.0689, Test Loss: -0.0272\n",
      "Epoch 15/100, Train Loss: -0.0699, Test Loss: -0.0084\n",
      "Epoch 16/100, Train Loss: -0.0695, Test Loss: 0.0026\n",
      "Epoch 17/100, Train Loss: -0.0706, Test Loss: 0.0198\n",
      "Epoch 18/100, Train Loss: -0.0694, Test Loss: 0.0366\n",
      "Early stopping due to overfitting.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -478     |\n",
      "| time/              |          |\n",
      "|    fps             | 297      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -506     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 298      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 13       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.793    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00834  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 233      |\n",
      "|    policy_objective       | 0.023    |\n",
      "|    std                    | 0.386    |\n",
      "|    value_loss             | 983      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -536     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 284      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 21       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.742    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.0092   |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 234      |\n",
      "|    policy_objective       | 0.0203   |\n",
      "|    std                    | 0.378    |\n",
      "|    value_loss             | 1.02e+03 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -503     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 273      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 29       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.716    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00916  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 235      |\n",
      "|    policy_objective       | 0.0199   |\n",
      "|    std                    | 0.379    |\n",
      "|    value_loss             | 1.2e+03  |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -497     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 278      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 36       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.792    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00851  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 236      |\n",
      "|    policy_objective       | 0.0219   |\n",
      "|    std                    | 0.373    |\n",
      "|    value_loss             | 1.05e+03 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -516     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 271      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 45       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.763    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00826  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 237      |\n",
      "|    policy_objective       | 0.0217   |\n",
      "|    std                    | 0.372    |\n",
      "|    value_loss             | 959      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -503     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 278      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 51       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.745    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.0085   |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 238      |\n",
      "|    policy_objective       | 0.0203   |\n",
      "|    std                    | 0.373    |\n",
      "|    value_loss             | 747      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -508     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 282      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 58       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.883    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00855  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 239      |\n",
      "|    policy_objective       | 0.0209   |\n",
      "|    std                    | 0.37     |\n",
      "|    value_loss             | 539      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-269.7530013364973\n"
     ]
    }
   ],
   "source": [
    "# initialize the client and server\n",
    "timesteps_real_per_round = 500\n",
    "timesteps_fc_per_round = timesteps_real_per_round * 30\n",
    "epoch_per_round = 100\n",
    "CLIENTS_NUM = 3\n",
    "rounds_num = 30\n",
    "batch_size_env_model = 128\n",
    "\n",
    "test_dir = \"Diff_Gaussian_test1\"\n",
    "model_tmp_path = test_dir + \"/models/tmp\"\n",
    "\n",
    "env_models = []\n",
    "MB_env = TimeLimit(MB_PendulumEnv(env_models,device), max_episode_steps = 200)\n",
    "\n",
    "# Global_RL = PPO(\"MlpPolicy\", MB_env, verbose=1)\n",
    "Global_RL = TRPO(\"MlpPolicy\", MB_env, verbose=1)\n",
    "\n",
    "env_theta = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "real_envs = []\n",
    "Clients = []\n",
    "for i in range(CLIENTS_NUM):\n",
    "    real_envs.append( TimeLimit(PendulumEnv(), max_episode_steps=200) )\n",
    "    policy_net = Global_RL\n",
    "    agent = SB3Agent(policy_net)\n",
    "    client = FRLClient(real_envs[i], agent, lr = 3e-4, hidden_size = 256, device = device)\n",
    "    Clients.append(client)\n",
    "    env_model = copy.deepcopy(client.model)\n",
    "    env_models.append(env_model)\n",
    "    \n",
    "\n",
    "    \n",
    "Global_RL.env.models = env_models\n",
    "\n",
    "Global_RL.save(model_tmp_path)\n",
    "\n",
    "rewards_log = []\n",
    "\n",
    "env_models = []\n",
    "for round_idx in range(rounds_num):\n",
    "    print('------------------------------')\n",
    "    print(\"round: \" + str(round_idx))\n",
    "    for client_idx in range(len(Clients)):\n",
    "        print('------------------------------')\n",
    "        print(\"client: \" + str(client_idx))\n",
    "        # update policy\n",
    "        Clients[client_idx].agent.policy_net = Global_RL\n",
    "        # train prediction models\n",
    "        Clients[client_idx].learn(timesteps_real_per_round, epoch_per_round, batch_size_env_model)\n",
    "        #\n",
    "        env_model = Clients[client_idx].get_prediction_model()\n",
    "        env_models.append(env_model)\n",
    "    \n",
    "#     Server.update_env_models(env_models)\n",
    "\n",
    "    MB_env = TimeLimit(MB_PendulumEnv(env_models,device), max_episode_steps = 200)\n",
    "    \n",
    "    Global_RL = TRPO.load(model_tmp_path, env = MB_env)\n",
    "#     Global_RL.env.models = env_models\n",
    "    #\n",
    "    Global_RL.learn(total_timesteps=timesteps_fc_per_round)\n",
    "    \n",
    "    Global_RL.save(model_tmp_path)\n",
    "#     Server.learn(timesteps_real_per_round = 10000)\n",
    "    # test performance\n",
    "    mean_reward, std_reward = evaluate_policy(Global_RL, real_envs[1], n_eval_episodes=10)\n",
    "    rewards_log.append(mean_reward)\n",
    "    print(\"mean_reward in real env:\" + str(mean_reward))\n",
    "    np.save( test_dir + \"/reward_logs.npy\", rewards_log)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "264f9e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1191.731351493299, -1201.7008823909914, -1100.020173463307, -1114.5696154884995, -1011.0279759377241, -898.0612158778589, -981.6393348578364, -1121.6699736960231, -1039.6600154669954, -1036.7660726503468, -930.416009156406, -870.0306994786952, -840.4977051302791, -754.8268289189785, -751.6086850993336, -612.6545992769301, -643.8770642060787, -350.6331199895329, -189.95389851189685, -190.92159692987335, -125.54477715431713, -191.4934958951082, -127.65472502240445, -155.7382440895075, -234.8515504431911, -152.1133960939478, -424.91970788645557, -448.77869549114257, -199.05985711975956, -269.7530013364973]\n"
     ]
    }
   ],
   "source": [
    "print(rewards_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f784c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_rewards(rewards, steps_per_round):\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    :\n",
    "    rewards (list or array): \n",
    "    steps_per_round (int): \n",
    "    \"\"\"\n",
    "    # \n",
    "    avg_rewards = [reward / steps_per_round for reward in rewards]\n",
    "\n",
    "    plt.figure(figsize=(10, 6), dpi=300)\n",
    "    plt.plot(range(1, len(rewards) + 1), rewards, label='Average Reward per Step')\n",
    "    plt.xlabel('Round')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.title('Average Reward per Step Over Rounds')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17903867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 3000x1800 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_rewards(rewards_log, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd142432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c36921a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " )]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Global_RL.env.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f54dcf51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " )]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Global_RL.env.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faf66af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b315ebde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.25910562, -0.965849  ,  0.49215463], dtype=float32), {})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MB_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6092195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.25910562, -0.965849  ,  0.49215463], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MB_env.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "75f8fde6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mMB_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMB_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mF:\\AI\\RL\\FedMBRL\\MBEnvs\\mb_pendulum2_gaussian.py:147\u001b[0m, in \u001b[0;36mMB_PendulumEnv.step\u001b[1;34m(self, u)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, u):\n\u001b[0;32m    145\u001b[0m     last_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs  \u001b[38;5;66;03m# th := theta\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m     next_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_obs_by_model(last_obs, action\u001b[38;5;241m=\u001b[39mu)\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs \u001b[38;5;241m=\u001b[39m next_obs\n",
      "File \u001b[1;32mF:\\AI\\RL\\FedMBRL\\MBEnvs\\mb_pendulum2_gaussian.py:210\u001b[0m, in \u001b[0;36mMB_PendulumEnv._get_reward\u001b[1;34m(self, obs, action)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_reward\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs, action):\n\u001b[1;32m--> 210\u001b[0m     obs1, obs2, thdot \u001b[38;5;241m=\u001b[39m obs\n\u001b[0;32m    211\u001b[0m     th \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recover_theta(obs1, obs2)\n\u001b[0;32m    212\u001b[0m     costs \u001b[38;5;241m=\u001b[39m angle_normalize(th) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m thdot\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.001\u001b[39m \u001b[38;5;241m*\u001b[39m (action\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "MB_env.step(MB_env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "99a7f293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2085361, -0.7650793, -1.1017272]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MB_env.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292dc859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366cff92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0481a6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f748ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs,_ = Clients[0].env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "200357a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.43307275,  0.90135896, -0.16240236], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c447055d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02079844",
   "metadata": {},
   "outputs": [],
   "source": [
    "action =  Clients[0].agent.act(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9e3bd2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.33290574], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53bab146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianModel(\n",
       "  (fc_net): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Clients[0].model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "464dbe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
    "action_tensor = torch.tensor(action, dtype=torch.float32)\n",
    "input_d = torch.cat((obs_tensor, action_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d1d9c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4331,  0.9014, -0.1624, -0.3329])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e39458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_d = input_d.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edbb8cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, logvar = Clients[0].model.forward(input_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "caf4e380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0052, -0.0026,  0.6155], device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([-3.5161, -2.7553, -2.9714], device='cuda:0', grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, logvar[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0b50dfc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1724, 0.2522, 0.2263], device='cuda:0', grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std = torch.exp(0.5 * logvar[0])\n",
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea0ea8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = torch.randn_like(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a634daaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7962, 0.2781, 1.4486], device='cuda:0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6ed95b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_state_change = mean + eps * std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "34ec3c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1320, 0.0675, 0.9434], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_state_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "579f73a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_np = pred_state_change.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "19eda842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13200563, 0.06751756, 0.94340324], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2e5570d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.43307275,  0.90135896, -0.16240236], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ec85e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_new = obs + output_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3e2e9c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5650784 , 0.96887654, 0.78100085], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b05573f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "edc20798",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_clipped = np.clip(obs, Clients[0].env.observation_space.low, Clients[0].env.observation_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7a1ced1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.43307275,  0.90135896, -0.16240236], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337cf7d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2beb4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0053a243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22093d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.30735433], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Clients[0].agent.policy_net.predict(obs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4bbe53a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1699efd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: -93.15121025741101 +/- 109.2765889131137\n"
     ]
    }
   ],
   "source": [
    "# mean_reward, std_reward = evaluate_policy(Global_RL, real_envs[1], n_eval_episodes=10)\n",
    "mean_reward, std_reward = evaluate_policy(Global_RL, MB_env, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9285a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import TRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a1ac5563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "TRPO_model = TRPO(\"MlpPolicy\", env=real_envs[0], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "23e35f65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -829     |\n",
      "| time/              |          |\n",
      "|    fps             | 520      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -847     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 478      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 8        |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.715    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00696  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 50       |\n",
      "|    policy_objective       | 0.0147   |\n",
      "|    std                    | 0.809    |\n",
      "|    value_loss             | 433      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -893     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 481      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 12       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.749    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00777  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 51       |\n",
      "|    policy_objective       | 0.0188   |\n",
      "|    std                    | 0.789    |\n",
      "|    value_loss             | 629      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -890     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 470      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 17       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.759    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00865  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 52       |\n",
      "|    policy_objective       | 0.0126   |\n",
      "|    std                    | 0.762    |\n",
      "|    value_loss             | 857      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -875     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 470      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 21       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.8      |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00702  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 53       |\n",
      "|    policy_objective       | 0.0123   |\n",
      "|    std                    | 0.769    |\n",
      "|    value_loss             | 679      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -860     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 465      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 26       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.825    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00762  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 54       |\n",
      "|    policy_objective       | 0.0143   |\n",
      "|    std                    | 0.757    |\n",
      "|    value_loss             | 660      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -832     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 467      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 30       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.851    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00773  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 55       |\n",
      "|    policy_objective       | 0.0167   |\n",
      "|    std                    | 0.743    |\n",
      "|    value_loss             | 428      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -808     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 470      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 34       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.881    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00726  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 56       |\n",
      "|    policy_objective       | 0.0213   |\n",
      "|    std                    | 0.72     |\n",
      "|    value_loss             | 291      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -794     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 472      |\n",
      "|    iterations             | 9        |\n",
      "|    time_elapsed           | 39       |\n",
      "|    total_timesteps        | 18432    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.876    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00698  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 57       |\n",
      "|    policy_objective       | 0.0147   |\n",
      "|    std                    | 0.728    |\n",
      "|    value_loss             | 506      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -758     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 471      |\n",
      "|    iterations             | 10       |\n",
      "|    time_elapsed           | 43       |\n",
      "|    total_timesteps        | 20480    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.914    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00761  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 58       |\n",
      "|    policy_objective       | 0.0205   |\n",
      "|    std                    | 0.72     |\n",
      "|    value_loss             | 376      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -734     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 472      |\n",
      "|    iterations             | 11       |\n",
      "|    time_elapsed           | 47       |\n",
      "|    total_timesteps        | 22528    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.883    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00787  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 59       |\n",
      "|    policy_objective       | 0.0231   |\n",
      "|    std                    | 0.698    |\n",
      "|    value_loss             | 250      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -690     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 473      |\n",
      "|    iterations             | 12       |\n",
      "|    time_elapsed           | 51       |\n",
      "|    total_timesteps        | 24576    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.927    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00787  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 60       |\n",
      "|    policy_objective       | 0.0255   |\n",
      "|    std                    | 0.692    |\n",
      "|    value_loss             | 283      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -644     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 473      |\n",
      "|    iterations             | 13       |\n",
      "|    time_elapsed           | 56       |\n",
      "|    total_timesteps        | 26624    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.928    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00833  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 61       |\n",
      "|    policy_objective       | 0.0195   |\n",
      "|    std                    | 0.681    |\n",
      "|    value_loss             | 334      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -604     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 473      |\n",
      "|    iterations             | 14       |\n",
      "|    time_elapsed           | 60       |\n",
      "|    total_timesteps        | 28672    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.884    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00801  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 62       |\n",
      "|    policy_objective       | 0.0296   |\n",
      "|    std                    | 0.677    |\n",
      "|    value_loss             | 282      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -550     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 474      |\n",
      "|    iterations             | 15       |\n",
      "|    time_elapsed           | 64       |\n",
      "|    total_timesteps        | 30720    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.927    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00836  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 63       |\n",
      "|    policy_objective       | 0.0263   |\n",
      "|    std                    | 0.654    |\n",
      "|    value_loss             | 260      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -499     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 474      |\n",
      "|    iterations             | 16       |\n",
      "|    time_elapsed           | 69       |\n",
      "|    total_timesteps        | 32768    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.926    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00848  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 64       |\n",
      "|    policy_objective       | 0.0303   |\n",
      "|    std                    | 0.638    |\n",
      "|    value_loss             | 274      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -445     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 473      |\n",
      "|    iterations             | 17       |\n",
      "|    time_elapsed           | 73       |\n",
      "|    total_timesteps        | 34816    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.954    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00868  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 65       |\n",
      "|    policy_objective       | 0.0229   |\n",
      "|    std                    | 0.639    |\n",
      "|    value_loss             | 202      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [61]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mTRPO_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\sb3_contrib\\trpo\\trpo.py:412\u001b[0m, in \u001b[0;36mTRPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfTRPO,\n\u001b[0;32m    405\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    410\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    411\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfTRPO:\n\u001b[1;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 259\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:169\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 169\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\policies.py:619\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[1;34m(self, obs, deterministic)\u001b[0m\n\u001b[0;32m    617\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_features(obs)\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[1;32m--> 619\u001b[0m     latent_pi, latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m     pi_features, vf_features \u001b[38;5;241m=\u001b[39m features\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\torch_layers.py:222\u001b[0m, in \u001b[0;36mMlpExtractor.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[th\u001b[38;5;241m.\u001b[39mTensor, th\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03m    :return: latent_policy, latent_value of the specified network.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m        If all layers are shared, then ``latent_policy == latent_value``\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_critic(features)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\torch_layers.py:225\u001b[0m, in \u001b[0;36mMlpExtractor.forward_actor\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_actor\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\activation.py:356\u001b[0m, in \u001b[0;36mTanh.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TRPO_model.learn(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bc7bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e8fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4fdc3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178374d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "897fb7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Input Mean: tensor([ 0.3523, -0.0157, -0.2429,  0.1068], grad_fn=<SqueezeBackward1>)\n",
      "Single Input Logvar: tensor([-0.5165, -0.4999, -0.4875, -0.4478], grad_fn=<SqueezeBackward1>)\n",
      "Batch Input Mean: tensor([[ 0.1343,  0.0868, -0.1983,  0.0582],\n",
      "        [ 0.1066, -0.1898, -0.6315,  0.0736],\n",
      "        [ 0.3875,  0.1861, -0.3479,  0.0122],\n",
      "        [ 0.1715,  0.0231, -0.1943,  0.1135],\n",
      "        [ 0.2438, -0.0612, -0.4051,  0.0425]], grad_fn=<SliceBackward0>)\n",
      "Batch Input Logvar: tensor([[-0.4949, -0.4842, -0.5894, -0.4223],\n",
      "        [-0.4841, -0.3873, -0.6761, -0.3461],\n",
      "        [-0.5958, -0.4917, -0.8158, -0.3497],\n",
      "        [-0.4940, -0.4847, -0.5807, -0.4696],\n",
      "        [-0.4818, -0.4734, -0.7298, -0.3792]], grad_fn=<AddBackward0>)\n",
      "Single Input Loss: 3.6077818870544434\n",
      "Batch Input Loss: 3.898193597793579\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# GaussianModel\n",
    "class GaussianModel(nn.Module):\n",
    "    def __init__(self, obs_size, action_size, hidden_size=256, learn_logvar_bounds=False):\n",
    "        super(GaussianModel, self).__init__()\n",
    "        self.out_size = obs_size\n",
    "        \n",
    "        self.fc_net = nn.Sequential(\n",
    "            nn.Linear(obs_size + action_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.mean_logvar = nn.Linear(hidden_size, obs_size * 2)\n",
    "        \n",
    "        self.min_logvar = nn.Parameter(\n",
    "            -10 * torch.ones(1, obs_size), requires_grad=learn_logvar_bounds\n",
    "        )\n",
    "        self.max_logvar = nn.Parameter(\n",
    "            0.5 * torch.ones(1, obs_size), requires_grad=learn_logvar_bounds\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc_net(x)\n",
    "        mean_and_logvar = self.mean_logvar(x)\n",
    "        mean = mean_and_logvar[..., :self.out_size]\n",
    "        logvar = mean_and_logvar[..., self.out_size:]\n",
    "        \n",
    "        logvar = self.max_logvar - F.softplus(self.max_logvar - logvar)\n",
    "        logvar = self.min_logvar + F.softplus(logvar - self.min_logvar)\n",
    "        \n",
    "        # If the original input was one-dimensional, squeeze the output\n",
    "        if logvar.shape[0] == 1:\n",
    "            mean = mean.squeeze(0)\n",
    "            logvar = logvar.squeeze(0)\n",
    "        \n",
    "        return mean, logvar\n",
    "\n",
    "# \n",
    "def test_gaussian_model():\n",
    "    obs_size = 4\n",
    "    action_size = 2\n",
    "    hidden_size = 256\n",
    "\n",
    "    model = GaussianModel(obs_size, action_size, hidden_size)\n",
    "\n",
    "    # \n",
    "    single_input = torch.randn(obs_size + action_size)\n",
    "    \n",
    "    # \n",
    "    batch_input = torch.randn(5, obs_size + action_size)\n",
    "    \n",
    "    # \n",
    "    single_mean, single_logvar = model(single_input)\n",
    "    batch_mean, batch_logvar = model(batch_input)\n",
    "    \n",
    "    print(\"Single Input Mean:\", single_mean)\n",
    "    print(\"Single Input Logvar:\", single_logvar)\n",
    "    print(\"Batch Input Mean:\", batch_mean)\n",
    "    print(\"Batch Input Logvar:\", batch_logvar)\n",
    "    \n",
    "    # \n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # \n",
    "    target_mean = torch.randn(obs_size)\n",
    "    target_logvar = torch.randn(obs_size)\n",
    "    \n",
    "    # \n",
    "    single_loss = loss_fn(single_mean, target_mean) + loss_fn(single_logvar, target_logvar)\n",
    "    batch_loss = loss_fn(batch_mean, target_mean.unsqueeze(0).expand_as(batch_mean)) + \\\n",
    "                 loss_fn(batch_logvar, target_logvar.unsqueeze(0).expand_as(batch_logvar))\n",
    "    \n",
    "    print(\"Single Input Loss:\", single_loss.item())\n",
    "    print(\"Batch Input Loss:\", batch_loss.item())\n",
    "\n",
    "# \n",
    "test_gaussian_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9a990d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1db1039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
