{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27727ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from H_Envs.pendulum import  PendulumEnv\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97ecea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Agent import SB3Agent\n",
    "import copy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from H_Envs.pendulum import PendulumEnv\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from MBEnvs.mb_pendulum_base import MB_PendulumEnv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sb3_contrib import TRPO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5086918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch as th\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from typing import Any, ClassVar, Dict, Optional, Type, TypeVar, Union\n",
    "from PredictionModel import PredictionModel\n",
    "from Utils import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gymnasium as gym\n",
    "from Agent import BaseAgent\n",
    "import copy\n",
    "\n",
    "DATA_MAX = 10000\n",
    "\n",
    "class FRLClient():\n",
    "    '''\n",
    "      FRLClient:\n",
    "      Train predcition model(predict the state trainsition) by sampled data\n",
    "      data is sampled by the true environment \n",
    "      \n",
    "      env: true environment\n",
    "      model: prediction the transition(train the model by supervised learning)\n",
    "      agent: interactive with the true environment by policy pi with explorating\n",
    "\t  \n",
    "      params:\n",
    "\t    lr: learning_rate of model\n",
    "\t    hidden_size:\n",
    "\t    device:\n",
    "    '''\n",
    "  \n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        agent: BaseAgent,\n",
    "        lr: float = 3e-4,\n",
    "        hidden_size: int = 256,\n",
    "        device: Union[th.device, str] = \"auto\",\n",
    "      ):\n",
    "        # Initialize true environment\n",
    "        self.env = env\n",
    "        self.obs_size = env.observation_space.shape[0]\n",
    "        # Check if action_space is Discrete\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            self.action_size = 1\n",
    "        else:\n",
    "            self.action_size = env.action_space.shape[0]\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Initialize prediction model (predict the state transition)\n",
    "        self.model = PredictionModel(self.obs_size, self.action_size, hidden_size).to(device)\n",
    "        self.agent = agent\n",
    "        self.policy = self.agent.policy_net\n",
    "\n",
    "        self.lr = lr\n",
    "        self.device = device\n",
    "        self.dataset_X = None\n",
    "        self.dataset_y = None\n",
    "        self.dataMax = DATA_MAX\n",
    "        \n",
    "    def get_prediction_model(self):\n",
    "        # Return the prediction model\n",
    "        return copy.deepcopy(self.model)\n",
    "\n",
    "    def get_prediction_model_params(self):\n",
    "        # Return a deepcopy of the model's state dictionary\n",
    "        return copy.deepcopy(self.model.state_dict())\n",
    "      \n",
    "    def update_policy(self, policy_net):\n",
    "        # update the policy pi by policy parameters sended by server\n",
    "        self.agent.update_policy_net(policy_net)\n",
    "        self.policy = self.agent.policy_net\n",
    "\n",
    "    def train_prediction_model(self, num_data=1000, num_epoch=100, batch_size=32):\n",
    "        '''\n",
    "        Train the prediction model with true data\n",
    "        '''\n",
    "        self.train_loss_list = []\n",
    "        self.test_avg_list = []\n",
    "        # Split the dataset\n",
    "        train_X, test_X, train_y, test_y = train_test_split(self.dataset_X, self.dataset_y, test_size=0.2)\n",
    "        \n",
    "        # Flatten the column \"actions\"\n",
    "        train_X = expand_action_column(train_X, action_column_name=\"actions\")\n",
    "        test_X = expand_action_column(test_X, action_column_name=\"actions\")\n",
    "        self.train_X = train_X\n",
    "        self.test_X = test_X\n",
    "        self.train_y = train_y\n",
    "        self.test_y = test_y\n",
    "\n",
    "        model = self.model.to(self.device)\n",
    "        \n",
    "        # Define loss function and optimizer\n",
    "        loss_fn = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.lr)\n",
    "\n",
    "        overfit = 0\n",
    "        last_test_avg = 10000\n",
    "\n",
    "        # Convert data to numpy arrays\n",
    "        train_X = train_X.to_numpy()\n",
    "        train_y = train_y.to_numpy()\n",
    "        test_X = test_X.to_numpy()\n",
    "        test_y = test_y.to_numpy()\n",
    "\n",
    "        # Train the model for a given number of epochs\n",
    "        for t in range(num_epoch):\n",
    "            train_loss = model_train(train_X, train_y, self.model, loss_fn, optimizer, batch_size)\n",
    "            test_avg = model_test(test_X, test_y, self.model, loss_fn, batch_size)\n",
    "            self.train_loss_list.append(train_loss)\n",
    "            self.test_avg_list.append(test_avg)\n",
    "            # Early stopping if overfitting occurs\n",
    "            if test_avg > last_test_avg:\n",
    "                overfit += 1\n",
    "            else:\n",
    "                overfit = 0\n",
    "                last_test_avg = test_avg\n",
    "            if overfit >= 10:\n",
    "                break\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "    def sample_seq_data(self, n, seq_length):\n",
    "        env = self.env\n",
    "        \n",
    "        # observation, info = env.reset()\n",
    "        # df = pd.DataFrame(observation).T\n",
    "        # actions = []\n",
    "        dataset_X_list = []\n",
    "        dataset_y_list = []\n",
    "        cnt = 0\n",
    "        while(cnt < n):\n",
    "            observation, info = env.reset()\n",
    "            obs_df = pd.DataFrame(observation).T \n",
    "            actions = []\n",
    "            done = False\n",
    "            truncated = False\n",
    "            while done == False and truncated == False:\n",
    "                action = self.agent.act(observation)\n",
    "                obs_df[len(obs_df)] = observation\n",
    "                actions.append(action)\n",
    "                observation, reward, done, truncated, info = env.step(action)\n",
    "                cnt += 1\n",
    "                if cnt >= n:\n",
    "                    break\n",
    "            # df_temp[\"actions\"] = actions\n",
    "            dataset_X_temp, dataset_y_temp = create_sequences_diff(obs_df, action_df, seq_length)\n",
    "            dataset_X_list.append(dataset_X_temp)\n",
    "            dataset_y_list.append(dataset_y_temp)\n",
    "        \n",
    "            # 展平数据集\n",
    "        dataset_X = np.concatenate(dataset_X_list, axis=0)\n",
    "        dataset_y = np.concatenate(dataset_y_list, axis=0)\n",
    "        self.cur_dataset_X, self.cur_dataset_y = dataset_X, dataset_y\n",
    "            \n",
    "        \n",
    "\n",
    "    # # gymnasium\n",
    "    # def sample_data(self, n):\n",
    "    #     '''\n",
    "    #     Sample data using policy π to train the prediction model\n",
    "    #     '''\n",
    "    #     env = self.env\n",
    "    #     observation, info = env.reset()\n",
    "    #     df = pd.DataFrame(observation).T\n",
    "    #     actions = []\n",
    "\n",
    "    #     # Sample data from the environment\n",
    "    #     for i in range(int(n * 0.5)):\n",
    "    #         action = self.agent.act(observation)\n",
    "\n",
    "    #         if i != 0:\n",
    "    #             df.loc[len(df)] = observation\n",
    "    #         actions.append(action)\n",
    "    #         observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    #         if done or truncated:\n",
    "    #             observation, info = env.reset()\n",
    "    #     # env.close()\n",
    "        \n",
    "    #     observation, info = env.reset()\n",
    "    #     for i in range(int(n * 0.5)):\n",
    "    #         # action = self.agent.act(observation)\n",
    "    #         action = env.action_space.sample()\n",
    "    #         # if i != 0:\n",
    "    #         df.loc[len(df)] = observation\n",
    "    #         actions.append(action)\n",
    "    #         observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    #         if done or truncated:\n",
    "    #             observation, info = env.reset()\n",
    "    #     env.close()\n",
    "\n",
    "    #     df[\"actions\"] = actions\n",
    "    #     self.trajetories = df\n",
    "    #     self.cur_dataset_X, self.cur_dataset_y = process_dfs_diff(df)\n",
    "    #     self.addData()\n",
    "        \n",
    "    def addData(self):\n",
    "        \"\"\"\n",
    "        Add cur_dataset_X and cur_dataset_y to the main dataset_X and dataset_y.\n",
    "        If the combined dataset exceeds the maximum size (self.dataMax), \n",
    "        randomly delete a batch of data to make space for the new data.\n",
    "        \"\"\"\n",
    "        if self.dataset_X is None or self.dataset_y is None:\n",
    "            # If the main dataset is not initialized, initialize it with the current data\n",
    "            self.dataset_X = self.cur_dataset_X.copy()\n",
    "            self.dataset_y = self.cur_dataset_y.copy()\n",
    "        else:\n",
    "            # Check if the combined dataset exceeds the maximum size\n",
    "            if len(self.dataset_X) + len(self.cur_dataset_X) > self.dataMax:\n",
    "                # Calculate the number of rows to remove\n",
    "                excess_size = (len(self.dataset_X) + len(self.cur_dataset_X)) - self.dataMax\n",
    "    \n",
    "                # Randomly select indices to remove\n",
    "                drop_indices = np.random.choice(self.dataset_X.index, excess_size, replace=False)\n",
    "    \n",
    "                # Drop rows from both dataset_X and dataset_y\n",
    "                self.dataset_X = self.dataset_X.drop(drop_indices).reset_index(drop=True)\n",
    "                self.dataset_y = self.dataset_y.drop(drop_indices).reset_index(drop=True)\n",
    "    \n",
    "            # Append new data\n",
    "            self.dataset_X = pd.concat([self.dataset_X, self.cur_dataset_X], ignore_index=True)\n",
    "            self.dataset_y = pd.concat([self.dataset_y, self.cur_dataset_y], ignore_index=True)\n",
    "        \n",
    "    def learn(self, timesteps = 1000, epoch = 10, batch_size = 32):\n",
    "        self.sample_data(timesteps)\n",
    "      \n",
    "        self.train_prediction_model(timesteps, num_epoch = epoch, batch_size=batch_size)\n",
    "      \n",
    "    def get_dataset(self):\n",
    "        return copy.deepcopy(self.train_X), copy.deepcopy(self.train_y),  copy.deepcopy(self.test_X),  copy.deepcopy(self.test_y)\n",
    "      \n",
    "\n",
    "\n",
    "def model_train(X, y, model, loss_fn, optimizer, batch_size):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "\n",
    "    # Loop through the dataset in batches\n",
    "    for i in range(round((len(y) / batch_size) + 0)):\n",
    "        # Convert batch data to torch tensors and move to GPU\n",
    "        train_X = torch.from_numpy(X[i * batch_size: (i+1) * batch_size]).cuda()\n",
    "        train_y = torch.from_numpy(y[i * batch_size: (i+1) * batch_size]).cuda()\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        # Compute loss for each sample in the batch\n",
    "        for k in range(min(batch_size, len(train_X))):\n",
    "            pred = model.forward(train_X[k].float())\n",
    "            loss += loss_fn(pred.to(torch.float32), train_y[k].to(torch.float32))\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        # Zero the gradients, perform backpropagation, and update the weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Return average loss per sample\n",
    "    return loss_sum / len(y)\n",
    "\n",
    "def model_test(X, y, model, loss_fn, batch_size):\n",
    "    loss_sum = 0\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        # Loop through the dataset in batches\n",
    "        for i in range(round((len(y) / batch_size) + 1)):\n",
    "            # Convert batch data to torch tensors and move to GPU\n",
    "            test_X = torch.from_numpy(X[i * batch_size: (i+1) * batch_size]).cuda()\n",
    "            test_y = torch.from_numpy(y[i * batch_size: (i+1) * batch_size]).cuda()\n",
    "\n",
    "            # Compute loss for each sample in the batch\n",
    "            for k in range(min(batch_size, len(test_X))):\n",
    "                pred = model.forward(test_X[k].float())\n",
    "                \n",
    "                # loss += loss_fn(pred.to(torch.float32), test_y[k].to(torch.float32))\n",
    "                loss = loss_fn(pred, test_y[k].float())\n",
    "                loss_sum += loss.item()\n",
    "                \n",
    "\n",
    "    # Compute and print average loss per sample\n",
    "    loss_sum /= len(y)\n",
    "    print(f\"Avg loss: {loss_sum}!\")\n",
    "    return loss_sum\n",
    "\n",
    "def create_sequences(data, sequence_length):\n",
    "    \"\"\" 将时间序列数据划分为样本，每个样本长度为 sequence_length \"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(len(data) - sequence_length):\n",
    "        # 获取长度为 sequence_length 的序列\n",
    "        seq = data[i:i+sequence_length]\n",
    "        # 目标是下一个时间步的数据\n",
    "        target = data[i+sequence_length]\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "    \n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "def create_sequences_diff(obs_df, action_df, sequence_length):\n",
    "    \"\"\"\n",
    "    生成时间序列样本，并将 action 作为 X 的一部分。\n",
    "    Y 是 observation 的差值 (next_obs - current_obs)。\n",
    "    \n",
    "    obs_df: 包含 observation 的 DataFrame\n",
    "    action_df: 包含 action 的 DataFrame\n",
    "    sequence_length: 序列的长度\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    # 计算整个 obs_df 的差分\n",
    "    obs_diff_df = obs_df.diff().dropna().reset_index(drop=True)\n",
    "    \n",
    "    # 确保 obs_df 和 action_df 长度相同\n",
    "    assert len(obs_df) == len(action_df), \"Observation and Action dataframes must have the same length.\"\n",
    "    \n",
    "    for i in range(len(obs_diff_df) - sequence_length):  # 确保不会越界\n",
    "        # 获取长度为 sequence_length 的 observation 和 action 序列\n",
    "        obs_seq = obs_df.iloc[i:i+sequence_length].values\n",
    "        action_seq = action_df.iloc[i:i+sequence_length].values\n",
    "        \n",
    "        # 将 observation 和 action 合并为 X，action 是 X 的一部分\n",
    "        sequence = np.concatenate([obs_seq, action_seq], axis=1)\n",
    "        \n",
    "        # 获取目标 Y，使用预计算的 obs_diff_df 差分序列\n",
    "        target = obs_diff_df.iloc[i:i+sequence_length].values\n",
    "        \n",
    "        sequences.append(sequence)\n",
    "        targets.append(target)\n",
    "    \n",
    "    return np.array(sequences), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e68a823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_diff(obs_df, action_df, sequence_length):\n",
    "    \"\"\"\n",
    "    生成时间序列样本，并将 action 作为 X 的一部分。\n",
    "    Y 是 observation 的差值 (next_obs - current_obs)。\n",
    "    \n",
    "    obs_df: 包含 observation 的 DataFrame\n",
    "    action_df: 包含 action 的 DataFrame\n",
    "    sequence_length: 序列的长度\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    # 计算整个 obs_df 的差分\n",
    "    obs_diff_df = obs_df.diff().dropna().reset_index(drop=True)\n",
    "    \n",
    "    # 确保 obs_df 和 action_df 长度相同\n",
    "    assert len(obs_df) == len(action_df) + 1, \"Observation and Action dataframes length.\"\n",
    "    \n",
    "    for i in range(len(obs_diff_df) - sequence_length):  # 确保不会越界\n",
    "        # 获取长度为 sequence_length 的 observation 和 action 序列\n",
    "        obs_seq = obs_df.iloc[i:i+sequence_length].values\n",
    "        action_seq = action_df.iloc[i:i+sequence_length].values\n",
    "        \n",
    "        # 将 observation 和 action 合并为 X，action 是 X 的一部分\n",
    "        sequence = np.concatenate([obs_seq, action_seq], axis=1)\n",
    "        \n",
    "        # 获取目标 Y，使用预计算的 obs_diff_df 差分序列\n",
    "        target = obs_diff_df.iloc[i+sequence_length].values\n",
    "        \n",
    "        sequences.append(sequence)\n",
    "        targets.append(target)\n",
    "    \n",
    "    return np.array(sequences), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3040e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578e72aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a758708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8198f63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps_real_per_round = 500\n",
    "timesteps_fc_per_round = timesteps_real_per_round * 30\n",
    "epoch_per_round = 10\n",
    "rounds_num = 40\n",
    "batch_size_env_model = 128\n",
    "env_paras = [10.0, 10.0, 10.0]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bfe09cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "CLIENTS_NUM = len(env_paras)\n",
    "env_models = []\n",
    "MB_env = TimeLimit(MB_PendulumEnv(env_models,device), max_episode_steps = 200)\n",
    "    \n",
    "    # Global_RL = PPO(\"MlpPolicy\", MB_env, verbose=1)\n",
    "Global_RL = TRPO(\"MlpPolicy\", MB_env, verbose=1)\n",
    "    \n",
    "train_loss_records = []\n",
    "test_loss_records = []\n",
    "    \n",
    "# env_theta = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "real_envs = []\n",
    "Clients = []\n",
    "for i in range(CLIENTS_NUM):\n",
    "    real_envs.append( TimeLimit(PendulumEnv(g=env_paras[i]), max_episode_steps=200) )\n",
    "    policy_net = Global_RL\n",
    "    agent = SB3Agent(policy_net)\n",
    "    client = FRLClient(real_envs[i], agent, lr = 3e-4, hidden_size = 256, device = device)\n",
    "    Clients.append(client)\n",
    "    env_model = copy.deepcopy(client.model)\n",
    "    env_models.append(env_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c83069af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Clients[0].dataset_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c6a6fd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0         1         2\n",
      "0    0.379969  0.924999  0.399528\n",
      "1    0.332464  0.943116  1.016949\n",
      "2    0.261965  0.965077  1.477137\n",
      "3    0.161589  0.986858  2.055147\n",
      "4    0.037517  0.999296  2.495488\n",
      "..        ...       ...       ...\n",
      "196 -0.170687  0.985325  2.391550\n",
      "197 -0.321279  0.946985  3.111057\n",
      "198 -0.483706  0.875231  3.556087\n",
      "199 -0.648809  0.760951  4.022689\n",
      "200 -0.801152  0.598461  4.463996\n",
      "\n",
      "[201 rows x 3 columns]\n",
      "201\n",
      "200\n",
      "            0\n",
      "0   -0.508860\n",
      "1   -1.647656\n",
      "2   -0.971991\n",
      "3   -1.998682\n",
      "4    1.081602\n",
      "..        ...\n",
      "195  1.348756\n",
      "196 -0.129912\n",
      "197 -1.768054\n",
      "198 -1.265471\n",
      "199 -0.862714\n",
      "\n",
      "[200 rows x 1 columns]\n",
      "            0         1         2\n",
      "0   -0.905639 -0.424049  0.747747\n",
      "1   -0.897235 -0.441554  0.388370\n",
      "2   -0.899665 -0.436580 -0.110724\n",
      "3   -0.903978 -0.427579 -0.199615\n",
      "4   -0.911937 -0.410330 -0.379948\n",
      "..        ...       ...       ...\n",
      "196 -0.970328  0.241794  3.261554\n",
      "197 -0.996728  0.080829  3.265942\n",
      "198 -0.995724 -0.092380  3.468593\n",
      "199 -0.964240 -0.265029  3.514438\n",
      "200 -0.905788 -0.423732  3.386544\n",
      "\n",
      "[201 rows x 3 columns]\n",
      "201\n",
      "200\n",
      "            0\n",
      "0   -0.275603\n",
      "1   -1.119520\n",
      "2    1.590291\n",
      "3    0.935676\n",
      "4    1.878545\n",
      "..        ...\n",
      "195 -0.935180\n",
      "196 -1.179714\n",
      "197  0.946859\n",
      "198  0.767535\n",
      "199  0.472516\n",
      "\n",
      "[200 rows x 1 columns]\n",
      "            0         1         2\n",
      "0    0.997030  0.077012  0.321942\n",
      "1    0.995880  0.090677  0.274261\n",
      "2    0.995663  0.093031  0.047287\n",
      "3    0.995517  0.094586  0.031222\n",
      "4    0.996134  0.087852 -0.135239\n",
      "..        ...       ...       ...\n",
      "196  0.349679  0.936870 -4.745896\n",
      "197  0.524267  0.851554 -3.892515\n",
      "198  0.665620  0.746291 -3.529405\n",
      "199  0.772613  0.634877 -3.092459\n",
      "200  0.855477  0.517841 -2.870472\n",
      "\n",
      "[201 rows x 3 columns]\n",
      "201\n",
      "200\n",
      "            0\n",
      "0   -0.702936\n",
      "1   -1.966543\n",
      "2   -0.572262\n",
      "3   -1.582665\n",
      "4   -0.289151\n",
      "..        ...\n",
      "195 -0.943514\n",
      "196  1.004854\n",
      "197 -1.837037\n",
      "198 -0.818480\n",
      "199 -1.694470\n",
      "\n",
      "[200 rows x 1 columns]\n",
      "            0         1         2\n",
      "0   -0.670877 -0.741568 -0.018239\n",
      "1   -0.690929 -0.722923 -0.547629\n",
      "2   -0.720420 -0.693539 -0.832684\n",
      "3   -0.756855 -0.653582 -1.081626\n",
      "4   -0.802787 -0.596266 -1.469316\n",
      "..        ...       ...       ...\n",
      "196 -0.796029 -0.605258  1.464286\n",
      "197 -0.762665 -0.646794  1.065643\n",
      "198 -0.734331 -0.678792  0.854877\n",
      "199 -0.727162 -0.686466  0.210020\n",
      "200 -0.735505 -0.677519 -0.244654\n",
      "\n",
      "[201 rows x 3 columns]\n",
      "201\n",
      "200\n",
      "            0\n",
      "0    0.178574\n",
      "1    1.714249\n",
      "2    1.808081\n",
      "3    0.683312\n",
      "4   -1.464130\n",
      "..        ...\n",
      "195 -1.426891\n",
      "196  0.368674\n",
      "197  1.828863\n",
      "198 -0.905089\n",
      "199  0.401170\n",
      "\n",
      "[200 rows x 1 columns]\n",
      "            0         1         2\n",
      "0    0.829109 -0.559086  0.110823\n",
      "1    0.819766 -0.572699 -0.330219\n",
      "2    0.802476 -0.596684 -0.591371\n",
      "3    0.773460 -0.633846 -0.943045\n",
      "4    0.726435 -0.687235 -1.423208\n",
      "..        ...       ...       ...\n",
      "196  0.679832 -0.733368  0.980801\n",
      "197  0.698304 -0.715801  0.509836\n",
      "198  0.690623 -0.723215 -0.213495\n",
      "199  0.665059 -0.746791 -0.695556\n",
      "200  0.623308 -0.781976 -1.092135\n",
      "\n",
      "[201 rows x 3 columns]\n",
      "201\n",
      "200\n",
      "            0\n",
      "0   -0.144850\n",
      "1    1.122481\n",
      "2    0.638930\n",
      "3   -0.031859\n",
      "4    0.025501\n",
      "..        ...\n",
      "195  1.482296\n",
      "196  0.527075\n",
      "197 -1.243200\n",
      "198  0.402332\n",
      "199  1.090095\n",
      "\n",
      "[200 rows x 1 columns]\n",
      "            0         1         2\n",
      "0    0.995232 -0.097531  0.264079\n",
      "1    0.994915 -0.100721 -0.064114\n",
      "2    0.995203 -0.097834  0.058026\n",
      "3    0.993740 -0.111717 -0.279191\n",
      "4    0.992973 -0.118343 -0.133406\n",
      "..        ...       ...       ...\n",
      "196  0.740180  0.672409 -3.124506\n",
      "197  0.820548  0.571578 -2.580613\n",
      "198  0.880726  0.473626 -2.300485\n",
      "199  0.918324  0.395829 -1.728649\n",
      "200  0.940608  0.339496 -1.211798\n",
      "\n",
      "[201 rows x 3 columns]\n",
      "201\n",
      "200\n",
      "            0\n",
      "0   -1.700299\n",
      "1    1.317874\n",
      "2   -1.758943\n",
      "3    1.530484\n",
      "4   -0.892019\n",
      "..        ...\n",
      "195  1.752467\n",
      "196  0.263909\n",
      "197 -0.990367\n",
      "198  1.444107\n",
      "199  1.466527\n",
      "\n",
      "[200 rows x 1 columns]\n",
      "            0         1         2\n",
      "0    0.296994 -0.954879 -0.793168\n",
      "1    0.216177 -0.976354 -1.672911\n",
      "2    0.104236 -0.994553 -2.269439\n",
      "3   -0.039642 -0.999214 -2.881553\n",
      "4   -0.222516 -0.974929 -3.694854\n",
      "..        ...       ...       ...\n",
      "196  0.876867 -0.480734  4.052866\n",
      "197  0.947867 -0.318667  3.543374\n",
      "198  0.986074 -0.166305  3.144834\n",
      "199  0.999604 -0.028145  2.778653\n",
      "200  0.994169  0.107837  2.723920\n",
      "\n",
      "[201 rows x 3 columns]\n",
      "201\n",
      "200\n",
      "            0\n",
      "0   -1.090561\n",
      "1    0.904919\n",
      "2    0.892003\n",
      "3   -0.425938\n",
      "4    0.153117\n",
      "..        ...\n",
      "195  1.341047\n",
      "196 -0.992944\n",
      "197 -1.063600\n",
      "198 -1.609678\n",
      "199 -0.224164\n",
      "\n",
      "[200 rows x 1 columns]\n",
      "            0         1         2\n",
      "0    0.336371  0.941730 -0.138881\n",
      "1    0.310575  0.950549  0.545260\n",
      "2    0.257655  0.966237  1.104075\n",
      "3    0.157854  0.987462  2.041540\n",
      "4    0.014556  0.999894  2.879219\n",
      "..        ...       ...       ...\n",
      "196 -0.960271  0.279068 -7.379543\n",
      "197 -0.801629  0.597822 -7.159161\n",
      "198 -0.563276  0.826269 -6.633407\n",
      "199 -0.293914  0.955832 -6.000521\n",
      "200 -0.029073  0.999577 -5.384830\n",
      "\n",
      "[201 rows x 3 columns]\n",
      "201\n",
      "200\n",
      "            0\n",
      "0   -0.147712\n",
      "1   -1.027310\n",
      "2    1.418579\n",
      "3    0.647216\n",
      "4    1.691967\n",
      "..        ...\n",
      "195  1.029732\n",
      "196  0.073874\n",
      "197  0.515918\n",
      "198  0.087893\n",
      "199 -0.674559\n",
      "\n",
      "[200 rows x 1 columns]\n",
      "            0         1         2\n",
      "0   -0.729945 -0.683506  0.331255\n",
      "1   -0.731346 -0.682007 -0.041033\n",
      "2   -0.751953 -0.659217 -0.614523\n",
      "3   -0.791363 -0.611347 -1.240315\n",
      "4   -0.838487 -0.544922 -1.629303\n",
      "..        ...       ...       ...\n",
      "196 -0.885208 -0.465195  0.351054\n",
      "197 -0.879631 -0.475656  0.237098\n",
      "198 -0.877929 -0.478791  0.071333\n",
      "199 -0.878204 -0.478286 -0.011489\n",
      "200 -0.885538 -0.464566 -0.311156\n",
      "\n",
      "[201 rows x 3 columns]\n",
      "201\n",
      "200\n",
      "            0\n",
      "0    0.935610\n",
      "1   -0.413236\n",
      "2   -0.875859\n",
      "3    0.463474\n",
      "4    1.977666\n",
      "..        ...\n",
      "195 -0.348627\n",
      "196  1.566273\n",
      "197  1.273182\n",
      "198  1.841806\n",
      "199  0.393648\n",
      "\n",
      "[200 rows x 1 columns]\n",
      "            0         1         2\n",
      "0   -0.960129 -0.279559  0.632178\n",
      "1   -0.952162 -0.305594  0.544557\n",
      "2   -0.950672 -0.310197  0.096763\n",
      "3   -0.956864 -0.290537 -0.412243\n",
      "4   -0.968022 -0.250863 -0.824321\n",
      "..        ...       ...       ...\n",
      "196 -0.887618  0.460580  1.473716\n",
      "197 -0.926171  0.377104  1.839630\n",
      "198 -0.958002  0.286762  1.916450\n",
      "199 -0.982921  0.184028  2.115236\n",
      "200 -0.997471  0.071077  2.278915\n",
      "\n",
      "[201 rows x 3 columns]\n",
      "201\n",
      "200\n",
      "            0\n",
      "0    0.813649\n",
      "1   -1.457324\n",
      "2   -1.842385\n",
      "3   -1.294504\n",
      "4   -1.961260\n",
      "..        ...\n",
      "195  1.245215\n",
      "196  0.136523\n",
      "197 -1.373389\n",
      "198 -0.108566\n",
      "199  0.171053\n",
      "\n",
      "[200 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "n = 2000\n",
    "seq_length = 3\n",
    "\n",
    "env = Clients[0].env\n",
    "        \n",
    "# observation, info = env.reset()\n",
    "# df = pd.DataFrame(observation).T\n",
    "# actions = []\n",
    "dataset_X_list = []\n",
    "dataset_y_list = []\n",
    "cnt = 0\n",
    "while(cnt < n):\n",
    "    observation, info = env.reset()\n",
    "    obs_df = pd.DataFrame(observation).T \n",
    "    actions = []\n",
    "    done = False\n",
    "    truncated = False\n",
    "    while done == False and truncated == False:\n",
    "        action = env.action_space.sample()\n",
    "#         print(obs_df)\n",
    "#         print(len(obs_df))\n",
    "#         print(observation)\n",
    "        \n",
    "        actions.append(action)\n",
    "        observation, reward, done, truncated, info = env.step(action)\n",
    "        obs_df.loc[len(obs_df)] = observation\n",
    "        cnt += 1\n",
    "        if cnt >= n:\n",
    "            break\n",
    "    \n",
    "    print(obs_df)\n",
    "    print(len(obs_df))\n",
    "    print(len(actions))\n",
    "    action_df = pd.DataFrame(actions)\n",
    "    print(action_df)\n",
    "        \n",
    "            # df_temp[\"actions\"] = actions\n",
    "    dataset_X_temp, dataset_y_temp = create_sequences_diff(obs_df, action_df, seq_length)\n",
    "    dataset_X_list.append(dataset_X_temp)\n",
    "    dataset_y_list.append(dataset_y_temp)\n",
    "        \n",
    "            # 展平数据集\n",
    "dataset_X = np.concatenate(dataset_X_list, axis=0)\n",
    "dataset_y = np.concatenate(dataset_y_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d1d32e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1970, 3, 4)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e88fa367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1970, 3)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fea282ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM expects inputs of shape (batch_size, sequence_length, feature_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Only need the output from the last time step\n",
    "        out = lstm_out[:, -1, :]  # shape: (batch_size, hidden_size)\n",
    "        out = self.fc(out)  # shape: (batch_size, output_size)\n",
    "        return out\n",
    "\n",
    "# Example model parameters\n",
    "input_size = 4  # number of features\n",
    "hidden_size = 64  # hidden size for LSTM\n",
    "output_size = 3  # number of output features\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1aba582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 假设 dataset_X 和 dataset_y 是 NumPy 数组\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_X, dataset_y, test_size=0.2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "773de663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1576, 3, 4)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "67d97d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据转换为 PyTorch 张量\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8de6b88d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [89]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 12\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# 每个 epoch 后评估模型在测试集上的表现\u001b[39;00m\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mmodel_train\u001b[1;34m(X, y, model, loss_fn, optimizer, batch_size)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# Compute loss for each sample in the batch\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(batch_size, \u001b[38;5;28mlen\u001b[39m(train_X))):\n\u001b[1;32m--> 251\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_fn(pred\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32), train_y[k]\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m    253\u001b[0m loss_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "Input \u001b[1;32mIn [80]\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Only need the output from the last time step\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mlstm_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# shape: (batch_size, hidden_size)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out)  \u001b[38;5;66;03m# shape: (batch_size, output_size)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "# 定义超参数\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = model_train(X_train, y_train, model, loss_fn, optimizer, batch_size)\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss}')\n",
    "\n",
    "    # 每个 epoch 后评估模型在测试集上的表现\n",
    "    test_loss = model_test(X_testr, y_test, model, loss_fn, batch_size)\n",
    "    print(f'Epoch {epoch+1}, Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b6c88a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1576, 3, 4)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6c0a23cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换为 PyTorch 张量\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).cuda()\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).cuda()\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).cuda()\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "96c90fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)  # LSTM 输出 (batch_size, seq_len, hidden_size)\n",
    "        out = lstm_out[:, -1, :]    # 取最后一个时间步的输出 (batch_size, hidden_size)\n",
    "        out = self.fc(out)          # 通过全连接层 (batch_size, output_size)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f39dc99d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "05d7555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(X, y, model, loss_fn, optimizer, batch_size):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "\n",
    "    # Loop through the dataset in batches\n",
    "    for i in range(0, len(y), batch_size):\n",
    "        # Convert batch data to torch tensors and move to GPU\n",
    "        train_X = torch.tensor(X[i: i + batch_size], dtype=torch.float32).cuda()\n",
    "        train_y = torch.tensor(y[i: i + batch_size], dtype=torch.float32).cuda()\n",
    "\n",
    "        # Forward pass\n",
    "        pred = model(train_X)\n",
    "\n",
    "        # Compute loss for the entire batch\n",
    "        loss = loss_fn(pred, train_y)\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        # Zero the gradients, perform backpropagation, and update the weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Return average loss per sample\n",
    "    return loss_sum / len(y)\n",
    "\n",
    "\n",
    "def model_test(X, y, model, loss_fn, batch_size):\n",
    "    loss_sum = 0\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        # Loop through the dataset in batches\n",
    "        for i in range(0, len(y), batch_size):\n",
    "            # Convert batch data to torch tensors and move to GPU\n",
    "            test_X = torch.tensor(X[i: i + batch_size], dtype=torch.float32).cuda()\n",
    "            test_y = torch.tensor(y[i: i + batch_size], dtype=torch.float32).cuda()\n",
    "\n",
    "            # Forward pass\n",
    "            pred = model(test_X)\n",
    "\n",
    "            # Compute loss for the entire batch\n",
    "            loss = loss_fn(pred, test_y)\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "    # Compute and print average loss per sample\n",
    "    avg_loss = loss_sum / len(y)\n",
    "    print(f\"Avg loss: {avg_loss}!\")\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d83849b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\syf\\AppData\\Local\\Temp\\ipykernel_21636\\3360410052.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_X = torch.tensor(X[i: i + batch_size], dtype=torch.float32).cuda()\n",
      "C:\\Users\\syf\\AppData\\Local\\Temp\\ipykernel_21636\\3360410052.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_y = torch.tensor(y[i: i + batch_size], dtype=torch.float32).cuda()\n",
      "C:\\Users\\syf\\AppData\\Local\\Temp\\ipykernel_21636\\3360410052.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_X = torch.tensor(X[i: i + batch_size], dtype=torch.float32).cuda()\n",
      "C:\\Users\\syf\\AppData\\Local\\Temp\\ipykernel_21636\\3360410052.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_y = torch.tensor(y[i: i + batch_size], dtype=torch.float32).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.0007351414149220523!\n",
      "Epoch 1/50, Train Loss: 0.00269769, Test Loss: 0.00073514\n",
      "Avg loss: 0.00039267516576774837!\n",
      "Epoch 2/50, Train Loss: 0.00099132, Test Loss: 0.00039268\n",
      "Avg loss: 0.00038170526607841405!\n",
      "Epoch 3/50, Train Loss: 0.00056791, Test Loss: 0.00038171\n",
      "Avg loss: 0.00035870245889509995!\n",
      "Epoch 4/50, Train Loss: 0.00045892, Test Loss: 0.00035870\n",
      "Avg loss: 0.00034204761374760704!\n",
      "Epoch 5/50, Train Loss: 0.00040270, Test Loss: 0.00034205\n",
      "Avg loss: 0.0003361879389373784!\n",
      "Epoch 6/50, Train Loss: 0.00037524, Test Loss: 0.00033619\n",
      "Avg loss: 0.0003339337683779183!\n",
      "Epoch 7/50, Train Loss: 0.00036032, Test Loss: 0.00033393\n",
      "Avg loss: 0.00033317589872341774!\n",
      "Epoch 8/50, Train Loss: 0.00035165, Test Loss: 0.00033318\n",
      "Avg loss: 0.0003330828645621157!\n",
      "Epoch 9/50, Train Loss: 0.00034661, Test Loss: 0.00033308\n",
      "Avg loss: 0.00033314858426146095!\n",
      "Epoch 10/50, Train Loss: 0.00034380, Test Loss: 0.00033315\n",
      "Avg loss: 0.0003331443353976846!\n",
      "Epoch 11/50, Train Loss: 0.00034220, Test Loss: 0.00033314\n",
      "Avg loss: 0.00033303036066103104!\n",
      "Epoch 12/50, Train Loss: 0.00034116, Test Loss: 0.00033303\n",
      "Avg loss: 0.0003328357154248178!\n",
      "Epoch 13/50, Train Loss: 0.00034034, Test Loss: 0.00033284\n",
      "Avg loss: 0.0003325988344734695!\n",
      "Epoch 14/50, Train Loss: 0.00033958, Test Loss: 0.00033260\n",
      "Avg loss: 0.0003323499869725426!\n",
      "Epoch 15/50, Train Loss: 0.00033884, Test Loss: 0.00033235\n",
      "Avg loss: 0.00033210918926610255!\n",
      "Epoch 16/50, Train Loss: 0.00033811, Test Loss: 0.00033211\n",
      "Avg loss: 0.00033188785123756996!\n",
      "Epoch 17/50, Train Loss: 0.00033740, Test Loss: 0.00033189\n",
      "Avg loss: 0.0003316912688976768!\n",
      "Epoch 18/50, Train Loss: 0.00033671, Test Loss: 0.00033169\n",
      "Avg loss: 0.00033152081559265626!\n",
      "Epoch 19/50, Train Loss: 0.00033604, Test Loss: 0.00033152\n",
      "Avg loss: 0.0003313758980180255!\n",
      "Epoch 20/50, Train Loss: 0.00033541, Test Loss: 0.00033138\n",
      "Avg loss: 0.00033125484617530995!\n",
      "Epoch 21/50, Train Loss: 0.00033479, Test Loss: 0.00033125\n",
      "Avg loss: 0.0003311559262444374!\n",
      "Epoch 22/50, Train Loss: 0.00033420, Test Loss: 0.00033116\n",
      "Avg loss: 0.00033107743277049006!\n",
      "Epoch 23/50, Train Loss: 0.00033362, Test Loss: 0.00033108\n",
      "Avg loss: 0.00033101801013545637!\n",
      "Epoch 24/50, Train Loss: 0.00033306, Test Loss: 0.00033102\n",
      "Avg loss: 0.00033097659228227766!\n",
      "Epoch 25/50, Train Loss: 0.00033252, Test Loss: 0.00033098\n",
      "Avg loss: 0.0003309524736277343!\n",
      "Epoch 26/50, Train Loss: 0.00033198, Test Loss: 0.00033095\n",
      "Avg loss: 0.00033094510814260107!\n",
      "Epoch 27/50, Train Loss: 0.00033145, Test Loss: 0.00033095\n",
      "Avg loss: 0.00033095426535999716!\n",
      "Epoch 28/50, Train Loss: 0.00033093, Test Loss: 0.00033095\n",
      "Avg loss: 0.00033097991573288656!\n",
      "Epoch 29/50, Train Loss: 0.00033041, Test Loss: 0.00033098\n",
      "Avg loss: 0.0003310220344417591!\n",
      "Epoch 30/50, Train Loss: 0.00032990, Test Loss: 0.00033102\n",
      "Avg loss: 0.0003310807715855577!\n",
      "Epoch 31/50, Train Loss: 0.00032939, Test Loss: 0.00033108\n",
      "Avg loss: 0.00033115636708621444!\n",
      "Epoch 32/50, Train Loss: 0.00032888, Test Loss: 0.00033116\n",
      "Avg loss: 0.00033124896986079093!\n",
      "Epoch 33/50, Train Loss: 0.00032838, Test Loss: 0.00033125\n",
      "Avg loss: 0.0003313588257406266!\n",
      "Epoch 34/50, Train Loss: 0.00032787, Test Loss: 0.00033136\n",
      "Avg loss: 0.0003314860292762367!\n",
      "Epoch 35/50, Train Loss: 0.00032737, Test Loss: 0.00033149\n",
      "Avg loss: 0.0003316306561080332!\n",
      "Epoch 36/50, Train Loss: 0.00032688, Test Loss: 0.00033163\n",
      "Avg loss: 0.0003317926022304496!\n",
      "Epoch 37/50, Train Loss: 0.00032638, Test Loss: 0.00033179\n",
      "Avg loss: 0.00033197167499681113!\n",
      "Epoch 38/50, Train Loss: 0.00032589, Test Loss: 0.00033197\n",
      "Avg loss: 0.0003321674548392066!\n",
      "Epoch 39/50, Train Loss: 0.00032540, Test Loss: 0.00033217\n",
      "Avg loss: 0.0003323793803639521!\n",
      "Epoch 40/50, Train Loss: 0.00032492, Test Loss: 0.00033238\n",
      "Avg loss: 0.0003326066112533439!\n",
      "Epoch 41/50, Train Loss: 0.00032444, Test Loss: 0.00033261\n",
      "Avg loss: 0.0003328482256398588!\n",
      "Epoch 42/50, Train Loss: 0.00032397, Test Loss: 0.00033285\n",
      "Avg loss: 0.0003331029281814389!\n",
      "Epoch 43/50, Train Loss: 0.00032350, Test Loss: 0.00033310\n",
      "Avg loss: 0.00033336943890048467!\n",
      "Epoch 44/50, Train Loss: 0.00032304, Test Loss: 0.00033337\n",
      "Avg loss: 0.0003336461646208152!\n",
      "Epoch 45/50, Train Loss: 0.00032259, Test Loss: 0.00033365\n",
      "Avg loss: 0.00033393153225823405!\n",
      "Epoch 46/50, Train Loss: 0.00032215, Test Loss: 0.00033393\n",
      "Avg loss: 0.0003342237122602723!\n",
      "Epoch 47/50, Train Loss: 0.00032172, Test Loss: 0.00033422\n",
      "Avg loss: 0.0003345210582660842!\n",
      "Epoch 48/50, Train Loss: 0.00032129, Test Loss: 0.00033452\n",
      "Avg loss: 0.00033482178799845843!\n",
      "Epoch 49/50, Train Loss: 0.00032088, Test Loss: 0.00033482\n",
      "Avg loss: 0.00033512421491258037!\n",
      "Epoch 50/50, Train Loss: 0.00032047, Test Loss: 0.00033512\n"
     ]
    }
   ],
   "source": [
    "# 超参数设置\n",
    "input_size = 4  # 每个时间步的特征数\n",
    "hidden_size = 64  # LSTM 隐藏层大小\n",
    "output_size = 3  # 标签的特征数\n",
    "num_layers = 1  # LSTM 层数\n",
    "batch_size = 32  # 每个批次的样本数\n",
    "num_epochs = 50  # 训练轮数\n",
    "\n",
    "# 模型、损失函数和优化器\n",
    "model = LSTMModel(input_size, hidden_size, output_size, num_layers).cuda()\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练和测试循环\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = model_train(X_train_tensor, y_train_tensor, model, loss_fn, optimizer, batch_size)\n",
    "    test_loss = model_test(X_test_tensor, y_test_tensor, model, loss_fn, batch_size)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.8f}, Test Loss: {test_loss:.8f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e3efbdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PredictionModel import PredictionModel\n",
    "HIDDEN_SIZE = 64\n",
    "class PredictionModel(nn.Module):\n",
    "    def __init__(self, obs_size, action_size, seq_length = 1 ,hidden_size = HIDDEN_SIZE):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential (\n",
    "            nn.Linear((obs_size + action_size) * seq_length, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, obs_size),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f536f7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_model = PredictionModel(obs_size = 3, action_size= 1, seq_length = 3, hidden_size= 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9f6be830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionModel(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=12, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c12e4d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm): LSTM(4, 64, batch_first=True)\n",
       "  (fc): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "687e8d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, T, F = dataset_X.shape\n",
    "X_reshaped = dataset_X.reshape(N, T * F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f92194e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "X_train_MLP, X_test_MLP, y_train_MLP, y_test_MLP = train_test_split(X_reshaped, dataset_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "40cca4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionModel(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=12, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7d88e771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.0007704195429377145!\n",
      "Epoch 1, MLP Train Loss: 0.0020958492041891676, MLP Test Loss: 0.0007704195429377145\n",
      "Avg loss: 0.000533101775300533!\n",
      "Epoch 2, MLP Train Loss: 0.0005555438063329625, MLP Test Loss: 0.000533101775300533\n",
      "Avg loss: 0.0004862645040573505!\n",
      "Epoch 3, MLP Train Loss: 0.0004457204075539626, MLP Test Loss: 0.0004862645040573505\n",
      "Avg loss: 0.00046662119959438514!\n",
      "Epoch 4, MLP Train Loss: 0.00041107271223560655, MLP Test Loss: 0.00046662119959438514\n",
      "Avg loss: 0.00045430948390497774!\n",
      "Epoch 5, MLP Train Loss: 0.0003921814412965917, MLP Test Loss: 0.00045430948390497774\n",
      "Avg loss: 0.0004485245855515681!\n",
      "Epoch 6, MLP Train Loss: 0.00038074357594661224, MLP Test Loss: 0.0004485245855515681\n",
      "Avg loss: 0.00044646360178662435!\n",
      "Epoch 7, MLP Train Loss: 0.00037199085835972563, MLP Test Loss: 0.00044646360178662435\n",
      "Avg loss: 0.0004437722781020675!\n",
      "Epoch 8, MLP Train Loss: 0.00036390443766243794, MLP Test Loss: 0.0004437722781020675\n",
      "Avg loss: 0.00043883859439927913!\n",
      "Epoch 9, MLP Train Loss: 0.0003585242973593286, MLP Test Loss: 0.00043883859439927913\n",
      "Avg loss: 0.00043775400543954166!\n",
      "Epoch 10, MLP Train Loss: 0.00035390496043592387, MLP Test Loss: 0.00043775400543954166\n",
      "Avg loss: 0.0004363513745480988!\n",
      "Epoch 11, MLP Train Loss: 0.00034876023289194416, MLP Test Loss: 0.0004363513745480988\n",
      "Avg loss: 0.00043520491409498424!\n",
      "Epoch 12, MLP Train Loss: 0.0003442273139575411, MLP Test Loss: 0.00043520491409498424\n",
      "Avg loss: 0.0004362932448913603!\n",
      "Epoch 13, MLP Train Loss: 0.00034098838205416193, MLP Test Loss: 0.0004362932448913603\n",
      "Avg loss: 0.00043290478607573485!\n",
      "Epoch 14, MLP Train Loss: 0.0003377210303342093, MLP Test Loss: 0.00043290478607573485\n",
      "Avg loss: 0.00043435354816353866!\n",
      "Epoch 15, MLP Train Loss: 0.000335460494161314, MLP Test Loss: 0.00043435354816353866\n",
      "Avg loss: 0.00043572533667692677!\n",
      "Epoch 16, MLP Train Loss: 0.00033228197974167047, MLP Test Loss: 0.00043572533667692677\n",
      "Avg loss: 0.0004358628864026614!\n",
      "Epoch 17, MLP Train Loss: 0.0003294413002222058, MLP Test Loss: 0.0004358628864026614\n",
      "Avg loss: 0.00043471779929578!\n",
      "Epoch 18, MLP Train Loss: 0.00032677643029444713, MLP Test Loss: 0.00043471779929578\n",
      "Avg loss: 0.00043570233726410694!\n",
      "Epoch 19, MLP Train Loss: 0.00032589483213124013, MLP Test Loss: 0.00043570233726410694\n",
      "Avg loss: 0.0004359645518441188!\n",
      "Epoch 20, MLP Train Loss: 0.000322824026112856, MLP Test Loss: 0.0004359645518441188\n",
      "Avg loss: 0.0004358922963404111!\n",
      "Epoch 21, MLP Train Loss: 0.0003213364672894367, MLP Test Loss: 0.0004358922963404111\n",
      "Avg loss: 0.00043574352819604923!\n",
      "Epoch 22, MLP Train Loss: 0.0003191077252046257, MLP Test Loss: 0.00043574352819604923\n",
      "Avg loss: 0.0004376606557158952!\n",
      "Epoch 23, MLP Train Loss: 0.0003181113365111012, MLP Test Loss: 0.0004376606557158952\n",
      "Avg loss: 0.0004394084905382945!\n",
      "Epoch 24, MLP Train Loss: 0.0003159634360031882, MLP Test Loss: 0.0004394084905382945\n",
      "Avg loss: 0.0004380570351131979!\n",
      "Epoch 25, MLP Train Loss: 0.00031395385403488644, MLP Test Loss: 0.0004380570351131979\n",
      "Avg loss: 0.0004391322304792513!\n",
      "Epoch 26, MLP Train Loss: 0.0003122284714142002, MLP Test Loss: 0.0004391322304792513\n",
      "Avg loss: 0.00043754766785024385!\n",
      "Epoch 27, MLP Train Loss: 0.00031036260504710493, MLP Test Loss: 0.00043754766785024385\n",
      "Avg loss: 0.00043867879693732046!\n",
      "Epoch 28, MLP Train Loss: 0.0003090176476061647, MLP Test Loss: 0.00043867879693732046\n",
      "Avg loss: 0.0004396819188094079!\n",
      "Epoch 29, MLP Train Loss: 0.00030589636461336527, MLP Test Loss: 0.0004396819188094079\n",
      "Avg loss: 0.0004407697532155792!\n",
      "Epoch 30, MLP Train Loss: 0.0003049110796959683, MLP Test Loss: 0.0004407697532155792\n",
      "Avg loss: 0.0004381928049254841!\n",
      "Epoch 31, MLP Train Loss: 0.00030272243027977156, MLP Test Loss: 0.0004381928049254841\n",
      "Avg loss: 0.0004426783995542127!\n",
      "Epoch 32, MLP Train Loss: 0.00030064320122009984, MLP Test Loss: 0.0004426783995542127\n",
      "Avg loss: 0.00044103757700490466!\n",
      "Epoch 33, MLP Train Loss: 0.00029897116577290564, MLP Test Loss: 0.00044103757700490466\n",
      "Avg loss: 0.0004431411935971473!\n",
      "Epoch 34, MLP Train Loss: 0.0002968735259551044, MLP Test Loss: 0.0004431411935971473\n",
      "Avg loss: 0.00044519957485916045!\n",
      "Epoch 35, MLP Train Loss: 0.00029512086935190075, MLP Test Loss: 0.00044519957485916045\n",
      "Avg loss: 0.0004474501276795332!\n",
      "Epoch 36, MLP Train Loss: 0.0002942036262354081, MLP Test Loss: 0.0004474501276795332\n",
      "Avg loss: 0.00044771719506473713!\n",
      "Epoch 37, MLP Train Loss: 0.0002928937638774136, MLP Test Loss: 0.00044771719506473713\n",
      "Avg loss: 0.00044878270373050936!\n",
      "Epoch 38, MLP Train Loss: 0.0002911671538728521, MLP Test Loss: 0.00044878270373050936\n",
      "Avg loss: 0.0004477465695460436!\n",
      "Epoch 39, MLP Train Loss: 0.0002893007408855908, MLP Test Loss: 0.0004477465695460436\n",
      "Avg loss: 0.000451875514903952!\n",
      "Epoch 40, MLP Train Loss: 0.000287529085242771, MLP Test Loss: 0.000451875514903952\n",
      "Avg loss: 0.00045032036221269423!\n",
      "Epoch 41, MLP Train Loss: 0.0002866946453308961, MLP Test Loss: 0.00045032036221269423\n",
      "Avg loss: 0.00045248484569909004!\n",
      "Epoch 42, MLP Train Loss: 0.0002851901208948779, MLP Test Loss: 0.00045248484569909004\n",
      "Avg loss: 0.0004519143018890456!\n",
      "Epoch 43, MLP Train Loss: 0.00028390989539586953, MLP Test Loss: 0.0004519143018890456\n",
      "Avg loss: 0.00045564876122371797!\n",
      "Epoch 44, MLP Train Loss: 0.00028291841416955375, MLP Test Loss: 0.00045564876122371797\n",
      "Avg loss: 0.0004561130704386585!\n",
      "Epoch 45, MLP Train Loss: 0.0002823210983811334, MLP Test Loss: 0.0004561130704386585\n",
      "Avg loss: 0.0004535348291702682!\n",
      "Epoch 46, MLP Train Loss: 0.00028130328838198165, MLP Test Loss: 0.0004535348291702682\n",
      "Avg loss: 0.0004564004756396797!\n",
      "Epoch 47, MLP Train Loss: 0.00027943011385100295, MLP Test Loss: 0.0004564004756396797\n",
      "Avg loss: 0.00045581420807895925!\n",
      "Epoch 48, MLP Train Loss: 0.0002791083102861448, MLP Test Loss: 0.00045581420807895925\n",
      "Avg loss: 0.0004600479555916665!\n",
      "Epoch 49, MLP Train Loss: 0.00027810208627074805, MLP Test Loss: 0.0004600479555916665\n",
      "Avg loss: 0.0004604290461706631!\n",
      "Epoch 50, MLP Train Loss: 0.00027689817012878614, MLP Test Loss: 0.0004604290461706631\n"
     ]
    }
   ],
   "source": [
    "MLP_model = PredictionModel(obs_size = 3, action_size= 1, seq_length = 3, hidden_size= 64)\n",
    "MLP_model.cuda()\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(MLP_model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练 MLP\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = model_train(X_train_MLP, y_train_MLP, MLP_model, loss_fn, optimizer, batch_size)\n",
    "    test_loss = model_test(X_test_MLP, y_test_MLP, MLP_model, loss_fn, batch_size)\n",
    "    print(f'Epoch {epoch+1}, MLP Train Loss: {train_loss}, MLP Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7fb9c633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1576, 12)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_MLP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd977b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
