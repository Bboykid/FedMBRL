{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2303b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Envs.pendulum import PendulumEnv\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TimeLimit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "494268cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PendulumEnv()\n",
    "env = TimeLimit(env, max_episode_steps=200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fba20c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.70250255,  0.71168125, -0.7742709 ], dtype=float32), {})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eec815b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2024f7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MBEnvs.mb_pendulum2 import MB_PendulumEnv\n",
    "env_models = []\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MB_env = MB_PendulumEnv(env_models, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07c32890",
   "metadata": {},
   "outputs": [],
   "source": [
    "Global_RL = PPO(\"MlpPolicy\", MB_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24c97b36",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SB3Agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m agent_sb3 \u001b[38;5;241m=\u001b[39m \u001b[43mSB3Agent\u001b[49m(policy_net\u001b[38;5;241m=\u001b[39mGlobal_RL)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SB3Agent' is not defined"
     ]
    }
   ],
   "source": [
    "agent_sb3 = SB3Agent(policy_net=Global_RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c62b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_client = FRLClient(env=env,agent=agent_sb3, lr=0.0001, hidden_size= 256, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a500b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_client.sample_data(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4875b2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_client.dataset_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a891705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_client.dataset_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfdeb4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aed97cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c69c26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import TRPO\n",
    "from Client_diff_Gaussian import FRLClient\n",
    "from Agent import SB3Agent\n",
    "import copy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d1f4b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Envs.pendulum import PendulumEnv\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from MBEnvs.mb_pendulum2_gaussian import MB_PendulumEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e29110c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc0bbb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "------------------------------\n",
      "round: 0\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: -0.0021, Test Loss: -0.0044\n",
      "Epoch 2/100, Train Loss: -0.0034, Test Loss: -0.0062\n",
      "Epoch 3/100, Train Loss: -0.0046, Test Loss: -0.0083\n",
      "Epoch 4/100, Train Loss: -0.0060, Test Loss: -0.0107\n",
      "Epoch 5/100, Train Loss: -0.0074, Test Loss: -0.0131\n",
      "Epoch 6/100, Train Loss: -0.0091, Test Loss: -0.0157\n",
      "Epoch 7/100, Train Loss: -0.0109, Test Loss: -0.0189\n",
      "Epoch 8/100, Train Loss: -0.0129, Test Loss: -0.0222\n",
      "Epoch 9/100, Train Loss: -0.0152, Test Loss: -0.0257\n",
      "Epoch 10/100, Train Loss: -0.0175, Test Loss: -0.0289\n",
      "Epoch 11/100, Train Loss: -0.0194, Test Loss: -0.0316\n",
      "Epoch 12/100, Train Loss: -0.0207, Test Loss: -0.0335\n",
      "Epoch 13/100, Train Loss: -0.0221, Test Loss: -0.0352\n",
      "Epoch 14/100, Train Loss: -0.0229, Test Loss: -0.0306\n",
      "Epoch 15/100, Train Loss: -0.0221, Test Loss: -0.0329\n",
      "Epoch 16/100, Train Loss: -0.0236, Test Loss: -0.0344\n",
      "Epoch 17/100, Train Loss: -0.0252, Test Loss: -0.0248\n",
      "Epoch 18/100, Train Loss: -0.0227, Test Loss: -0.0334\n",
      "Epoch 19/100, Train Loss: -0.0243, Test Loss: -0.0389\n",
      "Epoch 20/100, Train Loss: -0.0264, Test Loss: -0.0346\n",
      "Epoch 21/100, Train Loss: -0.0271, Test Loss: -0.0364\n",
      "Epoch 22/100, Train Loss: -0.0247, Test Loss: -0.0340\n",
      "Epoch 23/100, Train Loss: -0.0249, Test Loss: -0.0363\n",
      "Epoch 24/100, Train Loss: -0.0264, Test Loss: -0.0371\n",
      "Epoch 25/100, Train Loss: -0.0283, Test Loss: -0.0387\n",
      "Epoch 26/100, Train Loss: -0.0291, Test Loss: -0.0379\n",
      "Epoch 27/100, Train Loss: -0.0298, Test Loss: -0.0398\n",
      "Epoch 28/100, Train Loss: -0.0309, Test Loss: -0.0393\n",
      "Epoch 29/100, Train Loss: -0.0315, Test Loss: -0.0389\n",
      "Epoch 30/100, Train Loss: -0.0321, Test Loss: -0.0362\n",
      "Epoch 31/100, Train Loss: -0.0305, Test Loss: -0.0188\n",
      "Epoch 32/100, Train Loss: -0.0203, Test Loss: -0.0271\n",
      "Epoch 33/100, Train Loss: -0.0268, Test Loss: -0.0266\n",
      "Epoch 34/100, Train Loss: -0.0250, Test Loss: -0.0301\n",
      "Epoch 35/100, Train Loss: -0.0289, Test Loss: -0.0333\n",
      "Epoch 36/100, Train Loss: -0.0298, Test Loss: -0.0325\n",
      "Epoch 37/100, Train Loss: -0.0306, Test Loss: -0.0350\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: -0.0025, Test Loss: -0.0056\n",
      "Epoch 2/100, Train Loss: -0.0037, Test Loss: -0.0073\n",
      "Epoch 3/100, Train Loss: -0.0049, Test Loss: -0.0095\n",
      "Epoch 4/100, Train Loss: -0.0063, Test Loss: -0.0117\n",
      "Epoch 5/100, Train Loss: -0.0077, Test Loss: -0.0141\n",
      "Epoch 6/100, Train Loss: -0.0093, Test Loss: -0.0167\n",
      "Epoch 7/100, Train Loss: -0.0109, Test Loss: -0.0196\n",
      "Epoch 8/100, Train Loss: -0.0127, Test Loss: -0.0223\n",
      "Epoch 9/100, Train Loss: -0.0144, Test Loss: -0.0253\n",
      "Epoch 10/100, Train Loss: -0.0160, Test Loss: -0.0276\n",
      "Epoch 11/100, Train Loss: -0.0169, Test Loss: -0.0290\n",
      "Epoch 12/100, Train Loss: -0.0176, Test Loss: -0.0316\n",
      "Epoch 13/100, Train Loss: -0.0182, Test Loss: -0.0294\n",
      "Epoch 14/100, Train Loss: -0.0189, Test Loss: -0.0286\n",
      "Epoch 15/100, Train Loss: -0.0190, Test Loss: -0.0336\n",
      "Epoch 16/100, Train Loss: -0.0206, Test Loss: -0.0321\n",
      "Epoch 17/100, Train Loss: -0.0198, Test Loss: -0.0333\n",
      "Epoch 18/100, Train Loss: -0.0199, Test Loss: -0.0331\n",
      "Epoch 19/100, Train Loss: -0.0183, Test Loss: -0.0331\n",
      "Epoch 20/100, Train Loss: -0.0181, Test Loss: -0.0289\n",
      "Epoch 21/100, Train Loss: -0.0205, Test Loss: -0.0272\n",
      "Epoch 22/100, Train Loss: -0.0203, Test Loss: -0.0322\n",
      "Epoch 23/100, Train Loss: -0.0217, Test Loss: -0.0343\n",
      "Epoch 24/100, Train Loss: -0.0213, Test Loss: -0.0353\n",
      "Epoch 25/100, Train Loss: -0.0222, Test Loss: -0.0348\n",
      "Epoch 26/100, Train Loss: -0.0223, Test Loss: -0.0349\n",
      "Epoch 27/100, Train Loss: -0.0230, Test Loss: -0.0363\n",
      "Epoch 28/100, Train Loss: -0.0235, Test Loss: -0.0374\n",
      "Epoch 29/100, Train Loss: -0.0240, Test Loss: -0.0375\n",
      "Epoch 30/100, Train Loss: -0.0246, Test Loss: -0.0378\n",
      "Epoch 31/100, Train Loss: -0.0252, Test Loss: -0.0384\n",
      "Epoch 32/100, Train Loss: -0.0238, Test Loss: -0.0383\n",
      "Epoch 33/100, Train Loss: -0.0228, Test Loss: -0.0313\n",
      "Epoch 34/100, Train Loss: -0.0202, Test Loss: -0.0364\n",
      "Epoch 35/100, Train Loss: -0.0248, Test Loss: -0.0380\n",
      "Epoch 36/100, Train Loss: -0.0240, Test Loss: -0.0337\n",
      "Epoch 37/100, Train Loss: -0.0235, Test Loss: -0.0347\n",
      "Epoch 38/100, Train Loss: -0.0222, Test Loss: -0.0301\n",
      "Epoch 39/100, Train Loss: -0.0213, Test Loss: -0.0362\n",
      "Epoch 40/100, Train Loss: -0.0225, Test Loss: -0.0375\n",
      "Epoch 41/100, Train Loss: -0.0242, Test Loss: -0.0175\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: -0.0017, Test Loss: -0.0054\n",
      "Epoch 2/100, Train Loss: -0.0032, Test Loss: -0.0078\n",
      "Epoch 3/100, Train Loss: -0.0047, Test Loss: -0.0106\n",
      "Epoch 4/100, Train Loss: -0.0064, Test Loss: -0.0135\n",
      "Epoch 5/100, Train Loss: -0.0082, Test Loss: -0.0168\n",
      "Epoch 6/100, Train Loss: -0.0101, Test Loss: -0.0204\n",
      "Epoch 7/100, Train Loss: -0.0122, Test Loss: -0.0243\n",
      "Epoch 8/100, Train Loss: -0.0143, Test Loss: -0.0281\n",
      "Epoch 9/100, Train Loss: -0.0162, Test Loss: -0.0312\n",
      "Epoch 10/100, Train Loss: -0.0177, Test Loss: -0.0318\n",
      "Epoch 11/100, Train Loss: -0.0184, Test Loss: -0.0369\n",
      "Epoch 12/100, Train Loss: -0.0199, Test Loss: -0.0384\n",
      "Epoch 13/100, Train Loss: -0.0207, Test Loss: -0.0391\n",
      "Epoch 14/100, Train Loss: -0.0212, Test Loss: -0.0428\n",
      "Epoch 15/100, Train Loss: -0.0226, Test Loss: -0.0427\n",
      "Epoch 16/100, Train Loss: -0.0229, Test Loss: -0.0419\n",
      "Epoch 17/100, Train Loss: -0.0217, Test Loss: -0.0386\n",
      "Epoch 18/100, Train Loss: -0.0212, Test Loss: -0.0422\n",
      "Epoch 19/100, Train Loss: -0.0225, Test Loss: -0.0413\n",
      "Epoch 20/100, Train Loss: -0.0232, Test Loss: -0.0410\n",
      "Epoch 21/100, Train Loss: -0.0227, Test Loss: -0.0445\n",
      "Epoch 22/100, Train Loss: -0.0244, Test Loss: -0.0457\n",
      "Epoch 23/100, Train Loss: -0.0244, Test Loss: -0.0454\n",
      "Epoch 24/100, Train Loss: -0.0252, Test Loss: -0.0477\n",
      "Epoch 25/100, Train Loss: -0.0258, Test Loss: -0.0489\n",
      "Epoch 26/100, Train Loss: -0.0266, Test Loss: -0.0487\n",
      "Epoch 27/100, Train Loss: -0.0271, Test Loss: -0.0505\n",
      "Epoch 28/100, Train Loss: -0.0267, Test Loss: -0.0505\n",
      "Epoch 29/100, Train Loss: -0.0260, Test Loss: -0.0508\n",
      "Epoch 30/100, Train Loss: -0.0257, Test Loss: -0.0519\n",
      "Epoch 31/100, Train Loss: -0.0235, Test Loss: -0.0311\n",
      "Epoch 32/100, Train Loss: -0.0189, Test Loss: -0.0468\n",
      "Epoch 33/100, Train Loss: -0.0234, Test Loss: -0.0509\n",
      "Epoch 34/100, Train Loss: -0.0267, Test Loss: -0.0511\n",
      "Epoch 35/100, Train Loss: -0.0277, Test Loss: -0.0489\n",
      "Epoch 36/100, Train Loss: -0.0274, Test Loss: -0.0435\n",
      "Epoch 37/100, Train Loss: -0.0269, Test Loss: -0.0490\n",
      "Epoch 38/100, Train Loss: -0.0277, Test Loss: -0.0520\n",
      "Epoch 39/100, Train Loss: -0.0269, Test Loss: -0.0446\n",
      "Epoch 40/100, Train Loss: -0.0255, Test Loss: -0.0496\n",
      "Epoch 41/100, Train Loss: -0.0278, Test Loss: -0.0487\n",
      "Epoch 42/100, Train Loss: -0.0288, Test Loss: -0.0515\n",
      "Epoch 43/100, Train Loss: -0.0295, Test Loss: -0.0529\n",
      "Epoch 44/100, Train Loss: -0.0294, Test Loss: -0.0539\n",
      "Epoch 45/100, Train Loss: -0.0302, Test Loss: -0.0532\n",
      "Epoch 46/100, Train Loss: -0.0308, Test Loss: -0.0553\n",
      "Epoch 47/100, Train Loss: -0.0311, Test Loss: -0.0558\n",
      "Epoch 48/100, Train Loss: -0.0313, Test Loss: -0.0560\n",
      "Epoch 49/100, Train Loss: -0.0318, Test Loss: -0.0571\n",
      "Epoch 50/100, Train Loss: -0.0323, Test Loss: -0.0579\n",
      "Epoch 51/100, Train Loss: -0.0317, Test Loss: -0.0577\n",
      "Epoch 52/100, Train Loss: -0.0284, Test Loss: -0.0545\n",
      "Epoch 53/100, Train Loss: -0.0244, Test Loss: -0.0298\n",
      "Epoch 54/100, Train Loss: -0.0178, Test Loss: -0.0509\n",
      "Epoch 55/100, Train Loss: -0.0267, Test Loss: -0.0470\n",
      "Epoch 56/100, Train Loss: -0.0295, Test Loss: -0.0551\n",
      "Epoch 57/100, Train Loss: -0.0302, Test Loss: -0.0570\n",
      "Epoch 58/100, Train Loss: -0.0314, Test Loss: -0.0484\n",
      "Epoch 59/100, Train Loss: -0.0298, Test Loss: -0.0533\n",
      "Epoch 60/100, Train Loss: -0.0307, Test Loss: -0.0545\n",
      "Early stopping due to overfitting.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.03e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 308       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 6         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.03e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 307       |\n",
      "|    iterations             | 2         |\n",
      "|    time_elapsed           | 13        |\n",
      "|    total_timesteps        | 4096      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | -0.00274  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00763   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 1         |\n",
      "|    policy_objective       | 0.00511   |\n",
      "|    std                    | 0.956     |\n",
      "|    value_loss             | 6.78e+03  |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.17e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 309       |\n",
      "|    iterations             | 3         |\n",
      "|    time_elapsed           | 19        |\n",
      "|    total_timesteps        | 6144      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | -0.0266   |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00814   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 2         |\n",
      "|    policy_objective       | 0.0103    |\n",
      "|    std                    | 1         |\n",
      "|    value_loss             | 6.42e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.18e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 303       |\n",
      "|    iterations             | 4         |\n",
      "|    time_elapsed           | 26        |\n",
      "|    total_timesteps        | 8192      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.000269  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00617   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 3         |\n",
      "|    policy_objective       | 0.00803   |\n",
      "|    std                    | 1.01      |\n",
      "|    value_loss             | 1.25e+04  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.18e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 298       |\n",
      "|    iterations             | 5         |\n",
      "|    time_elapsed           | 34        |\n",
      "|    total_timesteps        | 10240     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.000365  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.0069    |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 4         |\n",
      "|    policy_objective       | 0.0071    |\n",
      "|    std                    | 1.06      |\n",
      "|    value_loss             | 8.15e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.21e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 293       |\n",
      "|    iterations             | 6         |\n",
      "|    time_elapsed           | 41        |\n",
      "|    total_timesteps        | 12288     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.000421  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00715   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 5         |\n",
      "|    policy_objective       | 0.00945   |\n",
      "|    std                    | 1.1       |\n",
      "|    value_loss             | 8.98e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.17e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 289       |\n",
      "|    iterations             | 7         |\n",
      "|    time_elapsed           | 49        |\n",
      "|    total_timesteps        | 14336     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.000173  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00788   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 6         |\n",
      "|    policy_objective       | 0.00653   |\n",
      "|    std                    | 1.07      |\n",
      "|    value_loss             | 1.02e+04  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.16e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 282       |\n",
      "|    iterations             | 8         |\n",
      "|    time_elapsed           | 57        |\n",
      "|    total_timesteps        | 16384     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 3.66e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00783   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 7         |\n",
      "|    policy_objective       | 0.00859   |\n",
      "|    std                    | 1.1       |\n",
      "|    value_loss             | 4.02e+03  |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-1133.879126134189\n",
      "------------------------------\n",
      "round: 1\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.1636, Test Loss: 0.0462\n",
      "Epoch 2/100, Train Loss: 0.0823, Test Loss: -0.0016\n",
      "Epoch 3/100, Train Loss: 0.0222, Test Loss: -0.0174\n",
      "Epoch 4/100, Train Loss: -0.0054, Test Loss: -0.0101\n",
      "Epoch 5/100, Train Loss: -0.0044, Test Loss: -0.0113\n",
      "Epoch 6/100, Train Loss: -0.0117, Test Loss: -0.0253\n",
      "Epoch 7/100, Train Loss: -0.0186, Test Loss: -0.0309\n",
      "Epoch 8/100, Train Loss: -0.0217, Test Loss: -0.0337\n",
      "Epoch 9/100, Train Loss: -0.0235, Test Loss: -0.0348\n",
      "Epoch 10/100, Train Loss: -0.0234, Test Loss: -0.0355\n",
      "Epoch 11/100, Train Loss: -0.0245, Test Loss: -0.0356\n",
      "Epoch 12/100, Train Loss: -0.0256, Test Loss: -0.0366\n",
      "Epoch 13/100, Train Loss: -0.0264, Test Loss: -0.0369\n",
      "Epoch 14/100, Train Loss: -0.0268, Test Loss: -0.0367\n",
      "Epoch 15/100, Train Loss: -0.0271, Test Loss: -0.0372\n",
      "Epoch 16/100, Train Loss: -0.0275, Test Loss: -0.0373\n",
      "Epoch 17/100, Train Loss: -0.0278, Test Loss: -0.0382\n",
      "Epoch 18/100, Train Loss: -0.0282, Test Loss: -0.0385\n",
      "Epoch 19/100, Train Loss: -0.0287, Test Loss: -0.0392\n",
      "Epoch 20/100, Train Loss: -0.0291, Test Loss: -0.0390\n",
      "Epoch 21/100, Train Loss: -0.0294, Test Loss: -0.0397\n",
      "Epoch 22/100, Train Loss: -0.0295, Test Loss: -0.0347\n",
      "Epoch 23/100, Train Loss: -0.0292, Test Loss: -0.0283\n",
      "Epoch 24/100, Train Loss: -0.0282, Test Loss: -0.0122\n",
      "Epoch 25/100, Train Loss: -0.0268, Test Loss: -0.0284\n",
      "Epoch 26/100, Train Loss: -0.0284, Test Loss: -0.0161\n",
      "Epoch 27/100, Train Loss: -0.0278, Test Loss: -0.0380\n",
      "Epoch 28/100, Train Loss: -0.0302, Test Loss: -0.0330\n",
      "Epoch 29/100, Train Loss: -0.0303, Test Loss: -0.0388\n",
      "Epoch 30/100, Train Loss: -0.0312, Test Loss: -0.0385\n",
      "Epoch 31/100, Train Loss: -0.0314, Test Loss: -0.0414\n",
      "Epoch 32/100, Train Loss: -0.0321, Test Loss: -0.0414\n",
      "Epoch 33/100, Train Loss: -0.0324, Test Loss: -0.0404\n",
      "Epoch 34/100, Train Loss: -0.0327, Test Loss: -0.0431\n",
      "Epoch 35/100, Train Loss: -0.0332, Test Loss: -0.0427\n",
      "Epoch 36/100, Train Loss: -0.0335, Test Loss: -0.0424\n",
      "Epoch 37/100, Train Loss: -0.0339, Test Loss: -0.0430\n",
      "Epoch 38/100, Train Loss: -0.0341, Test Loss: -0.0428\n",
      "Epoch 39/100, Train Loss: -0.0344, Test Loss: -0.0412\n",
      "Epoch 40/100, Train Loss: -0.0334, Test Loss: -0.0374\n",
      "Epoch 41/100, Train Loss: -0.0322, Test Loss: -0.0188\n",
      "Epoch 42/100, Train Loss: -0.0278, Test Loss: 0.0623\n",
      "Epoch 43/100, Train Loss: -0.0207, Test Loss: 0.0627\n",
      "Epoch 44/100, Train Loss: -0.0170, Test Loss: -0.0396\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 0.0325, Test Loss: 11.0786\n",
      "Epoch 2/100, Train Loss: 0.0090, Test Loss: 9.7043\n",
      "Epoch 3/100, Train Loss: -0.0090, Test Loss: 7.3071\n",
      "Epoch 4/100, Train Loss: -0.0192, Test Loss: 5.9693\n",
      "Epoch 5/100, Train Loss: -0.0206, Test Loss: 5.2723\n",
      "Epoch 6/100, Train Loss: -0.0248, Test Loss: 4.7743\n",
      "Epoch 7/100, Train Loss: -0.0255, Test Loss: 4.2951\n",
      "Epoch 8/100, Train Loss: -0.0259, Test Loss: 3.9447\n",
      "Epoch 9/100, Train Loss: -0.0265, Test Loss: 3.7446\n",
      "Epoch 10/100, Train Loss: -0.0275, Test Loss: 3.6506\n",
      "Epoch 11/100, Train Loss: -0.0280, Test Loss: 3.6084\n",
      "Epoch 12/100, Train Loss: -0.0280, Test Loss: 3.5768\n",
      "Epoch 13/100, Train Loss: -0.0284, Test Loss: 3.5832\n",
      "Epoch 14/100, Train Loss: -0.0292, Test Loss: 3.6221\n",
      "Epoch 15/100, Train Loss: -0.0293, Test Loss: 3.6900\n",
      "Epoch 16/100, Train Loss: -0.0298, Test Loss: 3.7891\n",
      "Epoch 17/100, Train Loss: -0.0302, Test Loss: 3.9231\n",
      "Epoch 18/100, Train Loss: -0.0306, Test Loss: 4.0625\n",
      "Epoch 19/100, Train Loss: -0.0310, Test Loss: 4.1970\n",
      "Epoch 20/100, Train Loss: -0.0314, Test Loss: 4.3776\n",
      "Epoch 21/100, Train Loss: -0.0318, Test Loss: 4.5999\n",
      "Epoch 22/100, Train Loss: -0.0322, Test Loss: 4.8043\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 0.4883, Test Loss: 0.5189\n",
      "Epoch 2/100, Train Loss: 0.5385, Test Loss: 0.0280\n",
      "Epoch 3/100, Train Loss: 0.2294, Test Loss: 0.3133\n",
      "Epoch 4/100, Train Loss: 0.2033, Test Loss: 0.0222\n",
      "Epoch 5/100, Train Loss: 0.0899, Test Loss: 0.0069\n",
      "Epoch 6/100, Train Loss: 0.0781, Test Loss: 0.0019\n",
      "Epoch 7/100, Train Loss: 0.0468, Test Loss: -0.0344\n",
      "Epoch 8/100, Train Loss: 0.0187, Test Loss: -0.0376\n",
      "Epoch 9/100, Train Loss: 0.0103, Test Loss: -0.0288\n",
      "Epoch 10/100, Train Loss: 0.0074, Test Loss: -0.0278\n",
      "Epoch 11/100, Train Loss: 0.0028, Test Loss: -0.0329\n",
      "Epoch 12/100, Train Loss: -0.0021, Test Loss: -0.0374\n",
      "Epoch 13/100, Train Loss: -0.0053, Test Loss: -0.0392\n",
      "Epoch 14/100, Train Loss: -0.0070, Test Loss: -0.0396\n",
      "Epoch 15/100, Train Loss: -0.0081, Test Loss: -0.0400\n",
      "Epoch 16/100, Train Loss: -0.0091, Test Loss: -0.0406\n",
      "Epoch 17/100, Train Loss: -0.0100, Test Loss: -0.0410\n",
      "Epoch 18/100, Train Loss: -0.0108, Test Loss: -0.0410\n",
      "Epoch 19/100, Train Loss: -0.0114, Test Loss: -0.0410\n",
      "Epoch 20/100, Train Loss: -0.0118, Test Loss: -0.0410\n",
      "Epoch 21/100, Train Loss: -0.0123, Test Loss: -0.0409\n",
      "Epoch 22/100, Train Loss: -0.0127, Test Loss: -0.0411\n",
      "Epoch 23/100, Train Loss: -0.0131, Test Loss: -0.0412\n",
      "Epoch 24/100, Train Loss: -0.0134, Test Loss: -0.0413\n",
      "Epoch 25/100, Train Loss: -0.0137, Test Loss: -0.0413\n",
      "Epoch 26/100, Train Loss: -0.0139, Test Loss: -0.0412\n",
      "Epoch 27/100, Train Loss: -0.0143, Test Loss: -0.0411\n",
      "Epoch 28/100, Train Loss: -0.0145, Test Loss: -0.0411\n",
      "Epoch 29/100, Train Loss: -0.0147, Test Loss: -0.0412\n",
      "Epoch 30/100, Train Loss: -0.0150, Test Loss: -0.0413\n",
      "Epoch 31/100, Train Loss: -0.0152, Test Loss: -0.0414\n",
      "Epoch 32/100, Train Loss: -0.0155, Test Loss: -0.0416\n",
      "Epoch 33/100, Train Loss: -0.0157, Test Loss: -0.0417\n",
      "Epoch 34/100, Train Loss: -0.0159, Test Loss: -0.0418\n",
      "Epoch 35/100, Train Loss: -0.0161, Test Loss: -0.0420\n",
      "Epoch 36/100, Train Loss: -0.0164, Test Loss: -0.0421\n",
      "Epoch 37/100, Train Loss: -0.0165, Test Loss: -0.0422\n",
      "Epoch 38/100, Train Loss: -0.0168, Test Loss: -0.0423\n",
      "Epoch 39/100, Train Loss: -0.0170, Test Loss: -0.0424\n",
      "Epoch 40/100, Train Loss: -0.0172, Test Loss: -0.0426\n",
      "Epoch 41/100, Train Loss: -0.0174, Test Loss: -0.0427\n",
      "Epoch 42/100, Train Loss: -0.0176, Test Loss: -0.0428\n",
      "Epoch 43/100, Train Loss: -0.0178, Test Loss: -0.0430\n",
      "Epoch 44/100, Train Loss: -0.0180, Test Loss: -0.0431\n",
      "Epoch 45/100, Train Loss: -0.0182, Test Loss: -0.0432\n",
      "Epoch 46/100, Train Loss: -0.0184, Test Loss: -0.0434\n",
      "Epoch 47/100, Train Loss: -0.0186, Test Loss: -0.0437\n",
      "Epoch 48/100, Train Loss: -0.0189, Test Loss: -0.0440\n",
      "Epoch 49/100, Train Loss: -0.0190, Test Loss: -0.0442\n",
      "Epoch 50/100, Train Loss: -0.0193, Test Loss: -0.0444\n",
      "Epoch 51/100, Train Loss: -0.0195, Test Loss: -0.0445\n",
      "Epoch 52/100, Train Loss: -0.0197, Test Loss: -0.0447\n",
      "Epoch 53/100, Train Loss: -0.0200, Test Loss: -0.0449\n",
      "Epoch 54/100, Train Loss: -0.0202, Test Loss: -0.0453\n",
      "Epoch 55/100, Train Loss: -0.0204, Test Loss: -0.0455\n",
      "Epoch 56/100, Train Loss: -0.0206, Test Loss: -0.0456\n",
      "Epoch 57/100, Train Loss: -0.0209, Test Loss: -0.0458\n",
      "Epoch 58/100, Train Loss: -0.0211, Test Loss: -0.0462\n",
      "Epoch 59/100, Train Loss: -0.0214, Test Loss: -0.0465\n",
      "Epoch 60/100, Train Loss: -0.0216, Test Loss: -0.0467\n",
      "Epoch 61/100, Train Loss: -0.0218, Test Loss: -0.0470\n",
      "Epoch 62/100, Train Loss: -0.0220, Test Loss: -0.0472\n",
      "Epoch 63/100, Train Loss: -0.0223, Test Loss: -0.0475\n",
      "Epoch 64/100, Train Loss: -0.0226, Test Loss: -0.0478\n",
      "Epoch 65/100, Train Loss: -0.0228, Test Loss: -0.0481\n",
      "Epoch 66/100, Train Loss: -0.0231, Test Loss: -0.0484\n",
      "Epoch 67/100, Train Loss: -0.0233, Test Loss: -0.0487\n",
      "Epoch 68/100, Train Loss: -0.0236, Test Loss: -0.0490\n",
      "Epoch 69/100, Train Loss: -0.0238, Test Loss: -0.0494\n",
      "Epoch 70/100, Train Loss: -0.0242, Test Loss: -0.0497\n",
      "Epoch 71/100, Train Loss: -0.0245, Test Loss: -0.0501\n",
      "Epoch 72/100, Train Loss: -0.0247, Test Loss: -0.0504\n",
      "Epoch 73/100, Train Loss: -0.0250, Test Loss: -0.0508\n",
      "Epoch 74/100, Train Loss: -0.0254, Test Loss: -0.0513\n",
      "Epoch 75/100, Train Loss: -0.0257, Test Loss: -0.0517\n",
      "Epoch 76/100, Train Loss: -0.0260, Test Loss: -0.0522\n",
      "Epoch 77/100, Train Loss: -0.0263, Test Loss: -0.0525\n",
      "Epoch 78/100, Train Loss: -0.0267, Test Loss: -0.0529\n",
      "Epoch 79/100, Train Loss: -0.0270, Test Loss: -0.0533\n",
      "Epoch 80/100, Train Loss: -0.0274, Test Loss: -0.0537\n",
      "Epoch 81/100, Train Loss: -0.0277, Test Loss: -0.0542\n",
      "Epoch 82/100, Train Loss: -0.0281, Test Loss: -0.0548\n",
      "Epoch 83/100, Train Loss: -0.0284, Test Loss: -0.0553\n",
      "Epoch 84/100, Train Loss: -0.0288, Test Loss: -0.0558\n",
      "Epoch 85/100, Train Loss: -0.0292, Test Loss: -0.0562\n",
      "Epoch 86/100, Train Loss: -0.0296, Test Loss: -0.0566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100, Train Loss: -0.0300, Test Loss: -0.0571\n",
      "Epoch 88/100, Train Loss: -0.0304, Test Loss: -0.0577\n",
      "Epoch 89/100, Train Loss: -0.0308, Test Loss: -0.0583\n",
      "Epoch 90/100, Train Loss: -0.0312, Test Loss: -0.0588\n",
      "Epoch 91/100, Train Loss: -0.0316, Test Loss: -0.0592\n",
      "Epoch 92/100, Train Loss: -0.0320, Test Loss: -0.0598\n",
      "Epoch 93/100, Train Loss: -0.0325, Test Loss: -0.0602\n",
      "Epoch 94/100, Train Loss: -0.0328, Test Loss: -0.0604\n",
      "Epoch 95/100, Train Loss: -0.0331, Test Loss: -0.0610\n",
      "Epoch 96/100, Train Loss: -0.0337, Test Loss: -0.0616\n",
      "Epoch 97/100, Train Loss: -0.0341, Test Loss: -0.0621\n",
      "Epoch 98/100, Train Loss: -0.0345, Test Loss: -0.0623\n",
      "Epoch 99/100, Train Loss: -0.0345, Test Loss: -0.0625\n",
      "Epoch 100/100, Train Loss: -0.0352, Test Loss: -0.0635\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.12e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 261       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 7         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.04e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 257       |\n",
      "|    iterations             | 2         |\n",
      "|    time_elapsed           | 15        |\n",
      "|    total_timesteps        | 4096      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | -3.45e-05 |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00626   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 9         |\n",
      "|    policy_objective       | 0.0107    |\n",
      "|    std                    | 1.14      |\n",
      "|    value_loss             | 5.82e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.11e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 254       |\n",
      "|    iterations             | 3         |\n",
      "|    time_elapsed           | 24        |\n",
      "|    total_timesteps        | 6144      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.81e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00765   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 10        |\n",
      "|    policy_objective       | 0.00767   |\n",
      "|    std                    | 1.08      |\n",
      "|    value_loss             | 4.13e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.13e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 255       |\n",
      "|    iterations             | 4         |\n",
      "|    time_elapsed           | 32        |\n",
      "|    total_timesteps        | 8192      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | -1.39e-05 |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00533   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 11        |\n",
      "|    policy_objective       | 0.00679   |\n",
      "|    std                    | 1.05      |\n",
      "|    value_loss             | 7.42e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.12e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 254       |\n",
      "|    iterations             | 5         |\n",
      "|    time_elapsed           | 40        |\n",
      "|    total_timesteps        | 10240     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | -2.5e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00543   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 12        |\n",
      "|    policy_objective       | 0.00489   |\n",
      "|    std                    | 1.01      |\n",
      "|    value_loss             | 6.1e+03   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.16e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 255       |\n",
      "|    iterations             | 6         |\n",
      "|    time_elapsed           | 48        |\n",
      "|    total_timesteps        | 12288     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | -1.07e-06 |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00758   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 13        |\n",
      "|    policy_objective       | 0.00801   |\n",
      "|    std                    | 0.956     |\n",
      "|    value_loss             | 5.35e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.12e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 255       |\n",
      "|    iterations             | 7         |\n",
      "|    time_elapsed           | 56        |\n",
      "|    total_timesteps        | 14336     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | -3.15e-05 |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00632   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 14        |\n",
      "|    policy_objective       | 0.00971   |\n",
      "|    std                    | 0.96      |\n",
      "|    value_loss             | 7.92e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.14e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 256       |\n",
      "|    iterations             | 8         |\n",
      "|    time_elapsed           | 63        |\n",
      "|    total_timesteps        | 16384     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 8.46e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00682   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 15        |\n",
      "|    policy_objective       | 0.00722   |\n",
      "|    std                    | 0.997     |\n",
      "|    value_loss             | 2.61e+03  |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-1312.622391496226\n",
      "------------------------------\n",
      "round: 2\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: -0.0177, Test Loss: -0.0372\n",
      "Epoch 2/100, Train Loss: -0.0216, Test Loss: -0.0389\n",
      "Epoch 3/100, Train Loss: -0.0227, Test Loss: -0.0399\n",
      "Epoch 4/100, Train Loss: -0.0231, Test Loss: -0.0416\n",
      "Epoch 5/100, Train Loss: -0.0246, Test Loss: -0.0451\n",
      "Epoch 6/100, Train Loss: -0.0253, Test Loss: -0.0446\n",
      "Epoch 7/100, Train Loss: -0.0258, Test Loss: -0.0475\n",
      "Epoch 8/100, Train Loss: -0.0267, Test Loss: -0.0479\n",
      "Epoch 9/100, Train Loss: -0.0273, Test Loss: -0.0499\n",
      "Epoch 10/100, Train Loss: -0.0281, Test Loss: -0.0515\n",
      "Epoch 11/100, Train Loss: -0.0287, Test Loss: -0.0524\n",
      "Epoch 12/100, Train Loss: -0.0294, Test Loss: -0.0529\n",
      "Epoch 13/100, Train Loss: -0.0295, Test Loss: -0.0533\n",
      "Epoch 14/100, Train Loss: -0.0301, Test Loss: -0.0514\n",
      "Epoch 15/100, Train Loss: -0.0301, Test Loss: -0.0496\n",
      "Epoch 16/100, Train Loss: -0.0295, Test Loss: -0.0430\n",
      "Epoch 17/100, Train Loss: -0.0289, Test Loss: -0.0301\n",
      "Epoch 18/100, Train Loss: -0.0269, Test Loss: -0.0326\n",
      "Epoch 19/100, Train Loss: -0.0249, Test Loss: -0.0479\n",
      "Epoch 20/100, Train Loss: -0.0252, Test Loss: -0.0498\n",
      "Epoch 21/100, Train Loss: -0.0295, Test Loss: -0.0551\n",
      "Epoch 22/100, Train Loss: -0.0299, Test Loss: -0.0541\n",
      "Epoch 23/100, Train Loss: -0.0305, Test Loss: -0.0543\n",
      "Epoch 24/100, Train Loss: -0.0305, Test Loss: -0.0537\n",
      "Epoch 25/100, Train Loss: -0.0298, Test Loss: -0.0542\n",
      "Epoch 26/100, Train Loss: -0.0294, Test Loss: -0.0483\n",
      "Epoch 27/100, Train Loss: -0.0284, Test Loss: -0.0427\n",
      "Epoch 28/100, Train Loss: -0.0263, Test Loss: -0.0338\n",
      "Epoch 29/100, Train Loss: -0.0241, Test Loss: -0.0516\n",
      "Epoch 30/100, Train Loss: -0.0277, Test Loss: -0.0557\n",
      "Epoch 31/100, Train Loss: -0.0312, Test Loss: -0.0526\n",
      "Epoch 32/100, Train Loss: -0.0300, Test Loss: -0.0561\n",
      "Epoch 33/100, Train Loss: -0.0316, Test Loss: -0.0565\n",
      "Epoch 34/100, Train Loss: -0.0315, Test Loss: -0.0577\n",
      "Epoch 35/100, Train Loss: -0.0320, Test Loss: -0.0578\n",
      "Epoch 36/100, Train Loss: -0.0324, Test Loss: -0.0587\n",
      "Epoch 37/100, Train Loss: -0.0324, Test Loss: -0.0552\n",
      "Epoch 38/100, Train Loss: -0.0308, Test Loss: -0.0597\n",
      "Epoch 39/100, Train Loss: -0.0331, Test Loss: -0.0547\n",
      "Epoch 40/100, Train Loss: -0.0302, Test Loss: -0.0581\n",
      "Epoch 41/100, Train Loss: -0.0332, Test Loss: -0.0534\n",
      "Epoch 42/100, Train Loss: -0.0313, Test Loss: -0.0591\n",
      "Epoch 43/100, Train Loss: -0.0329, Test Loss: -0.0605\n",
      "Epoch 44/100, Train Loss: -0.0337, Test Loss: -0.0618\n",
      "Epoch 45/100, Train Loss: -0.0342, Test Loss: -0.0621\n",
      "Epoch 46/100, Train Loss: -0.0348, Test Loss: -0.0633\n",
      "Epoch 47/100, Train Loss: -0.0351, Test Loss: -0.0634\n",
      "Epoch 48/100, Train Loss: -0.0354, Test Loss: -0.0604\n",
      "Epoch 49/100, Train Loss: -0.0351, Test Loss: -0.0638\n",
      "Epoch 50/100, Train Loss: -0.0355, Test Loss: -0.0636\n",
      "Epoch 51/100, Train Loss: -0.0350, Test Loss: -0.0646\n",
      "Epoch 52/100, Train Loss: -0.0359, Test Loss: -0.0448\n",
      "Epoch 53/100, Train Loss: -0.0325, Test Loss: -0.0609\n",
      "Epoch 54/100, Train Loss: -0.0346, Test Loss: -0.0609\n",
      "Epoch 55/100, Train Loss: -0.0331, Test Loss: -0.0655\n",
      "Epoch 56/100, Train Loss: -0.0351, Test Loss: -0.0557\n",
      "Epoch 57/100, Train Loss: -0.0349, Test Loss: -0.0515\n",
      "Epoch 58/100, Train Loss: -0.0338, Test Loss: -0.0475\n",
      "Epoch 59/100, Train Loss: -0.0329, Test Loss: -0.0606\n",
      "Epoch 60/100, Train Loss: -0.0347, Test Loss: -0.0572\n",
      "Epoch 61/100, Train Loss: -0.0352, Test Loss: -0.0552\n",
      "Epoch 62/100, Train Loss: -0.0334, Test Loss: -0.0526\n",
      "Epoch 63/100, Train Loss: -0.0335, Test Loss: -0.0455\n",
      "Epoch 64/100, Train Loss: -0.0303, Test Loss: -0.0366\n",
      "Epoch 65/100, Train Loss: -0.0287, Test Loss: -0.0542\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 0.5080, Test Loss: 0.0569\n",
      "Epoch 2/100, Train Loss: 0.2323, Test Loss: -0.0059\n",
      "Epoch 3/100, Train Loss: 0.1423, Test Loss: 0.0104\n",
      "Epoch 4/100, Train Loss: 0.0848, Test Loss: -0.0307\n",
      "Epoch 5/100, Train Loss: 0.0382, Test Loss: -0.0334\n",
      "Epoch 6/100, Train Loss: 0.0192, Test Loss: -0.0375\n",
      "Epoch 7/100, Train Loss: 0.0074, Test Loss: -0.0385\n",
      "Epoch 8/100, Train Loss: 0.0016, Test Loss: -0.0374\n",
      "Epoch 9/100, Train Loss: -0.0021, Test Loss: -0.0379\n",
      "Epoch 10/100, Train Loss: -0.0059, Test Loss: -0.0384\n",
      "Epoch 11/100, Train Loss: -0.0085, Test Loss: -0.0387\n",
      "Epoch 12/100, Train Loss: -0.0104, Test Loss: -0.0388\n",
      "Epoch 13/100, Train Loss: -0.0121, Test Loss: -0.0386\n",
      "Epoch 14/100, Train Loss: -0.0135, Test Loss: -0.0386\n",
      "Epoch 15/100, Train Loss: -0.0147, Test Loss: -0.0385\n",
      "Epoch 16/100, Train Loss: -0.0156, Test Loss: -0.0381\n",
      "Epoch 17/100, Train Loss: -0.0164, Test Loss: -0.0382\n",
      "Epoch 18/100, Train Loss: -0.0170, Test Loss: -0.0375\n",
      "Epoch 19/100, Train Loss: -0.0176, Test Loss: -0.0371\n",
      "Epoch 20/100, Train Loss: -0.0182, Test Loss: -0.0360\n",
      "Epoch 21/100, Train Loss: -0.0187, Test Loss: -0.0354\n",
      "Epoch 22/100, Train Loss: -0.0192, Test Loss: -0.0349\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: -0.0172, Test Loss: 0.1588\n",
      "Epoch 2/100, Train Loss: -0.0172, Test Loss: 0.0979\n",
      "Epoch 3/100, Train Loss: -0.0273, Test Loss: 0.1064\n",
      "Epoch 4/100, Train Loss: -0.0273, Test Loss: 0.0895\n",
      "Epoch 5/100, Train Loss: -0.0295, Test Loss: 0.0886\n",
      "Epoch 6/100, Train Loss: -0.0302, Test Loss: 0.0826\n",
      "Epoch 7/100, Train Loss: -0.0316, Test Loss: 0.0896\n",
      "Epoch 8/100, Train Loss: -0.0313, Test Loss: 0.1018\n",
      "Epoch 9/100, Train Loss: -0.0333, Test Loss: 0.1205\n",
      "Epoch 10/100, Train Loss: -0.0337, Test Loss: 0.1419\n",
      "Epoch 11/100, Train Loss: -0.0349, Test Loss: 0.1655\n",
      "Epoch 12/100, Train Loss: -0.0359, Test Loss: 0.2024\n",
      "Epoch 13/100, Train Loss: -0.0372, Test Loss: 0.2467\n",
      "Epoch 14/100, Train Loss: -0.0385, Test Loss: 0.2984\n",
      "Epoch 15/100, Train Loss: -0.0396, Test Loss: 0.3697\n",
      "Epoch 16/100, Train Loss: -0.0409, Test Loss: 0.4438\n",
      "Early stopping due to overfitting.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.06e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 351       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 5         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.09e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 317       |\n",
      "|    iterations             | 2         |\n",
      "|    time_elapsed           | 12        |\n",
      "|    total_timesteps        | 4096      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.08e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00729   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 17        |\n",
      "|    policy_objective       | 0.00786   |\n",
      "|    std                    | 1.02      |\n",
      "|    value_loss             | 4.01e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.08e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 307       |\n",
      "|    iterations             | 3         |\n",
      "|    time_elapsed           | 19        |\n",
      "|    total_timesteps        | 6144      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.25e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.006     |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 18        |\n",
      "|    policy_objective       | 0.00744   |\n",
      "|    std                    | 1.03      |\n",
      "|    value_loss             | 3.82e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.09e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 299       |\n",
      "|    iterations             | 4         |\n",
      "|    time_elapsed           | 27        |\n",
      "|    total_timesteps        | 8192      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 5.84e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00569   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 19        |\n",
      "|    policy_objective       | 0.00838   |\n",
      "|    std                    | 1.03      |\n",
      "|    value_loss             | 3.93e+03  |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.08e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 309       |\n",
      "|    iterations             | 5         |\n",
      "|    time_elapsed           | 33        |\n",
      "|    total_timesteps        | 10240     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | -1.43e-06 |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00785   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 20        |\n",
      "|    policy_objective       | 0.00871   |\n",
      "|    std                    | 1.01      |\n",
      "|    value_loss             | 4.04e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.06e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 308       |\n",
      "|    iterations             | 6         |\n",
      "|    time_elapsed           | 39        |\n",
      "|    total_timesteps        | 12288     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | -9.54e-06 |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00801   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 21        |\n",
      "|    policy_objective       | 0.0054    |\n",
      "|    std                    | 0.977     |\n",
      "|    value_loss             | 3.29e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.08e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 308       |\n",
      "|    iterations             | 7         |\n",
      "|    time_elapsed           | 46        |\n",
      "|    total_timesteps        | 14336     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 5.07e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00734   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 22        |\n",
      "|    policy_objective       | 0.00971   |\n",
      "|    std                    | 0.981     |\n",
      "|    value_loss             | 2.55e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.09e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 309       |\n",
      "|    iterations             | 8         |\n",
      "|    time_elapsed           | 52        |\n",
      "|    total_timesteps        | 16384     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 8.94e-07  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.0072    |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 23        |\n",
      "|    policy_objective       | 0.00755   |\n",
      "|    std                    | 1.02      |\n",
      "|    value_loss             | 4.83e+03  |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-1150.7978745026048\n",
      "------------------------------\n",
      "round: 3\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 1.5710, Test Loss: 0.1952\n",
      "Epoch 2/100, Train Loss: 0.6813, Test Loss: 0.0323\n",
      "Epoch 3/100, Train Loss: 0.3468, Test Loss: 0.0076\n",
      "Epoch 4/100, Train Loss: 0.1873, Test Loss: -0.0131\n",
      "Epoch 5/100, Train Loss: 0.1357, Test Loss: -0.0199\n",
      "Epoch 6/100, Train Loss: 0.0774, Test Loss: -0.0395\n",
      "Epoch 7/100, Train Loss: 0.0547, Test Loss: -0.0361\n",
      "Epoch 8/100, Train Loss: 0.0443, Test Loss: -0.0419\n",
      "Epoch 9/100, Train Loss: 0.0243, Test Loss: -0.0426\n",
      "Epoch 10/100, Train Loss: 0.0195, Test Loss: -0.0420\n",
      "Epoch 11/100, Train Loss: 0.0118, Test Loss: -0.0438\n",
      "Epoch 12/100, Train Loss: 0.0052, Test Loss: -0.0436\n",
      "Epoch 13/100, Train Loss: 0.0030, Test Loss: -0.0440\n",
      "Epoch 14/100, Train Loss: 0.0005, Test Loss: -0.0450\n",
      "Epoch 15/100, Train Loss: -0.0027, Test Loss: -0.0456\n",
      "Epoch 16/100, Train Loss: -0.0045, Test Loss: -0.0456\n",
      "Epoch 17/100, Train Loss: -0.0059, Test Loss: -0.0455\n",
      "Epoch 18/100, Train Loss: -0.0073, Test Loss: -0.0457\n",
      "Epoch 19/100, Train Loss: -0.0085, Test Loss: -0.0458\n",
      "Epoch 20/100, Train Loss: -0.0094, Test Loss: -0.0462\n",
      "Epoch 21/100, Train Loss: -0.0101, Test Loss: -0.0461\n",
      "Epoch 22/100, Train Loss: -0.0111, Test Loss: -0.0465\n",
      "Epoch 23/100, Train Loss: -0.0115, Test Loss: -0.0461\n",
      "Epoch 24/100, Train Loss: -0.0124, Test Loss: -0.0464\n",
      "Epoch 25/100, Train Loss: -0.0127, Test Loss: -0.0457\n",
      "Epoch 26/100, Train Loss: -0.0135, Test Loss: -0.0463\n",
      "Epoch 27/100, Train Loss: -0.0136, Test Loss: -0.0459\n",
      "Epoch 28/100, Train Loss: -0.0146, Test Loss: -0.0466\n",
      "Epoch 29/100, Train Loss: -0.0145, Test Loss: -0.0459\n",
      "Epoch 30/100, Train Loss: -0.0154, Test Loss: -0.0464\n",
      "Epoch 31/100, Train Loss: -0.0153, Test Loss: -0.0458\n",
      "Epoch 32/100, Train Loss: -0.0162, Test Loss: -0.0464\n",
      "Epoch 33/100, Train Loss: -0.0159, Test Loss: -0.0458\n",
      "Epoch 34/100, Train Loss: -0.0169, Test Loss: -0.0465\n",
      "Epoch 35/100, Train Loss: -0.0166, Test Loss: -0.0460\n",
      "Epoch 36/100, Train Loss: -0.0177, Test Loss: -0.0467\n",
      "Epoch 37/100, Train Loss: -0.0175, Test Loss: -0.0468\n",
      "Epoch 38/100, Train Loss: -0.0183, Test Loss: -0.0471\n",
      "Epoch 39/100, Train Loss: -0.0184, Test Loss: -0.0471\n",
      "Epoch 40/100, Train Loss: -0.0189, Test Loss: -0.0466\n",
      "Epoch 41/100, Train Loss: -0.0194, Test Loss: -0.0470\n",
      "Epoch 42/100, Train Loss: -0.0193, Test Loss: -0.0467\n",
      "Epoch 43/100, Train Loss: -0.0201, Test Loss: -0.0473\n",
      "Epoch 44/100, Train Loss: -0.0199, Test Loss: -0.0472\n",
      "Epoch 45/100, Train Loss: -0.0207, Test Loss: -0.0473\n",
      "Epoch 46/100, Train Loss: -0.0209, Test Loss: -0.0474\n",
      "Epoch 47/100, Train Loss: -0.0210, Test Loss: -0.0472\n",
      "Epoch 48/100, Train Loss: -0.0214, Test Loss: -0.0476\n",
      "Epoch 49/100, Train Loss: -0.0213, Test Loss: -0.0475\n",
      "Epoch 50/100, Train Loss: -0.0218, Test Loss: -0.0478\n",
      "Epoch 51/100, Train Loss: -0.0220, Test Loss: -0.0478\n",
      "Epoch 52/100, Train Loss: -0.0221, Test Loss: -0.0478\n",
      "Epoch 53/100, Train Loss: -0.0223, Test Loss: -0.0478\n",
      "Epoch 54/100, Train Loss: -0.0223, Test Loss: -0.0478\n",
      "Epoch 55/100, Train Loss: -0.0223, Test Loss: -0.0476\n",
      "Epoch 56/100, Train Loss: -0.0222, Test Loss: -0.0475\n",
      "Epoch 57/100, Train Loss: -0.0220, Test Loss: -0.0470\n",
      "Epoch 58/100, Train Loss: -0.0213, Test Loss: -0.0468\n",
      "Epoch 59/100, Train Loss: -0.0207, Test Loss: -0.0462\n",
      "Epoch 60/100, Train Loss: -0.0189, Test Loss: -0.0459\n",
      "Epoch 61/100, Train Loss: -0.0171, Test Loss: -0.0459\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 0.0665, Test Loss: 0.0291\n",
      "Epoch 2/100, Train Loss: 0.0308, Test Loss: -0.0272\n",
      "Epoch 3/100, Train Loss: 0.0084, Test Loss: -0.0365\n",
      "Epoch 4/100, Train Loss: -0.0088, Test Loss: -0.0440\n",
      "Epoch 5/100, Train Loss: -0.0106, Test Loss: -0.0397\n",
      "Epoch 6/100, Train Loss: -0.0140, Test Loss: -0.0462\n",
      "Epoch 7/100, Train Loss: -0.0180, Test Loss: -0.0424\n",
      "Epoch 8/100, Train Loss: -0.0178, Test Loss: -0.0418\n",
      "Epoch 9/100, Train Loss: -0.0191, Test Loss: -0.0443\n",
      "Epoch 10/100, Train Loss: -0.0204, Test Loss: -0.0446\n",
      "Epoch 11/100, Train Loss: -0.0205, Test Loss: -0.0439\n",
      "Epoch 12/100, Train Loss: -0.0208, Test Loss: -0.0444\n",
      "Epoch 13/100, Train Loss: -0.0214, Test Loss: -0.0436\n",
      "Epoch 14/100, Train Loss: -0.0216, Test Loss: -0.0408\n",
      "Epoch 15/100, Train Loss: -0.0217, Test Loss: -0.0453\n",
      "Epoch 16/100, Train Loss: -0.0220, Test Loss: -0.0452\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 0.2867, Test Loss: -0.0121\n",
      "Epoch 2/100, Train Loss: 0.0967, Test Loss: -0.0209\n",
      "Epoch 3/100, Train Loss: 0.0392, Test Loss: -0.0500\n",
      "Epoch 4/100, Train Loss: 0.0073, Test Loss: -0.0392\n",
      "Epoch 5/100, Train Loss: -0.0065, Test Loss: -0.0543\n",
      "Epoch 6/100, Train Loss: -0.0198, Test Loss: -0.0576\n",
      "Epoch 7/100, Train Loss: -0.0233, Test Loss: -0.0564\n",
      "Epoch 8/100, Train Loss: -0.0239, Test Loss: -0.0549\n",
      "Epoch 9/100, Train Loss: -0.0248, Test Loss: -0.0558\n",
      "Epoch 10/100, Train Loss: -0.0269, Test Loss: -0.0570\n",
      "Epoch 11/100, Train Loss: -0.0283, Test Loss: -0.0567\n",
      "Epoch 12/100, Train Loss: -0.0285, Test Loss: -0.0563\n",
      "Epoch 13/100, Train Loss: -0.0288, Test Loss: -0.0567\n",
      "Epoch 14/100, Train Loss: -0.0294, Test Loss: -0.0573\n",
      "Epoch 15/100, Train Loss: -0.0300, Test Loss: -0.0578\n",
      "Epoch 16/100, Train Loss: -0.0304, Test Loss: -0.0583\n",
      "Epoch 17/100, Train Loss: -0.0307, Test Loss: -0.0586\n",
      "Epoch 18/100, Train Loss: -0.0310, Test Loss: -0.0589\n",
      "Epoch 19/100, Train Loss: -0.0314, Test Loss: -0.0594\n",
      "Epoch 20/100, Train Loss: -0.0318, Test Loss: -0.0597\n",
      "Epoch 21/100, Train Loss: -0.0322, Test Loss: -0.0601\n",
      "Epoch 22/100, Train Loss: -0.0325, Test Loss: -0.0605\n",
      "Epoch 23/100, Train Loss: -0.0329, Test Loss: -0.0610\n",
      "Epoch 24/100, Train Loss: -0.0332, Test Loss: -0.0614\n",
      "Epoch 25/100, Train Loss: -0.0336, Test Loss: -0.0619\n",
      "Epoch 26/100, Train Loss: -0.0339, Test Loss: -0.0624\n",
      "Epoch 27/100, Train Loss: -0.0342, Test Loss: -0.0628\n",
      "Epoch 28/100, Train Loss: -0.0346, Test Loss: -0.0633\n",
      "Epoch 29/100, Train Loss: -0.0349, Test Loss: -0.0638\n",
      "Epoch 30/100, Train Loss: -0.0353, Test Loss: -0.0642\n",
      "Epoch 31/100, Train Loss: -0.0356, Test Loss: -0.0647\n",
      "Epoch 32/100, Train Loss: -0.0359, Test Loss: -0.0652\n",
      "Epoch 33/100, Train Loss: -0.0363, Test Loss: -0.0657\n",
      "Epoch 34/100, Train Loss: -0.0366, Test Loss: -0.0662\n",
      "Epoch 35/100, Train Loss: -0.0370, Test Loss: -0.0667\n",
      "Epoch 36/100, Train Loss: -0.0373, Test Loss: -0.0671\n",
      "Epoch 37/100, Train Loss: -0.0376, Test Loss: -0.0676\n",
      "Epoch 38/100, Train Loss: -0.0380, Test Loss: -0.0681\n",
      "Epoch 39/100, Train Loss: -0.0383, Test Loss: -0.0685\n",
      "Epoch 40/100, Train Loss: -0.0386, Test Loss: -0.0690\n",
      "Epoch 41/100, Train Loss: -0.0389, Test Loss: -0.0695\n",
      "Epoch 42/100, Train Loss: -0.0392, Test Loss: -0.0700\n",
      "Epoch 43/100, Train Loss: -0.0395, Test Loss: -0.0704\n",
      "Epoch 44/100, Train Loss: -0.0398, Test Loss: -0.0708\n",
      "Epoch 45/100, Train Loss: -0.0400, Test Loss: -0.0712\n",
      "Epoch 46/100, Train Loss: -0.0402, Test Loss: -0.0715\n",
      "Epoch 47/100, Train Loss: -0.0404, Test Loss: -0.0719\n",
      "Epoch 48/100, Train Loss: -0.0405, Test Loss: -0.0722\n",
      "Epoch 49/100, Train Loss: -0.0407, Test Loss: -0.0723\n",
      "Epoch 50/100, Train Loss: -0.0408, Test Loss: -0.0721\n",
      "Epoch 51/100, Train Loss: -0.0410, Test Loss: -0.0718\n",
      "Epoch 52/100, Train Loss: -0.0412, Test Loss: -0.0718\n",
      "Epoch 53/100, Train Loss: -0.0416, Test Loss: -0.0730\n",
      "Epoch 54/100, Train Loss: -0.0419, Test Loss: -0.0734\n",
      "Epoch 55/100, Train Loss: -0.0403, Test Loss: -0.0673\n",
      "Epoch 56/100, Train Loss: -0.0362, Test Loss: -0.0556\n",
      "Epoch 57/100, Train Loss: -0.0331, Test Loss: -0.0708\n",
      "Epoch 58/100, Train Loss: -0.0391, Test Loss: -0.0677\n",
      "Epoch 59/100, Train Loss: -0.0383, Test Loss: -0.0701\n",
      "Epoch 60/100, Train Loss: -0.0384, Test Loss: -0.0708\n",
      "Epoch 61/100, Train Loss: -0.0397, Test Loss: -0.0702\n",
      "Epoch 62/100, Train Loss: -0.0402, Test Loss: -0.0706\n",
      "Epoch 63/100, Train Loss: -0.0409, Test Loss: -0.0718\n",
      "Epoch 64/100, Train Loss: -0.0408, Test Loss: -0.0711\n",
      "Early stopping due to overfitting.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.13e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 318       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 6         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.12e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 319       |\n",
      "|    iterations             | 2         |\n",
      "|    time_elapsed           | 12        |\n",
      "|    total_timesteps        | 4096      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 6.14e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00799   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 25        |\n",
      "|    policy_objective       | 0.00568   |\n",
      "|    std                    | 0.898     |\n",
      "|    value_loss             | 3.82e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.11e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 322       |\n",
      "|    iterations             | 3         |\n",
      "|    time_elapsed           | 19        |\n",
      "|    total_timesteps        | 6144      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 9.54e-07  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00722   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 26        |\n",
      "|    policy_objective       | 0.0102    |\n",
      "|    std                    | 0.938     |\n",
      "|    value_loss             | 2.89e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.1e+03  |\n",
      "| time/                     |           |\n",
      "|    fps                    | 325       |\n",
      "|    iterations             | 4         |\n",
      "|    time_elapsed           | 25        |\n",
      "|    total_timesteps        | 8192      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | -1.07e-06 |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00755   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 27        |\n",
      "|    policy_objective       | 0.00878   |\n",
      "|    std                    | 0.975     |\n",
      "|    value_loss             | 3.54e+03  |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -1.1e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 322      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 31       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 1.31e-06 |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00713  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 28       |\n",
      "|    policy_objective       | 0.0132   |\n",
      "|    std                    | 0.978    |\n",
      "|    value_loss             | 2.98e+03 |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.11e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 323       |\n",
      "|    iterations             | 6         |\n",
      "|    time_elapsed           | 38        |\n",
      "|    total_timesteps        | 12288     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 8.34e-07  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00786   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 29        |\n",
      "|    policy_objective       | 0.00835   |\n",
      "|    std                    | 1.03      |\n",
      "|    value_loss             | 3.04e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.11e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 326       |\n",
      "|    iterations             | 7         |\n",
      "|    time_elapsed           | 43        |\n",
      "|    total_timesteps        | 14336     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.85e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00549   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 30        |\n",
      "|    policy_objective       | 0.00717   |\n",
      "|    std                    | 1.07      |\n",
      "|    value_loss             | 3.24e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.13e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 326       |\n",
      "|    iterations             | 8         |\n",
      "|    time_elapsed           | 50        |\n",
      "|    total_timesteps        | 16384     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 2.15e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.0085    |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 31        |\n",
      "|    policy_objective       | 0.0119    |\n",
      "|    std                    | 0.996     |\n",
      "|    value_loss             | 2.97e+03  |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-1137.081097756268\n",
      "------------------------------\n",
      "round: 4\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.1100, Test Loss: 0.2299\n",
      "Epoch 2/100, Train Loss: 0.0953, Test Loss: 0.1086\n",
      "Epoch 3/100, Train Loss: 0.0937, Test Loss: -0.0117\n",
      "Epoch 4/100, Train Loss: 0.0071, Test Loss: 0.0036\n",
      "Epoch 5/100, Train Loss: 0.0175, Test Loss: -0.0040\n",
      "Epoch 6/100, Train Loss: 0.0034, Test Loss: -0.0337\n",
      "Epoch 7/100, Train Loss: -0.0038, Test Loss: -0.0281\n",
      "Epoch 8/100, Train Loss: -0.0041, Test Loss: -0.0415\n",
      "Epoch 9/100, Train Loss: -0.0135, Test Loss: -0.0439\n",
      "Epoch 10/100, Train Loss: -0.0153, Test Loss: -0.0370\n",
      "Epoch 11/100, Train Loss: -0.0139, Test Loss: -0.0386\n",
      "Epoch 12/100, Train Loss: -0.0157, Test Loss: -0.0440\n",
      "Epoch 13/100, Train Loss: -0.0181, Test Loss: -0.0459\n",
      "Epoch 14/100, Train Loss: -0.0187, Test Loss: -0.0456\n",
      "Epoch 15/100, Train Loss: -0.0191, Test Loss: -0.0466\n",
      "Epoch 16/100, Train Loss: -0.0199, Test Loss: -0.0469\n",
      "Epoch 17/100, Train Loss: -0.0206, Test Loss: -0.0464\n",
      "Epoch 18/100, Train Loss: -0.0209, Test Loss: -0.0469\n",
      "Epoch 19/100, Train Loss: -0.0214, Test Loss: -0.0478\n",
      "Epoch 20/100, Train Loss: -0.0220, Test Loss: -0.0483\n",
      "Epoch 21/100, Train Loss: -0.0223, Test Loss: -0.0489\n",
      "Epoch 22/100, Train Loss: -0.0227, Test Loss: -0.0490\n",
      "Epoch 23/100, Train Loss: -0.0230, Test Loss: -0.0496\n",
      "Epoch 24/100, Train Loss: -0.0235, Test Loss: -0.0497\n",
      "Epoch 25/100, Train Loss: -0.0239, Test Loss: -0.0501\n",
      "Epoch 26/100, Train Loss: -0.0243, Test Loss: -0.0505\n",
      "Epoch 27/100, Train Loss: -0.0246, Test Loss: -0.0509\n",
      "Epoch 28/100, Train Loss: -0.0250, Test Loss: -0.0515\n",
      "Epoch 29/100, Train Loss: -0.0255, Test Loss: -0.0517\n",
      "Epoch 30/100, Train Loss: -0.0257, Test Loss: -0.0520\n",
      "Epoch 31/100, Train Loss: -0.0260, Test Loss: -0.0520\n",
      "Epoch 32/100, Train Loss: -0.0262, Test Loss: -0.0522\n",
      "Epoch 33/100, Train Loss: -0.0266, Test Loss: -0.0530\n",
      "Epoch 34/100, Train Loss: -0.0271, Test Loss: -0.0518\n",
      "Epoch 35/100, Train Loss: -0.0270, Test Loss: -0.0513\n",
      "Epoch 36/100, Train Loss: -0.0274, Test Loss: -0.0538\n",
      "Epoch 37/100, Train Loss: -0.0274, Test Loss: -0.0507\n",
      "Epoch 38/100, Train Loss: -0.0271, Test Loss: -0.0536\n",
      "Epoch 39/100, Train Loss: -0.0274, Test Loss: -0.0526\n",
      "Epoch 40/100, Train Loss: -0.0283, Test Loss: -0.0534\n",
      "Epoch 41/100, Train Loss: -0.0288, Test Loss: -0.0539\n",
      "Epoch 42/100, Train Loss: -0.0293, Test Loss: -0.0548\n",
      "Epoch 43/100, Train Loss: -0.0294, Test Loss: -0.0551\n",
      "Epoch 44/100, Train Loss: -0.0298, Test Loss: -0.0552\n",
      "Epoch 45/100, Train Loss: -0.0302, Test Loss: -0.0559\n",
      "Epoch 46/100, Train Loss: -0.0306, Test Loss: -0.0562\n",
      "Epoch 47/100, Train Loss: -0.0309, Test Loss: -0.0565\n",
      "Epoch 48/100, Train Loss: -0.0313, Test Loss: -0.0563\n",
      "Epoch 49/100, Train Loss: -0.0313, Test Loss: -0.0558\n",
      "Epoch 50/100, Train Loss: -0.0311, Test Loss: -0.0508\n",
      "Epoch 51/100, Train Loss: -0.0292, Test Loss: -0.0532\n",
      "Epoch 52/100, Train Loss: -0.0303, Test Loss: -0.0530\n",
      "Epoch 53/100, Train Loss: -0.0297, Test Loss: -0.0559\n",
      "Epoch 54/100, Train Loss: -0.0309, Test Loss: -0.0569\n",
      "Epoch 55/100, Train Loss: -0.0315, Test Loss: -0.0560\n",
      "Epoch 56/100, Train Loss: -0.0323, Test Loss: -0.0570\n",
      "Epoch 57/100, Train Loss: -0.0326, Test Loss: -0.0576\n",
      "Epoch 58/100, Train Loss: -0.0333, Test Loss: -0.0581\n",
      "Epoch 59/100, Train Loss: -0.0337, Test Loss: -0.0590\n",
      "Epoch 60/100, Train Loss: -0.0341, Test Loss: -0.0595\n",
      "Epoch 61/100, Train Loss: -0.0344, Test Loss: -0.0594\n",
      "Epoch 62/100, Train Loss: -0.0348, Test Loss: -0.0598\n",
      "Epoch 63/100, Train Loss: -0.0351, Test Loss: -0.0601\n",
      "Epoch 64/100, Train Loss: -0.0353, Test Loss: -0.0608\n",
      "Epoch 65/100, Train Loss: -0.0358, Test Loss: -0.0596\n",
      "Epoch 66/100, Train Loss: -0.0359, Test Loss: -0.0546\n",
      "Epoch 67/100, Train Loss: -0.0352, Test Loss: -0.0536\n",
      "Epoch 68/100, Train Loss: -0.0357, Test Loss: -0.0597\n",
      "Epoch 69/100, Train Loss: -0.0360, Test Loss: -0.0495\n",
      "Epoch 70/100, Train Loss: -0.0309, Test Loss: -0.0601\n",
      "Epoch 71/100, Train Loss: -0.0335, Test Loss: -0.0515\n",
      "Epoch 72/100, Train Loss: -0.0343, Test Loss: -0.0580\n",
      "Epoch 73/100, Train Loss: -0.0353, Test Loss: -0.0581\n",
      "Epoch 74/100, Train Loss: -0.0363, Test Loss: -0.0600\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 2.3544, Test Loss: 0.2762\n",
      "Epoch 2/100, Train Loss: 1.6159, Test Loss: 0.3212\n",
      "Epoch 3/100, Train Loss: 0.9615, Test Loss: 0.0260\n",
      "Epoch 4/100, Train Loss: 0.6374, Test Loss: 0.0504\n",
      "Epoch 5/100, Train Loss: 0.4589, Test Loss: 0.0144\n",
      "Epoch 6/100, Train Loss: 0.2979, Test Loss: -0.0133\n",
      "Epoch 7/100, Train Loss: 0.2071, Test Loss: -0.0132\n",
      "Epoch 8/100, Train Loss: 0.1631, Test Loss: -0.0097\n",
      "Epoch 9/100, Train Loss: 0.1330, Test Loss: -0.0143\n",
      "Epoch 10/100, Train Loss: 0.1051, Test Loss: -0.0243\n",
      "Epoch 11/100, Train Loss: 0.0807, Test Loss: -0.0332\n",
      "Epoch 12/100, Train Loss: 0.0627, Test Loss: -0.0376\n",
      "Epoch 13/100, Train Loss: 0.0510, Test Loss: -0.0385\n",
      "Epoch 14/100, Train Loss: 0.0433, Test Loss: -0.0384\n",
      "Epoch 15/100, Train Loss: 0.0374, Test Loss: -0.0385\n",
      "Epoch 16/100, Train Loss: 0.0323, Test Loss: -0.0388\n",
      "Epoch 17/100, Train Loss: 0.0279, Test Loss: -0.0396\n",
      "Epoch 18/100, Train Loss: 0.0238, Test Loss: -0.0405\n",
      "Epoch 19/100, Train Loss: 0.0204, Test Loss: -0.0412\n",
      "Epoch 20/100, Train Loss: 0.0175, Test Loss: -0.0417\n",
      "Epoch 21/100, Train Loss: 0.0150, Test Loss: -0.0418\n",
      "Epoch 22/100, Train Loss: 0.0129, Test Loss: -0.0418\n",
      "Epoch 23/100, Train Loss: 0.0114, Test Loss: -0.0423\n",
      "Epoch 24/100, Train Loss: 0.0097, Test Loss: -0.0422\n",
      "Epoch 25/100, Train Loss: 0.0082, Test Loss: -0.0412\n",
      "Epoch 26/100, Train Loss: 0.0071, Test Loss: -0.0383\n",
      "Epoch 27/100, Train Loss: 0.0067, Test Loss: -0.0352\n",
      "Epoch 28/100, Train Loss: 0.0075, Test Loss: -0.0433\n",
      "Epoch 29/100, Train Loss: 0.0058, Test Loss: -0.0321\n",
      "Epoch 30/100, Train Loss: 0.0052, Test Loss: -0.0383\n",
      "Epoch 31/100, Train Loss: 0.0033, Test Loss: -0.0403\n",
      "Epoch 32/100, Train Loss: 0.0018, Test Loss: -0.0412\n",
      "Epoch 33/100, Train Loss: 0.0007, Test Loss: -0.0415\n",
      "Epoch 34/100, Train Loss: -0.0002, Test Loss: -0.0415\n",
      "Epoch 35/100, Train Loss: -0.0011, Test Loss: -0.0418\n",
      "Epoch 36/100, Train Loss: -0.0019, Test Loss: -0.0421\n",
      "Epoch 37/100, Train Loss: -0.0026, Test Loss: -0.0424\n",
      "Epoch 38/100, Train Loss: -0.0033, Test Loss: -0.0429\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 0.0376, Test Loss: -0.0226\n",
      "Epoch 2/100, Train Loss: 0.0166, Test Loss: -0.0218\n",
      "Epoch 3/100, Train Loss: -0.0119, Test Loss: -0.0548\n",
      "Epoch 4/100, Train Loss: -0.0273, Test Loss: -0.0380\n",
      "Epoch 5/100, Train Loss: -0.0191, Test Loss: -0.0430\n",
      "Epoch 6/100, Train Loss: -0.0261, Test Loss: -0.0500\n",
      "Epoch 7/100, Train Loss: -0.0305, Test Loss: -0.0509\n",
      "Epoch 8/100, Train Loss: -0.0304, Test Loss: -0.0496\n",
      "Epoch 9/100, Train Loss: -0.0298, Test Loss: -0.0496\n",
      "Epoch 10/100, Train Loss: -0.0297, Test Loss: -0.0497\n",
      "Epoch 11/100, Train Loss: -0.0303, Test Loss: -0.0513\n",
      "Epoch 12/100, Train Loss: -0.0316, Test Loss: -0.0517\n",
      "Epoch 13/100, Train Loss: -0.0319, Test Loss: -0.0518\n",
      "Early stopping due to overfitting.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.04e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 347       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 5         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.04e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 329       |\n",
      "|    iterations             | 2         |\n",
      "|    time_elapsed           | 12        |\n",
      "|    total_timesteps        | 4096      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.85e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00824   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 33        |\n",
      "|    policy_objective       | 0.0114    |\n",
      "|    std                    | 1.02      |\n",
      "|    value_loss             | 2.48e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.05e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 322       |\n",
      "|    iterations             | 3         |\n",
      "|    time_elapsed           | 19        |\n",
      "|    total_timesteps        | 6144      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 2.74e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00621   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 34        |\n",
      "|    policy_objective       | 0.00594   |\n",
      "|    std                    | 1.02      |\n",
      "|    value_loss             | 2.43e+03  |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.05e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 321       |\n",
      "|    iterations             | 4         |\n",
      "|    time_elapsed           | 25        |\n",
      "|    total_timesteps        | 8192      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.61e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00673   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 35        |\n",
      "|    policy_objective       | 0.0108    |\n",
      "|    std                    | 1.02      |\n",
      "|    value_loss             | 2.13e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.07e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 324       |\n",
      "|    iterations             | 5         |\n",
      "|    time_elapsed           | 31        |\n",
      "|    total_timesteps        | 10240     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.55e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.0074    |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 36        |\n",
      "|    policy_objective       | 0.00627   |\n",
      "|    std                    | 1.07      |\n",
      "|    value_loss             | 2.76e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.07e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 327       |\n",
      "|    iterations             | 6         |\n",
      "|    time_elapsed           | 37        |\n",
      "|    total_timesteps        | 12288     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 2.5e-06   |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.0059    |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 37        |\n",
      "|    policy_objective       | 0.00495   |\n",
      "|    std                    | 1.05      |\n",
      "|    value_loss             | 2.44e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.07e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 328       |\n",
      "|    iterations             | 7         |\n",
      "|    time_elapsed           | 43        |\n",
      "|    total_timesteps        | 14336     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0         |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00886   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 38        |\n",
      "|    policy_objective       | 0.0109    |\n",
      "|    std                    | 1.06      |\n",
      "|    value_loss             | 1.69e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.09e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 330       |\n",
      "|    iterations             | 8         |\n",
      "|    time_elapsed           | 49        |\n",
      "|    total_timesteps        | 16384     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 2.98e-07  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.0079    |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 39        |\n",
      "|    policy_objective       | 0.013     |\n",
      "|    std                    | 1.07      |\n",
      "|    value_loss             | 1.73e+03  |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-1161.9898551722522\n",
      "------------------------------\n",
      "round: 5\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.0727, Test Loss: 0.1199\n",
      "Epoch 2/100, Train Loss: 0.0706, Test Loss: -0.0446\n",
      "Epoch 3/100, Train Loss: -0.0112, Test Loss: 0.0044\n",
      "Epoch 4/100, Train Loss: 0.0063, Test Loss: -0.0257\n",
      "Epoch 5/100, Train Loss: -0.0151, Test Loss: -0.0495\n",
      "Epoch 6/100, Train Loss: -0.0224, Test Loss: -0.0408\n",
      "Epoch 7/100, Train Loss: -0.0188, Test Loss: -0.0401\n",
      "Epoch 8/100, Train Loss: -0.0208, Test Loss: -0.0466\n",
      "Epoch 9/100, Train Loss: -0.0247, Test Loss: -0.0478\n",
      "Epoch 10/100, Train Loss: -0.0252, Test Loss: -0.0451\n",
      "Epoch 11/100, Train Loss: -0.0243, Test Loss: -0.0443\n",
      "Epoch 12/100, Train Loss: -0.0245, Test Loss: -0.0458\n",
      "Epoch 13/100, Train Loss: -0.0256, Test Loss: -0.0473\n",
      "Epoch 14/100, Train Loss: -0.0261, Test Loss: -0.0474\n",
      "Epoch 15/100, Train Loss: -0.0262, Test Loss: -0.0472\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: -0.0144, Test Loss: -0.0245\n",
      "Epoch 2/100, Train Loss: -0.0169, Test Loss: -0.0280\n",
      "Epoch 3/100, Train Loss: -0.0226, Test Loss: -0.0274\n",
      "Epoch 4/100, Train Loss: -0.0209, Test Loss: -0.0283\n",
      "Epoch 5/100, Train Loss: -0.0235, Test Loss: -0.0301\n",
      "Epoch 6/100, Train Loss: -0.0236, Test Loss: -0.0291\n",
      "Epoch 7/100, Train Loss: -0.0240, Test Loss: -0.0300\n",
      "Epoch 8/100, Train Loss: -0.0252, Test Loss: -0.0304\n",
      "Epoch 9/100, Train Loss: -0.0256, Test Loss: -0.0310\n",
      "Epoch 10/100, Train Loss: -0.0264, Test Loss: -0.0314\n",
      "Epoch 11/100, Train Loss: -0.0273, Test Loss: -0.0314\n",
      "Epoch 12/100, Train Loss: -0.0280, Test Loss: -0.0325\n",
      "Epoch 13/100, Train Loss: -0.0289, Test Loss: -0.0326\n",
      "Epoch 14/100, Train Loss: -0.0297, Test Loss: -0.0330\n",
      "Epoch 15/100, Train Loss: -0.0303, Test Loss: -0.0333\n",
      "Epoch 16/100, Train Loss: -0.0305, Test Loss: -0.0325\n",
      "Epoch 17/100, Train Loss: -0.0297, Test Loss: -0.0328\n",
      "Epoch 18/100, Train Loss: -0.0311, Test Loss: -0.0326\n",
      "Epoch 19/100, Train Loss: -0.0311, Test Loss: -0.0312\n",
      "Epoch 20/100, Train Loss: -0.0284, Test Loss: -0.0278\n",
      "Epoch 21/100, Train Loss: -0.0255, Test Loss: -0.0213\n",
      "Epoch 22/100, Train Loss: -0.0274, Test Loss: -0.0301\n",
      "Epoch 23/100, Train Loss: -0.0270, Test Loss: -0.0313\n",
      "Epoch 24/100, Train Loss: -0.0300, Test Loss: -0.0321\n",
      "Epoch 25/100, Train Loss: -0.0300, Test Loss: -0.0315\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: -0.0251, Test Loss: -0.0466\n",
      "Epoch 2/100, Train Loss: -0.0221, Test Loss: -0.0503\n",
      "Epoch 3/100, Train Loss: -0.0276, Test Loss: -0.0473\n",
      "Epoch 4/100, Train Loss: -0.0283, Test Loss: -0.0514\n",
      "Epoch 5/100, Train Loss: -0.0300, Test Loss: -0.0515\n",
      "Epoch 6/100, Train Loss: -0.0312, Test Loss: -0.0533\n",
      "Epoch 7/100, Train Loss: -0.0327, Test Loss: -0.0551\n",
      "Epoch 8/100, Train Loss: -0.0332, Test Loss: -0.0577\n",
      "Epoch 9/100, Train Loss: -0.0347, Test Loss: -0.0586\n",
      "Epoch 10/100, Train Loss: -0.0348, Test Loss: -0.0595\n",
      "Epoch 11/100, Train Loss: -0.0355, Test Loss: -0.0605\n",
      "Epoch 12/100, Train Loss: -0.0364, Test Loss: -0.0621\n",
      "Epoch 13/100, Train Loss: -0.0372, Test Loss: -0.0601\n",
      "Epoch 14/100, Train Loss: -0.0368, Test Loss: -0.0631\n",
      "Epoch 15/100, Train Loss: -0.0383, Test Loss: -0.0620\n",
      "Epoch 16/100, Train Loss: -0.0382, Test Loss: -0.0643\n",
      "Epoch 17/100, Train Loss: -0.0393, Test Loss: -0.0640\n",
      "Epoch 18/100, Train Loss: -0.0387, Test Loss: -0.0655\n",
      "Epoch 19/100, Train Loss: -0.0392, Test Loss: -0.0644\n",
      "Epoch 20/100, Train Loss: -0.0370, Test Loss: -0.0659\n",
      "Epoch 21/100, Train Loss: -0.0363, Test Loss: -0.0653\n",
      "Epoch 22/100, Train Loss: -0.0351, Test Loss: -0.0657\n",
      "Epoch 23/100, Train Loss: -0.0381, Test Loss: -0.0584\n",
      "Epoch 24/100, Train Loss: -0.0394, Test Loss: -0.0625\n",
      "Epoch 25/100, Train Loss: -0.0398, Test Loss: -0.0663\n",
      "Epoch 26/100, Train Loss: -0.0412, Test Loss: -0.0635\n",
      "Epoch 27/100, Train Loss: -0.0417, Test Loss: -0.0655\n",
      "Epoch 28/100, Train Loss: -0.0418, Test Loss: -0.0675\n",
      "Epoch 29/100, Train Loss: -0.0430, Test Loss: -0.0676\n",
      "Epoch 30/100, Train Loss: -0.0435, Test Loss: -0.0690\n",
      "Epoch 31/100, Train Loss: -0.0432, Test Loss: -0.0654\n",
      "Epoch 32/100, Train Loss: -0.0409, Test Loss: -0.0528\n",
      "Epoch 33/100, Train Loss: -0.0332, Test Loss: -0.0567\n",
      "Epoch 34/100, Train Loss: -0.0362, Test Loss: -0.0585\n",
      "Epoch 35/100, Train Loss: -0.0348, Test Loss: -0.0668\n",
      "Epoch 36/100, Train Loss: -0.0376, Test Loss: -0.0681\n",
      "Epoch 37/100, Train Loss: -0.0365, Test Loss: -0.0641\n",
      "Epoch 38/100, Train Loss: -0.0370, Test Loss: -0.0621\n",
      "Epoch 39/100, Train Loss: -0.0373, Test Loss: -0.0657\n",
      "Epoch 40/100, Train Loss: -0.0361, Test Loss: -0.0673\n",
      "Early stopping due to overfitting.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.25e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 301       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 6         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.21e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 306       |\n",
      "|    iterations             | 2         |\n",
      "|    time_elapsed           | 13        |\n",
      "|    total_timesteps        | 4096      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 2.68e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00642   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 41        |\n",
      "|    policy_objective       | 0.00595   |\n",
      "|    std                    | 1.02      |\n",
      "|    value_loss             | 3.13e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.21e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 309       |\n",
      "|    iterations             | 3         |\n",
      "|    time_elapsed           | 19        |\n",
      "|    total_timesteps        | 6144      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | -9.54e-07 |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00769   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 42        |\n",
      "|    policy_objective       | 0.0111    |\n",
      "|    std                    | 1.04      |\n",
      "|    value_loss             | 1.92e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.19e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 310       |\n",
      "|    iterations             | 4         |\n",
      "|    time_elapsed           | 26        |\n",
      "|    total_timesteps        | 8192      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 2.98e-07  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00667   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 43        |\n",
      "|    policy_objective       | 0.00993   |\n",
      "|    std                    | 1.1       |\n",
      "|    value_loss             | 2.08e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.17e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 312       |\n",
      "|    iterations             | 5         |\n",
      "|    time_elapsed           | 32        |\n",
      "|    total_timesteps        | 10240     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 8.94e-07  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00746   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 44        |\n",
      "|    policy_objective       | 0.00637   |\n",
      "|    std                    | 1.14      |\n",
      "|    value_loss             | 1.46e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.18e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 311       |\n",
      "|    iterations             | 6         |\n",
      "|    time_elapsed           | 39        |\n",
      "|    total_timesteps        | 12288     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.43e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00835   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 45        |\n",
      "|    policy_objective       | 0.00856   |\n",
      "|    std                    | 1.13      |\n",
      "|    value_loss             | 1.84e+03  |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.19e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 308       |\n",
      "|    iterations             | 7         |\n",
      "|    time_elapsed           | 46        |\n",
      "|    total_timesteps        | 14336     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.61e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00571   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 46        |\n",
      "|    policy_objective       | 0.00686   |\n",
      "|    std                    | 1.16      |\n",
      "|    value_loss             | 2.64e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.21e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 302       |\n",
      "|    iterations             | 8         |\n",
      "|    time_elapsed           | 54        |\n",
      "|    total_timesteps        | 16384     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 2.32e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00584   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 47        |\n",
      "|    policy_objective       | 0.00911   |\n",
      "|    std                    | 1.19      |\n",
      "|    value_loss             | 1.62e+03  |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-1292.6096162810923\n",
      "------------------------------\n",
      "round: 6\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: -0.0250, Test Loss: 0.0113\n",
      "Epoch 2/100, Train Loss: -0.0238, Test Loss: 0.0075\n",
      "Epoch 3/100, Train Loss: -0.0316, Test Loss: 0.0371\n",
      "Epoch 4/100, Train Loss: -0.0298, Test Loss: 0.0447\n",
      "Epoch 5/100, Train Loss: -0.0332, Test Loss: 0.0659\n",
      "Epoch 6/100, Train Loss: -0.0344, Test Loss: 0.1248\n",
      "Epoch 7/100, Train Loss: -0.0349, Test Loss: 0.1670\n",
      "Epoch 8/100, Train Loss: -0.0363, Test Loss: 0.1877\n",
      "Epoch 9/100, Train Loss: -0.0372, Test Loss: 0.1949\n",
      "Epoch 10/100, Train Loss: -0.0380, Test Loss: 0.1876\n",
      "Epoch 11/100, Train Loss: -0.0395, Test Loss: 0.1881\n",
      "Epoch 12/100, Train Loss: -0.0407, Test Loss: 0.1987\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 0.1037, Test Loss: 0.0583\n",
      "Epoch 2/100, Train Loss: 0.0531, Test Loss: 0.0480\n",
      "Epoch 3/100, Train Loss: 0.0108, Test Loss: -0.0024\n",
      "Epoch 4/100, Train Loss: -0.0028, Test Loss: -0.0008\n",
      "Epoch 5/100, Train Loss: -0.0069, Test Loss: -0.0175\n",
      "Epoch 6/100, Train Loss: -0.0139, Test Loss: -0.0263\n",
      "Epoch 7/100, Train Loss: -0.0164, Test Loss: -0.0246\n",
      "Epoch 8/100, Train Loss: -0.0174, Test Loss: -0.0287\n",
      "Epoch 9/100, Train Loss: -0.0197, Test Loss: -0.0333\n",
      "Epoch 10/100, Train Loss: -0.0210, Test Loss: -0.0337\n",
      "Epoch 11/100, Train Loss: -0.0211, Test Loss: -0.0333\n",
      "Epoch 12/100, Train Loss: -0.0212, Test Loss: -0.0339\n",
      "Epoch 13/100, Train Loss: -0.0219, Test Loss: -0.0356\n",
      "Epoch 14/100, Train Loss: -0.0226, Test Loss: -0.0365\n",
      "Epoch 15/100, Train Loss: -0.0229, Test Loss: -0.0366\n",
      "Epoch 16/100, Train Loss: -0.0231, Test Loss: -0.0371\n",
      "Epoch 17/100, Train Loss: -0.0234, Test Loss: -0.0376\n",
      "Epoch 18/100, Train Loss: -0.0237, Test Loss: -0.0380\n",
      "Epoch 19/100, Train Loss: -0.0240, Test Loss: -0.0383\n",
      "Epoch 20/100, Train Loss: -0.0243, Test Loss: -0.0388\n",
      "Epoch 21/100, Train Loss: -0.0246, Test Loss: -0.0393\n",
      "Epoch 22/100, Train Loss: -0.0249, Test Loss: -0.0397\n",
      "Epoch 23/100, Train Loss: -0.0252, Test Loss: -0.0402\n",
      "Epoch 24/100, Train Loss: -0.0255, Test Loss: -0.0407\n",
      "Epoch 25/100, Train Loss: -0.0258, Test Loss: -0.0410\n",
      "Epoch 26/100, Train Loss: -0.0261, Test Loss: -0.0416\n",
      "Epoch 27/100, Train Loss: -0.0265, Test Loss: -0.0420\n",
      "Epoch 28/100, Train Loss: -0.0268, Test Loss: -0.0425\n",
      "Epoch 29/100, Train Loss: -0.0271, Test Loss: -0.0428\n",
      "Epoch 30/100, Train Loss: -0.0274, Test Loss: -0.0433\n",
      "Epoch 31/100, Train Loss: -0.0278, Test Loss: -0.0438\n",
      "Epoch 32/100, Train Loss: -0.0281, Test Loss: -0.0443\n",
      "Epoch 33/100, Train Loss: -0.0284, Test Loss: -0.0447\n",
      "Epoch 34/100, Train Loss: -0.0288, Test Loss: -0.0452\n",
      "Epoch 35/100, Train Loss: -0.0291, Test Loss: -0.0457\n",
      "Epoch 36/100, Train Loss: -0.0294, Test Loss: -0.0462\n",
      "Epoch 37/100, Train Loss: -0.0297, Test Loss: -0.0467\n",
      "Epoch 38/100, Train Loss: -0.0299, Test Loss: -0.0472\n",
      "Epoch 39/100, Train Loss: -0.0299, Test Loss: -0.0476\n",
      "Epoch 40/100, Train Loss: -0.0297, Test Loss: -0.0470\n",
      "Epoch 41/100, Train Loss: -0.0297, Test Loss: -0.0450\n",
      "Epoch 42/100, Train Loss: -0.0294, Test Loss: -0.0460\n",
      "Epoch 43/100, Train Loss: -0.0284, Test Loss: -0.0454\n",
      "Epoch 44/100, Train Loss: -0.0287, Test Loss: -0.0327\n",
      "Epoch 45/100, Train Loss: -0.0250, Test Loss: -0.0481\n",
      "Epoch 46/100, Train Loss: -0.0285, Test Loss: -0.0391\n",
      "Epoch 47/100, Train Loss: -0.0269, Test Loss: -0.0429\n",
      "Epoch 48/100, Train Loss: -0.0293, Test Loss: -0.0367\n",
      "Epoch 49/100, Train Loss: -0.0280, Test Loss: -0.0430\n",
      "Epoch 50/100, Train Loss: -0.0296, Test Loss: -0.0437\n",
      "Epoch 51/100, Train Loss: -0.0301, Test Loss: -0.0440\n",
      "Epoch 52/100, Train Loss: -0.0307, Test Loss: -0.0431\n",
      "Epoch 53/100, Train Loss: -0.0301, Test Loss: -0.0458\n",
      "Epoch 54/100, Train Loss: -0.0318, Test Loss: -0.0473\n",
      "Epoch 55/100, Train Loss: -0.0319, Test Loss: -0.0478\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 0.3455, Test Loss: 1.7262\n",
      "Epoch 2/100, Train Loss: 0.2174, Test Loss: 0.9499\n",
      "Epoch 3/100, Train Loss: 0.0212, Test Loss: 0.5182\n",
      "Epoch 4/100, Train Loss: 0.0440, Test Loss: 0.3158\n",
      "Epoch 5/100, Train Loss: -0.0046, Test Loss: 0.1775\n",
      "Epoch 6/100, Train Loss: -0.0243, Test Loss: 0.1481\n",
      "Epoch 7/100, Train Loss: -0.0185, Test Loss: 0.1116\n",
      "Epoch 8/100, Train Loss: -0.0253, Test Loss: 0.0685\n",
      "Epoch 9/100, Train Loss: -0.0337, Test Loss: 0.0432\n",
      "Epoch 10/100, Train Loss: -0.0356, Test Loss: 0.0320\n",
      "Epoch 11/100, Train Loss: -0.0351, Test Loss: 0.0243\n",
      "Epoch 12/100, Train Loss: -0.0356, Test Loss: 0.0174\n",
      "Epoch 13/100, Train Loss: -0.0370, Test Loss: 0.0119\n",
      "Epoch 14/100, Train Loss: -0.0382, Test Loss: 0.0096\n",
      "Epoch 15/100, Train Loss: -0.0388, Test Loss: 0.0091\n",
      "Epoch 16/100, Train Loss: -0.0393, Test Loss: 0.0089\n",
      "Epoch 17/100, Train Loss: -0.0398, Test Loss: 0.0083\n",
      "Epoch 18/100, Train Loss: -0.0406, Test Loss: 0.0075\n",
      "Epoch 19/100, Train Loss: -0.0412, Test Loss: 0.0068\n",
      "Epoch 20/100, Train Loss: -0.0417, Test Loss: 0.0061\n",
      "Epoch 21/100, Train Loss: -0.0423, Test Loss: 0.0062\n",
      "Epoch 22/100, Train Loss: -0.0428, Test Loss: 0.0072\n",
      "Epoch 23/100, Train Loss: -0.0434, Test Loss: 0.0090\n",
      "Epoch 24/100, Train Loss: -0.0439, Test Loss: 0.0119\n",
      "Epoch 25/100, Train Loss: -0.0444, Test Loss: 0.0151\n",
      "Epoch 26/100, Train Loss: -0.0449, Test Loss: 0.0181\n",
      "Epoch 27/100, Train Loss: -0.0454, Test Loss: 0.0214\n",
      "Epoch 28/100, Train Loss: -0.0458, Test Loss: 0.0247\n",
      "Epoch 29/100, Train Loss: -0.0462, Test Loss: 0.0277\n",
      "Epoch 30/100, Train Loss: -0.0466, Test Loss: 0.0304\n",
      "Early stopping due to overfitting.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.07e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 313       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 6         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.14e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 316       |\n",
      "|    iterations             | 2         |\n",
      "|    time_elapsed           | 12        |\n",
      "|    total_timesteps        | 4096      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 7.15e-07  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.0091    |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 49        |\n",
      "|    policy_objective       | 0.0096    |\n",
      "|    std                    | 1.1       |\n",
      "|    value_loss             | 1.25e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.13e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 321       |\n",
      "|    iterations             | 3         |\n",
      "|    time_elapsed           | 19        |\n",
      "|    total_timesteps        | 6144      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.49e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00577   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 50        |\n",
      "|    policy_objective       | 0.00868   |\n",
      "|    std                    | 1.13      |\n",
      "|    value_loss             | 1.37e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.14e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 319       |\n",
      "|    iterations             | 4         |\n",
      "|    time_elapsed           | 25        |\n",
      "|    total_timesteps        | 8192      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.19e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00612   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 51        |\n",
      "|    policy_objective       | 0.00842   |\n",
      "|    std                    | 1.13      |\n",
      "|    value_loss             | 854       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.14e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 319       |\n",
      "|    iterations             | 5         |\n",
      "|    time_elapsed           | 32        |\n",
      "|    total_timesteps        | 10240     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 8.94e-07  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00794   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 52        |\n",
      "|    policy_objective       | 0.0121    |\n",
      "|    std                    | 1.2       |\n",
      "|    value_loss             | 936       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.13e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 321       |\n",
      "|    iterations             | 6         |\n",
      "|    time_elapsed           | 38        |\n",
      "|    total_timesteps        | 12288     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.19e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00863   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 53        |\n",
      "|    policy_objective       | 0.00856   |\n",
      "|    std                    | 1.24      |\n",
      "|    value_loss             | 1.09e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.14e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 312       |\n",
      "|    iterations             | 7         |\n",
      "|    time_elapsed           | 45        |\n",
      "|    total_timesteps        | 14336     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.43e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00691   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 54        |\n",
      "|    policy_objective       | 0.00777   |\n",
      "|    std                    | 1.28      |\n",
      "|    value_loss             | 764       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.15e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 303       |\n",
      "|    iterations             | 8         |\n",
      "|    time_elapsed           | 54        |\n",
      "|    total_timesteps        | 16384     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.07e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00664   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 55        |\n",
      "|    policy_objective       | 0.0073    |\n",
      "|    std                    | 1.27      |\n",
      "|    value_loss             | 1.52e+03  |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-1253.219683345058\n",
      "------------------------------\n",
      "round: 7\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.0342, Test Loss: 0.0036\n",
      "Epoch 2/100, Train Loss: 0.0068, Test Loss: -0.0186\n",
      "Epoch 3/100, Train Loss: -0.0185, Test Loss: -0.0330\n",
      "Epoch 4/100, Train Loss: -0.0220, Test Loss: -0.0290\n",
      "Epoch 5/100, Train Loss: -0.0204, Test Loss: -0.0333\n",
      "Epoch 6/100, Train Loss: -0.0227, Test Loss: -0.0366\n",
      "Epoch 7/100, Train Loss: -0.0243, Test Loss: -0.0378\n",
      "Epoch 8/100, Train Loss: -0.0250, Test Loss: -0.0393\n",
      "Epoch 9/100, Train Loss: -0.0252, Test Loss: -0.0391\n",
      "Epoch 10/100, Train Loss: -0.0252, Test Loss: -0.0392\n",
      "Epoch 11/100, Train Loss: -0.0256, Test Loss: -0.0408\n",
      "Epoch 12/100, Train Loss: -0.0264, Test Loss: -0.0419\n",
      "Epoch 13/100, Train Loss: -0.0270, Test Loss: -0.0421\n",
      "Epoch 14/100, Train Loss: -0.0274, Test Loss: -0.0434\n",
      "Epoch 15/100, Train Loss: -0.0280, Test Loss: -0.0445\n",
      "Epoch 16/100, Train Loss: -0.0286, Test Loss: -0.0459\n",
      "Epoch 17/100, Train Loss: -0.0293, Test Loss: -0.0472\n",
      "Epoch 18/100, Train Loss: -0.0300, Test Loss: -0.0473\n",
      "Epoch 19/100, Train Loss: -0.0306, Test Loss: -0.0486\n",
      "Epoch 20/100, Train Loss: -0.0314, Test Loss: -0.0504\n",
      "Epoch 21/100, Train Loss: -0.0321, Test Loss: -0.0499\n",
      "Epoch 22/100, Train Loss: -0.0325, Test Loss: -0.0512\n",
      "Epoch 23/100, Train Loss: -0.0330, Test Loss: -0.0526\n",
      "Epoch 24/100, Train Loss: -0.0332, Test Loss: -0.0508\n",
      "Epoch 25/100, Train Loss: -0.0337, Test Loss: -0.0541\n",
      "Epoch 26/100, Train Loss: -0.0338, Test Loss: -0.0530\n",
      "Epoch 27/100, Train Loss: -0.0346, Test Loss: -0.0558\n",
      "Epoch 28/100, Train Loss: -0.0353, Test Loss: -0.0546\n",
      "Epoch 29/100, Train Loss: -0.0356, Test Loss: -0.0575\n",
      "Epoch 30/100, Train Loss: -0.0363, Test Loss: -0.0577\n",
      "Epoch 31/100, Train Loss: -0.0369, Test Loss: -0.0587\n",
      "Epoch 32/100, Train Loss: -0.0374, Test Loss: -0.0604\n",
      "Epoch 33/100, Train Loss: -0.0379, Test Loss: -0.0619\n",
      "Epoch 34/100, Train Loss: -0.0384, Test Loss: -0.0620\n",
      "Epoch 35/100, Train Loss: -0.0387, Test Loss: -0.0618\n",
      "Epoch 36/100, Train Loss: -0.0388, Test Loss: -0.0618\n",
      "Epoch 37/100, Train Loss: -0.0388, Test Loss: -0.0563\n",
      "Epoch 38/100, Train Loss: -0.0384, Test Loss: -0.0537\n",
      "Epoch 39/100, Train Loss: -0.0378, Test Loss: -0.0606\n",
      "Epoch 40/100, Train Loss: -0.0367, Test Loss: -0.0462\n",
      "Epoch 41/100, Train Loss: -0.0331, Test Loss: -0.0581\n",
      "Epoch 42/100, Train Loss: -0.0351, Test Loss: -0.0574\n",
      "Epoch 43/100, Train Loss: -0.0382, Test Loss: -0.0566\n",
      "Epoch 44/100, Train Loss: -0.0389, Test Loss: -0.0584\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 2.1766, Test Loss: 1.4151\n",
      "Epoch 2/100, Train Loss: 0.9113, Test Loss: 0.2385\n",
      "Epoch 3/100, Train Loss: 0.4790, Test Loss: 0.0396\n",
      "Epoch 4/100, Train Loss: 0.1646, Test Loss: 0.0431\n",
      "Epoch 5/100, Train Loss: 0.0831, Test Loss: 0.0539\n",
      "Epoch 6/100, Train Loss: 0.0546, Test Loss: 0.0264\n",
      "Epoch 7/100, Train Loss: 0.0331, Test Loss: -0.0016\n",
      "Epoch 8/100, Train Loss: 0.0142, Test Loss: -0.0202\n",
      "Epoch 9/100, Train Loss: 0.0030, Test Loss: -0.0349\n",
      "Epoch 10/100, Train Loss: -0.0052, Test Loss: -0.0444\n",
      "Epoch 11/100, Train Loss: -0.0100, Test Loss: -0.0487\n",
      "Epoch 12/100, Train Loss: -0.0132, Test Loss: -0.0510\n",
      "Epoch 13/100, Train Loss: -0.0151, Test Loss: -0.0518\n",
      "Epoch 14/100, Train Loss: -0.0164, Test Loss: -0.0529\n",
      "Epoch 15/100, Train Loss: -0.0176, Test Loss: -0.0534\n",
      "Epoch 16/100, Train Loss: -0.0184, Test Loss: -0.0540\n",
      "Epoch 17/100, Train Loss: -0.0193, Test Loss: -0.0543\n",
      "Epoch 18/100, Train Loss: -0.0201, Test Loss: -0.0547\n",
      "Epoch 19/100, Train Loss: -0.0208, Test Loss: -0.0552\n",
      "Epoch 20/100, Train Loss: -0.0215, Test Loss: -0.0556\n",
      "Epoch 21/100, Train Loss: -0.0221, Test Loss: -0.0559\n",
      "Epoch 22/100, Train Loss: -0.0227, Test Loss: -0.0561\n",
      "Epoch 23/100, Train Loss: -0.0232, Test Loss: -0.0564\n",
      "Epoch 24/100, Train Loss: -0.0237, Test Loss: -0.0567\n",
      "Epoch 25/100, Train Loss: -0.0243, Test Loss: -0.0564\n",
      "Epoch 26/100, Train Loss: -0.0244, Test Loss: -0.0565\n",
      "Epoch 27/100, Train Loss: -0.0249, Test Loss: -0.0567\n",
      "Epoch 28/100, Train Loss: -0.0243, Test Loss: -0.0560\n",
      "Epoch 29/100, Train Loss: -0.0243, Test Loss: -0.0551\n",
      "Epoch 30/100, Train Loss: -0.0245, Test Loss: -0.0567\n",
      "Epoch 31/100, Train Loss: -0.0256, Test Loss: -0.0566\n",
      "Epoch 32/100, Train Loss: -0.0259, Test Loss: -0.0572\n",
      "Epoch 33/100, Train Loss: -0.0262, Test Loss: -0.0574\n",
      "Epoch 34/100, Train Loss: -0.0263, Test Loss: -0.0577\n",
      "Epoch 35/100, Train Loss: -0.0262, Test Loss: -0.0578\n",
      "Epoch 36/100, Train Loss: -0.0264, Test Loss: -0.0577\n",
      "Epoch 37/100, Train Loss: -0.0265, Test Loss: -0.0579\n",
      "Epoch 38/100, Train Loss: -0.0272, Test Loss: -0.0573\n",
      "Epoch 39/100, Train Loss: -0.0276, Test Loss: -0.0580\n",
      "Epoch 40/100, Train Loss: -0.0279, Test Loss: -0.0583\n",
      "Epoch 41/100, Train Loss: -0.0282, Test Loss: -0.0585\n",
      "Epoch 42/100, Train Loss: -0.0284, Test Loss: -0.0586\n",
      "Epoch 43/100, Train Loss: -0.0287, Test Loss: -0.0584\n",
      "Epoch 44/100, Train Loss: -0.0287, Test Loss: -0.0581\n",
      "Epoch 45/100, Train Loss: -0.0285, Test Loss: -0.0590\n",
      "Epoch 46/100, Train Loss: -0.0290, Test Loss: -0.0559\n",
      "Epoch 47/100, Train Loss: -0.0261, Test Loss: -0.0588\n",
      "Epoch 48/100, Train Loss: -0.0279, Test Loss: -0.0548\n",
      "Epoch 49/100, Train Loss: -0.0264, Test Loss: -0.0581\n",
      "Epoch 50/100, Train Loss: -0.0289, Test Loss: -0.0585\n",
      "Epoch 51/100, Train Loss: -0.0292, Test Loss: -0.0589\n",
      "Epoch 52/100, Train Loss: -0.0293, Test Loss: -0.0594\n",
      "Epoch 53/100, Train Loss: -0.0295, Test Loss: -0.0595\n",
      "Epoch 54/100, Train Loss: -0.0293, Test Loss: -0.0595\n",
      "Epoch 55/100, Train Loss: -0.0294, Test Loss: -0.0594\n",
      "Epoch 56/100, Train Loss: -0.0292, Test Loss: -0.0596\n",
      "Epoch 57/100, Train Loss: -0.0295, Test Loss: -0.0594\n",
      "Epoch 58/100, Train Loss: -0.0298, Test Loss: -0.0596\n",
      "Epoch 59/100, Train Loss: -0.0302, Test Loss: -0.0594\n",
      "Epoch 60/100, Train Loss: -0.0306, Test Loss: -0.0598\n",
      "Epoch 61/100, Train Loss: -0.0308, Test Loss: -0.0599\n",
      "Epoch 62/100, Train Loss: -0.0310, Test Loss: -0.0602\n",
      "Epoch 63/100, Train Loss: -0.0310, Test Loss: -0.0602\n",
      "Epoch 64/100, Train Loss: -0.0312, Test Loss: -0.0603\n",
      "Epoch 65/100, Train Loss: -0.0312, Test Loss: -0.0603\n",
      "Epoch 66/100, Train Loss: -0.0314, Test Loss: -0.0605\n",
      "Epoch 67/100, Train Loss: -0.0314, Test Loss: -0.0602\n",
      "Epoch 68/100, Train Loss: -0.0316, Test Loss: -0.0608\n",
      "Epoch 69/100, Train Loss: -0.0317, Test Loss: -0.0603\n",
      "Epoch 70/100, Train Loss: -0.0318, Test Loss: -0.0609\n",
      "Epoch 71/100, Train Loss: -0.0320, Test Loss: -0.0604\n",
      "Epoch 72/100, Train Loss: -0.0322, Test Loss: -0.0611\n",
      "Epoch 73/100, Train Loss: -0.0324, Test Loss: -0.0606\n",
      "Epoch 74/100, Train Loss: -0.0325, Test Loss: -0.0615\n",
      "Epoch 75/100, Train Loss: -0.0328, Test Loss: -0.0611\n",
      "Epoch 76/100, Train Loss: -0.0329, Test Loss: -0.0619\n",
      "Epoch 77/100, Train Loss: -0.0332, Test Loss: -0.0617\n",
      "Epoch 78/100, Train Loss: -0.0334, Test Loss: -0.0620\n",
      "Epoch 79/100, Train Loss: -0.0336, Test Loss: -0.0622\n",
      "Epoch 80/100, Train Loss: -0.0337, Test Loss: -0.0622\n",
      "Epoch 81/100, Train Loss: -0.0338, Test Loss: -0.0626\n",
      "Epoch 82/100, Train Loss: -0.0340, Test Loss: -0.0629\n",
      "Epoch 83/100, Train Loss: -0.0341, Test Loss: -0.0631\n",
      "Epoch 84/100, Train Loss: -0.0343, Test Loss: -0.0625\n",
      "Epoch 85/100, Train Loss: -0.0337, Test Loss: -0.0620\n",
      "Epoch 86/100, Train Loss: -0.0335, Test Loss: -0.0634\n",
      "Epoch 87/100, Train Loss: -0.0339, Test Loss: -0.0593\n",
      "Epoch 88/100, Train Loss: -0.0297, Test Loss: -0.0555\n",
      "Epoch 89/100, Train Loss: -0.0278, Test Loss: -0.0583\n",
      "Epoch 90/100, Train Loss: -0.0273, Test Loss: -0.0603\n",
      "Epoch 91/100, Train Loss: -0.0292, Test Loss: -0.0581\n",
      "Epoch 92/100, Train Loss: -0.0310, Test Loss: -0.0614\n",
      "Epoch 93/100, Train Loss: -0.0325, Test Loss: -0.0600\n",
      "Epoch 94/100, Train Loss: -0.0327, Test Loss: -0.0615\n",
      "Epoch 95/100, Train Loss: -0.0325, Test Loss: -0.0611\n",
      "Epoch 96/100, Train Loss: -0.0325, Test Loss: -0.0624\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 0.2966, Test Loss: 0.0619\n",
      "Epoch 2/100, Train Loss: 0.1441, Test Loss: 0.0697\n",
      "Epoch 3/100, Train Loss: 0.0402, Test Loss: -0.0325\n",
      "Epoch 4/100, Train Loss: -0.0022, Test Loss: -0.0269\n",
      "Epoch 5/100, Train Loss: -0.0061, Test Loss: -0.0313\n",
      "Epoch 6/100, Train Loss: -0.0121, Test Loss: -0.0331\n",
      "Epoch 7/100, Train Loss: -0.0159, Test Loss: -0.0332\n",
      "Epoch 8/100, Train Loss: -0.0188, Test Loss: -0.0366\n",
      "Epoch 9/100, Train Loss: -0.0221, Test Loss: -0.0409\n",
      "Epoch 10/100, Train Loss: -0.0244, Test Loss: -0.0421\n",
      "Epoch 11/100, Train Loss: -0.0249, Test Loss: -0.0412\n",
      "Epoch 12/100, Train Loss: -0.0250, Test Loss: -0.0407\n",
      "Epoch 13/100, Train Loss: -0.0254, Test Loss: -0.0406\n",
      "Epoch 14/100, Train Loss: -0.0259, Test Loss: -0.0404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100, Train Loss: -0.0261, Test Loss: -0.0402\n",
      "Epoch 16/100, Train Loss: -0.0265, Test Loss: -0.0402\n",
      "Epoch 17/100, Train Loss: -0.0271, Test Loss: -0.0399\n",
      "Epoch 18/100, Train Loss: -0.0275, Test Loss: -0.0393\n",
      "Epoch 19/100, Train Loss: -0.0279, Test Loss: -0.0386\n",
      "Epoch 20/100, Train Loss: -0.0284, Test Loss: -0.0371\n",
      "Early stopping due to overfitting.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.21e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 256       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 7         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.25e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 281       |\n",
      "|    iterations             | 2         |\n",
      "|    time_elapsed           | 14        |\n",
      "|    total_timesteps        | 4096      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.85e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00518   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 57        |\n",
      "|    policy_objective       | 0.00494   |\n",
      "|    std                    | 1.24      |\n",
      "|    value_loss             | 1.92e+03  |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -1.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 298      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 20       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 1.91e-06 |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00643  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 58       |\n",
      "|    policy_objective       | 0.0101   |\n",
      "|    std                    | 1.29     |\n",
      "|    value_loss             | 2.01e+03 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -1.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 286      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 28       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 1.97e-06 |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00638  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 59       |\n",
      "|    policy_objective       | 0.00792  |\n",
      "|    std                    | 1.35     |\n",
      "|    value_loss             | 767      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.18e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 283       |\n",
      "|    iterations             | 5         |\n",
      "|    time_elapsed           | 36        |\n",
      "|    total_timesteps        | 10240     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 9.54e-07  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00613   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 60        |\n",
      "|    policy_objective       | 0.00566   |\n",
      "|    std                    | 1.4       |\n",
      "|    value_loss             | 1.21e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.17e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 279       |\n",
      "|    iterations             | 6         |\n",
      "|    time_elapsed           | 43        |\n",
      "|    total_timesteps        | 12288     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.49e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00668   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 61        |\n",
      "|    policy_objective       | 0.00911   |\n",
      "|    std                    | 1.37      |\n",
      "|    value_loss             | 822       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -1.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 277      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 51       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 3.52e-06 |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.0071   |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 62       |\n",
      "|    policy_objective       | 0.00836  |\n",
      "|    std                    | 1.4      |\n",
      "|    value_loss             | 1.21e+03 |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.18e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 278       |\n",
      "|    iterations             | 8         |\n",
      "|    time_elapsed           | 58        |\n",
      "|    total_timesteps        | 16384     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 9.95e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00946   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 63        |\n",
      "|    policy_objective       | 0.0102    |\n",
      "|    std                    | 1.47      |\n",
      "|    value_loss             | 1.81e+03  |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-1209.560057112947\n",
      "------------------------------\n",
      "round: 8\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 11.3570, Test Loss: 2.3123\n",
      "Epoch 2/100, Train Loss: 10.3712, Test Loss: 1.3447\n",
      "Epoch 3/100, Train Loss: 9.0099, Test Loss: 1.8360\n",
      "Epoch 4/100, Train Loss: 8.6278, Test Loss: 1.2198\n",
      "Epoch 5/100, Train Loss: 7.6190, Test Loss: 1.1355\n",
      "Epoch 6/100, Train Loss: 6.5975, Test Loss: 1.1567\n",
      "Epoch 7/100, Train Loss: 5.2142, Test Loss: 1.3422\n",
      "Epoch 8/100, Train Loss: 3.6536, Test Loss: 0.9470\n",
      "Epoch 9/100, Train Loss: 2.1300, Test Loss: 0.8520\n",
      "Epoch 10/100, Train Loss: 1.2042, Test Loss: 0.5852\n",
      "Epoch 11/100, Train Loss: 0.6704, Test Loss: 0.4105\n",
      "Epoch 12/100, Train Loss: 0.4005, Test Loss: 0.2024\n",
      "Epoch 13/100, Train Loss: 0.2276, Test Loss: 0.0923\n",
      "Epoch 14/100, Train Loss: 0.1331, Test Loss: 0.0186\n",
      "Epoch 15/100, Train Loss: 0.0790, Test Loss: -0.0064\n",
      "Epoch 16/100, Train Loss: 0.0516, Test Loss: -0.0281\n",
      "Epoch 17/100, Train Loss: 0.0303, Test Loss: -0.0351\n",
      "Epoch 18/100, Train Loss: 0.0193, Test Loss: -0.0426\n",
      "Epoch 19/100, Train Loss: 0.0098, Test Loss: -0.0491\n",
      "Epoch 20/100, Train Loss: 0.0042, Test Loss: -0.0516\n",
      "Epoch 21/100, Train Loss: 0.0003, Test Loss: -0.0543\n",
      "Epoch 22/100, Train Loss: -0.0034, Test Loss: -0.0550\n",
      "Epoch 23/100, Train Loss: -0.0058, Test Loss: -0.0564\n",
      "Epoch 24/100, Train Loss: -0.0081, Test Loss: -0.0586\n",
      "Epoch 25/100, Train Loss: -0.0099, Test Loss: -0.0599\n",
      "Epoch 26/100, Train Loss: -0.0114, Test Loss: -0.0609\n",
      "Epoch 27/100, Train Loss: -0.0128, Test Loss: -0.0615\n",
      "Epoch 28/100, Train Loss: -0.0140, Test Loss: -0.0622\n",
      "Epoch 29/100, Train Loss: -0.0151, Test Loss: -0.0631\n",
      "Epoch 30/100, Train Loss: -0.0161, Test Loss: -0.0640\n",
      "Epoch 31/100, Train Loss: -0.0170, Test Loss: -0.0647\n",
      "Epoch 32/100, Train Loss: -0.0178, Test Loss: -0.0653\n",
      "Epoch 33/100, Train Loss: -0.0186, Test Loss: -0.0658\n",
      "Epoch 34/100, Train Loss: -0.0194, Test Loss: -0.0664\n",
      "Epoch 35/100, Train Loss: -0.0202, Test Loss: -0.0670\n",
      "Epoch 36/100, Train Loss: -0.0209, Test Loss: -0.0676\n",
      "Epoch 37/100, Train Loss: -0.0216, Test Loss: -0.0681\n",
      "Epoch 38/100, Train Loss: -0.0223, Test Loss: -0.0686\n",
      "Epoch 39/100, Train Loss: -0.0230, Test Loss: -0.0691\n",
      "Epoch 40/100, Train Loss: -0.0236, Test Loss: -0.0697\n",
      "Epoch 41/100, Train Loss: -0.0243, Test Loss: -0.0702\n",
      "Epoch 42/100, Train Loss: -0.0249, Test Loss: -0.0707\n",
      "Epoch 43/100, Train Loss: -0.0255, Test Loss: -0.0712\n",
      "Epoch 44/100, Train Loss: -0.0261, Test Loss: -0.0717\n",
      "Epoch 45/100, Train Loss: -0.0267, Test Loss: -0.0721\n",
      "Epoch 46/100, Train Loss: -0.0273, Test Loss: -0.0725\n",
      "Epoch 47/100, Train Loss: -0.0278, Test Loss: -0.0730\n",
      "Epoch 48/100, Train Loss: -0.0283, Test Loss: -0.0727\n",
      "Epoch 49/100, Train Loss: -0.0280, Test Loss: -0.0711\n",
      "Epoch 50/100, Train Loss: -0.0272, Test Loss: -0.0712\n",
      "Epoch 51/100, Train Loss: -0.0282, Test Loss: -0.0732\n",
      "Epoch 52/100, Train Loss: -0.0290, Test Loss: -0.0710\n",
      "Epoch 53/100, Train Loss: -0.0286, Test Loss: -0.0704\n",
      "Epoch 54/100, Train Loss: -0.0289, Test Loss: -0.0719\n",
      "Epoch 55/100, Train Loss: -0.0294, Test Loss: -0.0705\n",
      "Epoch 56/100, Train Loss: -0.0290, Test Loss: -0.0712\n",
      "Epoch 57/100, Train Loss: -0.0276, Test Loss: -0.0692\n",
      "Epoch 58/100, Train Loss: -0.0231, Test Loss: -0.0728\n",
      "Epoch 59/100, Train Loss: -0.0167, Test Loss: -0.0733\n",
      "Epoch 60/100, Train Loss: -0.0215, Test Loss: -0.0617\n",
      "Epoch 61/100, Train Loss: -0.0277, Test Loss: -0.0720\n",
      "Epoch 62/100, Train Loss: -0.0269, Test Loss: -0.0681\n",
      "Epoch 63/100, Train Loss: -0.0293, Test Loss: -0.0718\n",
      "Epoch 64/100, Train Loss: -0.0303, Test Loss: -0.0705\n",
      "Epoch 65/100, Train Loss: -0.0298, Test Loss: -0.0711\n",
      "Epoch 66/100, Train Loss: -0.0307, Test Loss: -0.0721\n",
      "Epoch 67/100, Train Loss: -0.0313, Test Loss: -0.0719\n",
      "Epoch 68/100, Train Loss: -0.0312, Test Loss: -0.0720\n",
      "Epoch 69/100, Train Loss: -0.0316, Test Loss: -0.0725\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 3.2303, Test Loss: 0.4792\n",
      "Epoch 2/100, Train Loss: 1.3554, Test Loss: 0.0730\n",
      "Epoch 3/100, Train Loss: 0.5537, Test Loss: 0.0624\n",
      "Epoch 4/100, Train Loss: 0.3182, Test Loss: 0.0201\n",
      "Epoch 5/100, Train Loss: 0.1652, Test Loss: -0.0298\n",
      "Epoch 6/100, Train Loss: 0.0937, Test Loss: -0.0260\n",
      "Epoch 7/100, Train Loss: 0.0628, Test Loss: -0.0309\n",
      "Epoch 8/100, Train Loss: 0.0387, Test Loss: -0.0427\n",
      "Epoch 9/100, Train Loss: 0.0205, Test Loss: -0.0496\n",
      "Epoch 10/100, Train Loss: 0.0097, Test Loss: -0.0504\n",
      "Epoch 11/100, Train Loss: 0.0040, Test Loss: -0.0497\n",
      "Epoch 12/100, Train Loss: -0.0000, Test Loss: -0.0498\n",
      "Epoch 13/100, Train Loss: -0.0036, Test Loss: -0.0508\n",
      "Epoch 14/100, Train Loss: -0.0068, Test Loss: -0.0516\n",
      "Epoch 15/100, Train Loss: -0.0092, Test Loss: -0.0522\n",
      "Epoch 16/100, Train Loss: -0.0110, Test Loss: -0.0524\n",
      "Epoch 17/100, Train Loss: -0.0123, Test Loss: -0.0524\n",
      "Epoch 18/100, Train Loss: -0.0134, Test Loss: -0.0528\n",
      "Epoch 19/100, Train Loss: -0.0145, Test Loss: -0.0531\n",
      "Epoch 20/100, Train Loss: -0.0155, Test Loss: -0.0535\n",
      "Epoch 21/100, Train Loss: -0.0164, Test Loss: -0.0538\n",
      "Epoch 22/100, Train Loss: -0.0172, Test Loss: -0.0541\n",
      "Epoch 23/100, Train Loss: -0.0178, Test Loss: -0.0542\n",
      "Epoch 24/100, Train Loss: -0.0184, Test Loss: -0.0545\n",
      "Epoch 25/100, Train Loss: -0.0190, Test Loss: -0.0548\n",
      "Epoch 26/100, Train Loss: -0.0195, Test Loss: -0.0549\n",
      "Epoch 27/100, Train Loss: -0.0201, Test Loss: -0.0551\n",
      "Epoch 28/100, Train Loss: -0.0204, Test Loss: -0.0555\n",
      "Epoch 29/100, Train Loss: -0.0209, Test Loss: -0.0556\n",
      "Epoch 30/100, Train Loss: -0.0212, Test Loss: -0.0550\n",
      "Epoch 31/100, Train Loss: -0.0205, Test Loss: -0.0553\n",
      "Epoch 32/100, Train Loss: -0.0203, Test Loss: -0.0536\n",
      "Epoch 33/100, Train Loss: -0.0211, Test Loss: -0.0545\n",
      "Epoch 34/100, Train Loss: -0.0204, Test Loss: -0.0525\n",
      "Epoch 35/100, Train Loss: -0.0209, Test Loss: -0.0551\n",
      "Epoch 36/100, Train Loss: -0.0221, Test Loss: -0.0558\n",
      "Epoch 37/100, Train Loss: -0.0231, Test Loss: -0.0560\n",
      "Epoch 38/100, Train Loss: -0.0235, Test Loss: -0.0561\n",
      "Epoch 39/100, Train Loss: -0.0237, Test Loss: -0.0562\n",
      "Epoch 40/100, Train Loss: -0.0240, Test Loss: -0.0561\n",
      "Epoch 41/100, Train Loss: -0.0244, Test Loss: -0.0565\n",
      "Epoch 42/100, Train Loss: -0.0246, Test Loss: -0.0561\n",
      "Epoch 43/100, Train Loss: -0.0249, Test Loss: -0.0565\n",
      "Epoch 44/100, Train Loss: -0.0252, Test Loss: -0.0567\n",
      "Epoch 45/100, Train Loss: -0.0255, Test Loss: -0.0570\n",
      "Epoch 46/100, Train Loss: -0.0258, Test Loss: -0.0571\n",
      "Epoch 47/100, Train Loss: -0.0261, Test Loss: -0.0572\n",
      "Epoch 48/100, Train Loss: -0.0264, Test Loss: -0.0575\n",
      "Epoch 49/100, Train Loss: -0.0266, Test Loss: -0.0576\n",
      "Epoch 50/100, Train Loss: -0.0270, Test Loss: -0.0577\n",
      "Epoch 51/100, Train Loss: -0.0267, Test Loss: -0.0579\n",
      "Epoch 52/100, Train Loss: -0.0261, Test Loss: -0.0582\n",
      "Epoch 53/100, Train Loss: -0.0271, Test Loss: -0.0557\n",
      "Epoch 54/100, Train Loss: -0.0261, Test Loss: -0.0581\n",
      "Epoch 55/100, Train Loss: -0.0264, Test Loss: -0.0555\n",
      "Epoch 56/100, Train Loss: -0.0263, Test Loss: -0.0572\n",
      "Epoch 57/100, Train Loss: -0.0272, Test Loss: -0.0586\n",
      "Epoch 58/100, Train Loss: -0.0279, Test Loss: -0.0582\n",
      "Epoch 59/100, Train Loss: -0.0276, Test Loss: -0.0575\n",
      "Epoch 60/100, Train Loss: -0.0281, Test Loss: -0.0586\n",
      "Epoch 61/100, Train Loss: -0.0279, Test Loss: -0.0575\n",
      "Epoch 62/100, Train Loss: -0.0285, Test Loss: -0.0589\n",
      "Epoch 63/100, Train Loss: -0.0282, Test Loss: -0.0581\n",
      "Epoch 64/100, Train Loss: -0.0283, Test Loss: -0.0598\n",
      "Epoch 65/100, Train Loss: -0.0286, Test Loss: -0.0577\n",
      "Epoch 66/100, Train Loss: -0.0284, Test Loss: -0.0581\n",
      "Epoch 67/100, Train Loss: -0.0289, Test Loss: -0.0601\n",
      "Epoch 68/100, Train Loss: -0.0295, Test Loss: -0.0591\n",
      "Epoch 69/100, Train Loss: -0.0297, Test Loss: -0.0598\n",
      "Epoch 70/100, Train Loss: -0.0302, Test Loss: -0.0600\n",
      "Epoch 71/100, Train Loss: -0.0307, Test Loss: -0.0605\n",
      "Epoch 72/100, Train Loss: -0.0309, Test Loss: -0.0608\n",
      "Epoch 73/100, Train Loss: -0.0310, Test Loss: -0.0607\n",
      "Epoch 74/100, Train Loss: -0.0313, Test Loss: -0.0610\n",
      "Epoch 75/100, Train Loss: -0.0315, Test Loss: -0.0611\n",
      "Epoch 76/100, Train Loss: -0.0317, Test Loss: -0.0616\n",
      "Epoch 77/100, Train Loss: -0.0320, Test Loss: -0.0615\n",
      "Epoch 78/100, Train Loss: -0.0321, Test Loss: -0.0616\n",
      "Epoch 79/100, Train Loss: -0.0323, Test Loss: -0.0618\n",
      "Epoch 80/100, Train Loss: -0.0323, Test Loss: -0.0621\n",
      "Epoch 81/100, Train Loss: -0.0323, Test Loss: -0.0612\n",
      "Epoch 82/100, Train Loss: -0.0327, Test Loss: -0.0622\n",
      "Epoch 83/100, Train Loss: -0.0316, Test Loss: -0.0623\n",
      "Epoch 84/100, Train Loss: -0.0316, Test Loss: -0.0618\n",
      "Epoch 85/100, Train Loss: -0.0288, Test Loss: -0.0597\n",
      "Epoch 86/100, Train Loss: -0.0311, Test Loss: -0.0579\n",
      "Epoch 87/100, Train Loss: -0.0285, Test Loss: -0.0570\n",
      "Epoch 88/100, Train Loss: -0.0309, Test Loss: -0.0609\n",
      "Epoch 89/100, Train Loss: -0.0318, Test Loss: -0.0608\n",
      "Epoch 90/100, Train Loss: -0.0327, Test Loss: -0.0632\n",
      "Epoch 91/100, Train Loss: -0.0331, Test Loss: -0.0622\n",
      "Epoch 92/100, Train Loss: -0.0336, Test Loss: -0.0625\n",
      "Epoch 93/100, Train Loss: -0.0334, Test Loss: -0.0615\n",
      "Epoch 94/100, Train Loss: -0.0335, Test Loss: -0.0626\n",
      "Epoch 95/100, Train Loss: -0.0337, Test Loss: -0.0617\n",
      "Epoch 96/100, Train Loss: -0.0338, Test Loss: -0.0624\n",
      "Epoch 97/100, Train Loss: -0.0339, Test Loss: -0.0619\n",
      "Epoch 98/100, Train Loss: -0.0338, Test Loss: -0.0621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100, Train Loss: -0.0339, Test Loss: -0.0610\n",
      "Epoch 100/100, Train Loss: -0.0338, Test Loss: -0.0619\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: -0.0106, Test Loss: 0.2968\n",
      "Epoch 2/100, Train Loss: -0.0130, Test Loss: 0.1534\n",
      "Epoch 3/100, Train Loss: -0.0293, Test Loss: 0.0957\n",
      "Epoch 4/100, Train Loss: -0.0276, Test Loss: 0.0560\n",
      "Epoch 5/100, Train Loss: -0.0314, Test Loss: 0.0364\n",
      "Epoch 6/100, Train Loss: -0.0323, Test Loss: 0.0250\n",
      "Epoch 7/100, Train Loss: -0.0335, Test Loss: 0.0188\n",
      "Epoch 8/100, Train Loss: -0.0341, Test Loss: 0.0136\n",
      "Epoch 9/100, Train Loss: -0.0354, Test Loss: 0.0118\n",
      "Epoch 10/100, Train Loss: -0.0364, Test Loss: 0.0118\n",
      "Epoch 11/100, Train Loss: -0.0370, Test Loss: 0.0138\n",
      "Epoch 12/100, Train Loss: -0.0380, Test Loss: 0.0173\n",
      "Epoch 13/100, Train Loss: -0.0391, Test Loss: 0.0241\n",
      "Epoch 14/100, Train Loss: -0.0397, Test Loss: 0.0329\n",
      "Epoch 15/100, Train Loss: -0.0411, Test Loss: 0.0444\n",
      "Epoch 16/100, Train Loss: -0.0418, Test Loss: 0.0565\n",
      "Epoch 17/100, Train Loss: -0.0428, Test Loss: 0.0677\n",
      "Epoch 18/100, Train Loss: -0.0438, Test Loss: 0.0809\n",
      "Epoch 19/100, Train Loss: -0.0445, Test Loss: 0.0964\n",
      "Early stopping due to overfitting.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.19e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 287       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 7         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.16e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 287       |\n",
      "|    iterations             | 2         |\n",
      "|    time_elapsed           | 14        |\n",
      "|    total_timesteps        | 4096      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | -4.77e-07 |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00597   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 65        |\n",
      "|    policy_objective       | 0.00999   |\n",
      "|    std                    | 1.47      |\n",
      "|    value_loss             | 1.33e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.15e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 278       |\n",
      "|    iterations             | 3         |\n",
      "|    time_elapsed           | 22        |\n",
      "|    total_timesteps        | 6144      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | -5.96e-07 |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00809   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 66        |\n",
      "|    policy_objective       | 0.00734   |\n",
      "|    std                    | 1.48      |\n",
      "|    value_loss             | 1.33e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.17e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 281       |\n",
      "|    iterations             | 4         |\n",
      "|    time_elapsed           | 29        |\n",
      "|    total_timesteps        | 8192      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | -7.15e-07 |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00766   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 67        |\n",
      "|    policy_objective       | 0.00875   |\n",
      "|    std                    | 1.39      |\n",
      "|    value_loss             | 1.03e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.16e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 277       |\n",
      "|    iterations             | 5         |\n",
      "|    time_elapsed           | 36        |\n",
      "|    total_timesteps        | 10240     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 8.34e-07  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00699   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 68        |\n",
      "|    policy_objective       | 0.00942   |\n",
      "|    std                    | 1.4       |\n",
      "|    value_loss             | 1.01e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.17e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 271       |\n",
      "|    iterations             | 6         |\n",
      "|    time_elapsed           | 45        |\n",
      "|    total_timesteps        | 12288     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | -1.19e-07 |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00598   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 69        |\n",
      "|    policy_objective       | 0.00726   |\n",
      "|    std                    | 1.37      |\n",
      "|    value_loss             | 790       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -1.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 269      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 53       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 1.13e-06 |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00523  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 70       |\n",
      "|    policy_objective       | 0.00716  |\n",
      "|    std                    | 1.38     |\n",
      "|    value_loss             | 1.12e+03 |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.17e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 267       |\n",
      "|    iterations             | 8         |\n",
      "|    time_elapsed           | 61        |\n",
      "|    total_timesteps        | 16384     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 3.58e-07  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00368   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 71        |\n",
      "|    policy_objective       | 0.0104    |\n",
      "|    std                    | 1.39      |\n",
      "|    value_loss             | 1.44e+03  |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-1231.3144655518233\n",
      "------------------------------\n",
      "round: 9\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.1808, Test Loss: -0.0311\n",
      "Epoch 2/100, Train Loss: 0.0353, Test Loss: -0.0352\n",
      "Epoch 3/100, Train Loss: 0.0032, Test Loss: -0.0511\n",
      "Epoch 4/100, Train Loss: -0.0133, Test Loss: -0.0441\n",
      "Epoch 5/100, Train Loss: -0.0159, Test Loss: -0.0462\n",
      "Epoch 6/100, Train Loss: -0.0213, Test Loss: -0.0474\n",
      "Epoch 7/100, Train Loss: -0.0232, Test Loss: -0.0453\n",
      "Epoch 8/100, Train Loss: -0.0237, Test Loss: -0.0451\n",
      "Epoch 9/100, Train Loss: -0.0248, Test Loss: -0.0456\n",
      "Epoch 10/100, Train Loss: -0.0258, Test Loss: -0.0460\n",
      "Epoch 11/100, Train Loss: -0.0263, Test Loss: -0.0462\n",
      "Epoch 12/100, Train Loss: -0.0267, Test Loss: -0.0465\n",
      "Epoch 13/100, Train Loss: -0.0272, Test Loss: -0.0469\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 0.1695, Test Loss: -0.0419\n",
      "Epoch 2/100, Train Loss: 0.0605, Test Loss: -0.0256\n",
      "Epoch 3/100, Train Loss: 0.0067, Test Loss: -0.0491\n",
      "Epoch 4/100, Train Loss: -0.0091, Test Loss: -0.0341\n",
      "Epoch 5/100, Train Loss: -0.0095, Test Loss: -0.0371\n",
      "Epoch 6/100, Train Loss: -0.0159, Test Loss: -0.0428\n",
      "Epoch 7/100, Train Loss: -0.0204, Test Loss: -0.0436\n",
      "Epoch 8/100, Train Loss: -0.0211, Test Loss: -0.0423\n",
      "Epoch 9/100, Train Loss: -0.0209, Test Loss: -0.0415\n",
      "Epoch 10/100, Train Loss: -0.0213, Test Loss: -0.0419\n",
      "Epoch 11/100, Train Loss: -0.0222, Test Loss: -0.0423\n",
      "Epoch 12/100, Train Loss: -0.0228, Test Loss: -0.0425\n",
      "Epoch 13/100, Train Loss: -0.0230, Test Loss: -0.0423\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 0.4176, Test Loss: -0.0458\n",
      "Epoch 2/100, Train Loss: 0.1949, Test Loss: -0.0398\n",
      "Epoch 3/100, Train Loss: 0.0868, Test Loss: -0.0530\n",
      "Epoch 4/100, Train Loss: 0.0344, Test Loss: -0.0510\n",
      "Epoch 5/100, Train Loss: 0.0127, Test Loss: -0.0485\n",
      "Epoch 6/100, Train Loss: 0.0007, Test Loss: -0.0483\n",
      "Epoch 7/100, Train Loss: -0.0071, Test Loss: -0.0480\n",
      "Epoch 8/100, Train Loss: -0.0113, Test Loss: -0.0476\n",
      "Epoch 9/100, Train Loss: -0.0140, Test Loss: -0.0473\n",
      "Epoch 10/100, Train Loss: -0.0158, Test Loss: -0.0465\n",
      "Epoch 11/100, Train Loss: -0.0167, Test Loss: -0.0458\n",
      "Epoch 12/100, Train Loss: -0.0174, Test Loss: -0.0457\n",
      "Epoch 13/100, Train Loss: -0.0182, Test Loss: -0.0459\n",
      "Early stopping due to overfitting.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.12e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 261       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 7         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.18e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 251       |\n",
      "|    iterations             | 2         |\n",
      "|    time_elapsed           | 16        |\n",
      "|    total_timesteps        | 4096      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 4.77e-07  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00649   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 73        |\n",
      "|    policy_objective       | 0.00861   |\n",
      "|    std                    | 1.45      |\n",
      "|    value_loss             | 1.05e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.18e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 248       |\n",
      "|    iterations             | 3         |\n",
      "|    time_elapsed           | 24        |\n",
      "|    total_timesteps        | 6144      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.79e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00698   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 74        |\n",
      "|    policy_objective       | 0.00917   |\n",
      "|    std                    | 1.39      |\n",
      "|    value_loss             | 1.39e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.18e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 246       |\n",
      "|    iterations             | 4         |\n",
      "|    time_elapsed           | 33        |\n",
      "|    total_timesteps        | 8192      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.79e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00907   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 75        |\n",
      "|    policy_objective       | 0.0136    |\n",
      "|    std                    | 1.4       |\n",
      "|    value_loss             | 1.12e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.21e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 245       |\n",
      "|    iterations             | 5         |\n",
      "|    time_elapsed           | 41        |\n",
      "|    total_timesteps        | 10240     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.01e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00537   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 76        |\n",
      "|    policy_objective       | 0.00587   |\n",
      "|    std                    | 1.37      |\n",
      "|    value_loss             | 1.1e+03   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.19e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 248       |\n",
      "|    iterations             | 6         |\n",
      "|    time_elapsed           | 49        |\n",
      "|    total_timesteps        | 12288     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.91e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00633   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 77        |\n",
      "|    policy_objective       | 0.00891   |\n",
      "|    std                    | 1.42      |\n",
      "|    value_loss             | 1.23e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.17e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 246       |\n",
      "|    iterations             | 7         |\n",
      "|    time_elapsed           | 58        |\n",
      "|    total_timesteps        | 14336     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 3.52e-06  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00769   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 78        |\n",
      "|    policy_objective       | 0.0109    |\n",
      "|    std                    | 1.38      |\n",
      "|    value_loss             | 799       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.18e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 248       |\n",
      "|    iterations             | 8         |\n",
      "|    time_elapsed           | 65        |\n",
      "|    total_timesteps        | 16384     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 1.04e-05  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00586   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 79        |\n",
      "|    policy_objective       | 0.0113    |\n",
      "|    std                    | 1.41      |\n",
      "|    value_loss             | 987       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-1416.7280437111854\n",
      "------------------------------\n",
      "round: 10\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: -0.0133, Test Loss: -0.0429\n",
      "Epoch 2/100, Train Loss: -0.0211, Test Loss: -0.0416\n",
      "Epoch 3/100, Train Loss: -0.0268, Test Loss: -0.0453\n",
      "Epoch 4/100, Train Loss: -0.0271, Test Loss: -0.0450\n",
      "Epoch 5/100, Train Loss: -0.0283, Test Loss: -0.0467\n",
      "Epoch 6/100, Train Loss: -0.0290, Test Loss: -0.0462\n",
      "Epoch 7/100, Train Loss: -0.0298, Test Loss: -0.0476\n",
      "Epoch 8/100, Train Loss: -0.0307, Test Loss: -0.0480\n",
      "Epoch 9/100, Train Loss: -0.0314, Test Loss: -0.0490\n",
      "Epoch 10/100, Train Loss: -0.0325, Test Loss: -0.0494\n",
      "Epoch 11/100, Train Loss: -0.0334, Test Loss: -0.0496\n",
      "Epoch 12/100, Train Loss: -0.0342, Test Loss: -0.0498\n",
      "Epoch 13/100, Train Loss: -0.0343, Test Loss: -0.0503\n",
      "Epoch 14/100, Train Loss: -0.0353, Test Loss: -0.0515\n",
      "Epoch 15/100, Train Loss: -0.0354, Test Loss: -0.0524\n",
      "Epoch 16/100, Train Loss: -0.0365, Test Loss: -0.0515\n",
      "Epoch 17/100, Train Loss: -0.0364, Test Loss: -0.0529\n",
      "Epoch 18/100, Train Loss: -0.0375, Test Loss: -0.0543\n",
      "Epoch 19/100, Train Loss: -0.0383, Test Loss: -0.0532\n",
      "Epoch 20/100, Train Loss: -0.0387, Test Loss: -0.0537\n",
      "Epoch 21/100, Train Loss: -0.0396, Test Loss: -0.0538\n",
      "Epoch 22/100, Train Loss: -0.0400, Test Loss: -0.0531\n",
      "Epoch 23/100, Train Loss: -0.0397, Test Loss: -0.0532\n",
      "Epoch 24/100, Train Loss: -0.0402, Test Loss: -0.0549\n",
      "Epoch 25/100, Train Loss: -0.0405, Test Loss: -0.0507\n",
      "Epoch 26/100, Train Loss: -0.0394, Test Loss: -0.0524\n",
      "Epoch 27/100, Train Loss: -0.0406, Test Loss: -0.0510\n",
      "Epoch 28/100, Train Loss: -0.0407, Test Loss: -0.0519\n",
      "Epoch 29/100, Train Loss: -0.0413, Test Loss: -0.0538\n",
      "Epoch 30/100, Train Loss: -0.0423, Test Loss: -0.0523\n",
      "Epoch 31/100, Train Loss: -0.0421, Test Loss: -0.0492\n",
      "Epoch 32/100, Train Loss: -0.0427, Test Loss: -0.0522\n",
      "Epoch 33/100, Train Loss: -0.0435, Test Loss: -0.0518\n",
      "Epoch 34/100, Train Loss: -0.0440, Test Loss: -0.0478\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 0.0246, Test Loss: -0.0093\n",
      "Epoch 2/100, Train Loss: 0.0205, Test Loss: -0.0392\n",
      "Epoch 3/100, Train Loss: -0.0177, Test Loss: -0.0293\n",
      "Epoch 4/100, Train Loss: -0.0082, Test Loss: -0.0286\n",
      "Epoch 5/100, Train Loss: -0.0144, Test Loss: -0.0396\n",
      "Epoch 6/100, Train Loss: -0.0206, Test Loss: -0.0382\n",
      "Epoch 7/100, Train Loss: -0.0189, Test Loss: -0.0346\n",
      "Epoch 8/100, Train Loss: -0.0178, Test Loss: -0.0359\n",
      "Epoch 9/100, Train Loss: -0.0193, Test Loss: -0.0382\n",
      "Epoch 10/100, Train Loss: -0.0206, Test Loss: -0.0384\n",
      "Epoch 11/100, Train Loss: -0.0205, Test Loss: -0.0377\n",
      "Epoch 12/100, Train Loss: -0.0201, Test Loss: -0.0378\n",
      "Epoch 13/100, Train Loss: -0.0204, Test Loss: -0.0387\n",
      "Epoch 14/100, Train Loss: -0.0209, Test Loss: -0.0393\n",
      "Epoch 15/100, Train Loss: -0.0212, Test Loss: -0.0395\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: -0.0098, Test Loss: -0.0269\n",
      "Epoch 2/100, Train Loss: -0.0148, Test Loss: -0.0376\n",
      "Epoch 3/100, Train Loss: -0.0247, Test Loss: -0.0419\n",
      "Epoch 4/100, Train Loss: -0.0227, Test Loss: -0.0419\n",
      "Epoch 5/100, Train Loss: -0.0251, Test Loss: -0.0461\n",
      "Epoch 6/100, Train Loss: -0.0271, Test Loss: -0.0444\n",
      "Epoch 7/100, Train Loss: -0.0268, Test Loss: -0.0453\n",
      "Epoch 8/100, Train Loss: -0.0279, Test Loss: -0.0487\n",
      "Epoch 9/100, Train Loss: -0.0294, Test Loss: -0.0509\n",
      "Epoch 10/100, Train Loss: -0.0297, Test Loss: -0.0518\n",
      "Epoch 11/100, Train Loss: -0.0307, Test Loss: -0.0540\n",
      "Epoch 12/100, Train Loss: -0.0316, Test Loss: -0.0542\n",
      "Epoch 13/100, Train Loss: -0.0324, Test Loss: -0.0553\n",
      "Epoch 14/100, Train Loss: -0.0334, Test Loss: -0.0572\n",
      "Epoch 15/100, Train Loss: -0.0342, Test Loss: -0.0583\n",
      "Epoch 16/100, Train Loss: -0.0351, Test Loss: -0.0587\n",
      "Epoch 17/100, Train Loss: -0.0357, Test Loss: -0.0595\n",
      "Epoch 18/100, Train Loss: -0.0365, Test Loss: -0.0587\n",
      "Epoch 19/100, Train Loss: -0.0367, Test Loss: -0.0543\n",
      "Epoch 20/100, Train Loss: -0.0367, Test Loss: -0.0542\n",
      "Epoch 21/100, Train Loss: -0.0368, Test Loss: -0.0494\n",
      "Epoch 22/100, Train Loss: -0.0353, Test Loss: -0.0428\n",
      "Epoch 23/100, Train Loss: -0.0326, Test Loss: -0.0403\n",
      "Epoch 24/100, Train Loss: -0.0319, Test Loss: -0.0500\n",
      "Epoch 25/100, Train Loss: -0.0350, Test Loss: -0.0508\n",
      "Epoch 26/100, Train Loss: -0.0361, Test Loss: -0.0536\n",
      "Epoch 27/100, Train Loss: -0.0361, Test Loss: -0.0470\n",
      "Early stopping due to overfitting.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.11e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 268       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 7         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.09e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 270       |\n",
      "|    iterations             | 2         |\n",
      "|    time_elapsed           | 15        |\n",
      "|    total_timesteps        | 4096      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.404     |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00771   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 81        |\n",
      "|    policy_objective       | 0.00895   |\n",
      "|    std                    | 1.33      |\n",
      "|    value_loss             | 1.09e+03  |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -1.1e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 272      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 22       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.434    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.007    |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 82       |\n",
      "|    policy_objective       | 0.00975  |\n",
      "|    std                    | 1.33     |\n",
      "|    value_loss             | 1.32e+03 |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.09e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 267       |\n",
      "|    iterations             | 4         |\n",
      "|    time_elapsed           | 30        |\n",
      "|    total_timesteps        | 8192      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.429     |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.0055    |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 83        |\n",
      "|    policy_objective       | 0.00596   |\n",
      "|    std                    | 1.35      |\n",
      "|    value_loss             | 1.19e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.09e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 262       |\n",
      "|    iterations             | 5         |\n",
      "|    time_elapsed           | 38        |\n",
      "|    total_timesteps        | 10240     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.468     |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00643   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 84        |\n",
      "|    policy_objective       | 0.00888   |\n",
      "|    std                    | 1.32      |\n",
      "|    value_loss             | 685       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -1.1e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 260      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 47       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.431    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00681  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 85       |\n",
      "|    policy_objective       | 0.00777  |\n",
      "|    std                    | 1.29     |\n",
      "|    value_loss             | 975      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -1.1e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 262      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 54       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.48     |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00702  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 86       |\n",
      "|    policy_objective       | 0.00611  |\n",
      "|    std                    | 1.24     |\n",
      "|    value_loss             | 1.16e+03 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -1.1e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 260      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 62       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.37     |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00734  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 87       |\n",
      "|    policy_objective       | 0.0128   |\n",
      "|    std                    | 1.23     |\n",
      "|    value_loss             | 1.16e+03 |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-1271.1095975561998\n",
      "------------------------------\n",
      "round: 11\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.0825, Test Loss: 0.1700\n",
      "Epoch 2/100, Train Loss: 0.0628, Test Loss: 0.0856\n",
      "Epoch 3/100, Train Loss: -0.0159, Test Loss: 0.0047\n",
      "Epoch 4/100, Train Loss: -0.0258, Test Loss: 0.0121\n",
      "Epoch 5/100, Train Loss: -0.0170, Test Loss: 0.0005\n",
      "Epoch 6/100, Train Loss: -0.0243, Test Loss: -0.0188\n",
      "Epoch 7/100, Train Loss: -0.0321, Test Loss: -0.0270\n",
      "Epoch 8/100, Train Loss: -0.0337, Test Loss: -0.0285\n",
      "Epoch 9/100, Train Loss: -0.0336, Test Loss: -0.0292\n",
      "Epoch 10/100, Train Loss: -0.0334, Test Loss: -0.0297\n",
      "Epoch 11/100, Train Loss: -0.0333, Test Loss: -0.0304\n",
      "Epoch 12/100, Train Loss: -0.0336, Test Loss: -0.0313\n",
      "Epoch 13/100, Train Loss: -0.0340, Test Loss: -0.0323\n",
      "Epoch 14/100, Train Loss: -0.0348, Test Loss: -0.0330\n",
      "Epoch 15/100, Train Loss: -0.0354, Test Loss: -0.0329\n",
      "Epoch 16/100, Train Loss: -0.0356, Test Loss: -0.0325\n",
      "Epoch 17/100, Train Loss: -0.0359, Test Loss: -0.0322\n",
      "Epoch 18/100, Train Loss: -0.0364, Test Loss: -0.0321\n",
      "Epoch 19/100, Train Loss: -0.0368, Test Loss: -0.0317\n",
      "Epoch 20/100, Train Loss: -0.0374, Test Loss: -0.0311\n",
      "Epoch 21/100, Train Loss: -0.0379, Test Loss: -0.0303\n",
      "Epoch 22/100, Train Loss: -0.0384, Test Loss: -0.0296\n",
      "Epoch 23/100, Train Loss: -0.0388, Test Loss: -0.0288\n",
      "Epoch 24/100, Train Loss: -0.0394, Test Loss: -0.0280\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: -0.0194, Test Loss: 0.0030\n",
      "Epoch 2/100, Train Loss: -0.0232, Test Loss: -0.0029\n",
      "Epoch 3/100, Train Loss: -0.0266, Test Loss: -0.0044\n",
      "Epoch 4/100, Train Loss: -0.0262, Test Loss: -0.0057\n",
      "Epoch 5/100, Train Loss: -0.0287, Test Loss: -0.0008\n",
      "Epoch 6/100, Train Loss: -0.0292, Test Loss: 0.0001\n",
      "Epoch 7/100, Train Loss: -0.0310, Test Loss: 0.0042\n",
      "Epoch 8/100, Train Loss: -0.0321, Test Loss: 0.0138\n",
      "Epoch 9/100, Train Loss: -0.0322, Test Loss: 0.0243\n",
      "Epoch 10/100, Train Loss: -0.0331, Test Loss: 0.0302\n",
      "Epoch 11/100, Train Loss: -0.0349, Test Loss: 0.0434\n",
      "Epoch 12/100, Train Loss: -0.0352, Test Loss: 0.0499\n",
      "Epoch 13/100, Train Loss: -0.0355, Test Loss: 0.0602\n",
      "Epoch 14/100, Train Loss: -0.0360, Test Loss: 0.0673\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 1.0320, Test Loss: -0.0521\n",
      "Epoch 2/100, Train Loss: 0.4427, Test Loss: 0.0039\n",
      "Epoch 3/100, Train Loss: 0.2192, Test Loss: -0.0147\n",
      "Epoch 4/100, Train Loss: 0.1078, Test Loss: -0.0346\n",
      "Epoch 5/100, Train Loss: 0.0537, Test Loss: -0.0394\n",
      "Epoch 6/100, Train Loss: 0.0289, Test Loss: -0.0398\n",
      "Epoch 7/100, Train Loss: 0.0157, Test Loss: -0.0404\n",
      "Epoch 8/100, Train Loss: 0.0067, Test Loss: -0.0415\n",
      "Epoch 9/100, Train Loss: 0.0008, Test Loss: -0.0424\n",
      "Epoch 10/100, Train Loss: -0.0033, Test Loss: -0.0431\n",
      "Epoch 11/100, Train Loss: -0.0065, Test Loss: -0.0438\n",
      "Early stopping due to overfitting.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.07e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 326       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 6         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -1.1e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 297      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 13       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.433    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00917  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 89       |\n",
      "|    policy_objective       | 0.0111   |\n",
      "|    std                    | 1.16     |\n",
      "|    value_loss             | 1.14e+03 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -1.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 288      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 21       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.46     |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00589  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 90       |\n",
      "|    policy_objective       | 0.00811  |\n",
      "|    std                    | 1.15     |\n",
      "|    value_loss             | 1.17e+03 |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.15e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 287       |\n",
      "|    iterations             | 4         |\n",
      "|    time_elapsed           | 28        |\n",
      "|    total_timesteps        | 8192      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.496     |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00734   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 91        |\n",
      "|    policy_objective       | 0.0119    |\n",
      "|    std                    | 1.16      |\n",
      "|    value_loss             | 1.83e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.14e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 292       |\n",
      "|    iterations             | 5         |\n",
      "|    time_elapsed           | 34        |\n",
      "|    total_timesteps        | 10240     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.488     |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00697   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 92        |\n",
      "|    policy_objective       | 0.00806   |\n",
      "|    std                    | 1.16      |\n",
      "|    value_loss             | 1.19e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.13e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 297       |\n",
      "|    iterations             | 6         |\n",
      "|    time_elapsed           | 41        |\n",
      "|    total_timesteps        | 12288     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.518     |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00445   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 93        |\n",
      "|    policy_objective       | 0.00575   |\n",
      "|    std                    | 1.11      |\n",
      "|    value_loss             | 1.22e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.12e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 302       |\n",
      "|    iterations             | 7         |\n",
      "|    time_elapsed           | 47        |\n",
      "|    total_timesteps        | 14336     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.507     |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00548   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 94        |\n",
      "|    policy_objective       | 0.00811   |\n",
      "|    std                    | 1.1       |\n",
      "|    value_loss             | 1.62e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.13e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 301       |\n",
      "|    iterations             | 8         |\n",
      "|    time_elapsed           | 54        |\n",
      "|    total_timesteps        | 16384     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.494     |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00696   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 95        |\n",
      "|    policy_objective       | 0.00948   |\n",
      "|    std                    | 1.08      |\n",
      "|    value_loss             | 1.13e+03  |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-1251.9016894541214\n",
      "------------------------------\n",
      "round: 12\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.0439, Test Loss: 1.7012\n",
      "Epoch 2/100, Train Loss: 0.0345, Test Loss: 0.7926\n",
      "Epoch 3/100, Train Loss: -0.0224, Test Loss: 0.4499\n",
      "Epoch 4/100, Train Loss: -0.0148, Test Loss: 0.2728\n",
      "Epoch 5/100, Train Loss: -0.0242, Test Loss: 0.1665\n",
      "Epoch 6/100, Train Loss: -0.0313, Test Loss: 0.1190\n",
      "Epoch 7/100, Train Loss: -0.0304, Test Loss: 0.0980\n",
      "Epoch 8/100, Train Loss: -0.0289, Test Loss: 0.0852\n",
      "Epoch 9/100, Train Loss: -0.0294, Test Loss: 0.0770\n",
      "Epoch 10/100, Train Loss: -0.0312, Test Loss: 0.0741\n",
      "Epoch 11/100, Train Loss: -0.0327, Test Loss: 0.0774\n",
      "Epoch 12/100, Train Loss: -0.0328, Test Loss: 0.0851\n",
      "Epoch 13/100, Train Loss: -0.0329, Test Loss: 0.0938\n",
      "Epoch 14/100, Train Loss: -0.0337, Test Loss: 0.1028\n",
      "Epoch 15/100, Train Loss: -0.0348, Test Loss: 0.1142\n",
      "Epoch 16/100, Train Loss: -0.0357, Test Loss: 0.1310\n",
      "Epoch 17/100, Train Loss: -0.0364, Test Loss: 0.1531\n",
      "Epoch 18/100, Train Loss: -0.0372, Test Loss: 0.1784\n",
      "Epoch 19/100, Train Loss: -0.0378, Test Loss: 0.2080\n",
      "Epoch 20/100, Train Loss: -0.0371, Test Loss: 0.2459\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: 0.1993, Test Loss: 0.1260\n",
      "Epoch 2/100, Train Loss: 0.1106, Test Loss: 0.0879\n",
      "Epoch 3/100, Train Loss: 0.0002, Test Loss: 0.0127\n",
      "Epoch 4/100, Train Loss: 0.0008, Test Loss: 0.0399\n",
      "Epoch 5/100, Train Loss: 0.0052, Test Loss: -0.0063\n",
      "Epoch 6/100, Train Loss: -0.0215, Test Loss: -0.0320\n",
      "Epoch 7/100, Train Loss: -0.0299, Test Loss: -0.0297\n",
      "Epoch 8/100, Train Loss: -0.0256, Test Loss: -0.0250\n",
      "Epoch 9/100, Train Loss: -0.0232, Test Loss: -0.0272\n",
      "Epoch 10/100, Train Loss: -0.0251, Test Loss: -0.0321\n",
      "Epoch 11/100, Train Loss: -0.0279, Test Loss: -0.0353\n",
      "Epoch 12/100, Train Loss: -0.0295, Test Loss: -0.0363\n",
      "Epoch 13/100, Train Loss: -0.0299, Test Loss: -0.0357\n",
      "Epoch 14/100, Train Loss: -0.0295, Test Loss: -0.0349\n",
      "Epoch 15/100, Train Loss: -0.0295, Test Loss: -0.0353\n",
      "Epoch 16/100, Train Loss: -0.0300, Test Loss: -0.0358\n",
      "Epoch 17/100, Train Loss: -0.0306, Test Loss: -0.0362\n",
      "Epoch 18/100, Train Loss: -0.0312, Test Loss: -0.0363\n",
      "Epoch 19/100, Train Loss: -0.0315, Test Loss: -0.0359\n",
      "Epoch 20/100, Train Loss: -0.0317, Test Loss: -0.0357\n",
      "Epoch 21/100, Train Loss: -0.0320, Test Loss: -0.0355\n",
      "Epoch 22/100, Train Loss: -0.0323, Test Loss: -0.0350\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: -0.0213, Test Loss: -0.0463\n",
      "Epoch 2/100, Train Loss: -0.0253, Test Loss: -0.0472\n",
      "Epoch 3/100, Train Loss: -0.0273, Test Loss: -0.0485\n",
      "Epoch 4/100, Train Loss: -0.0292, Test Loss: -0.0511\n",
      "Epoch 5/100, Train Loss: -0.0303, Test Loss: -0.0547\n",
      "Epoch 6/100, Train Loss: -0.0321, Test Loss: -0.0567\n",
      "Epoch 7/100, Train Loss: -0.0336, Test Loss: -0.0591\n",
      "Epoch 8/100, Train Loss: -0.0347, Test Loss: -0.0624\n",
      "Epoch 9/100, Train Loss: -0.0361, Test Loss: -0.0636\n",
      "Epoch 10/100, Train Loss: -0.0370, Test Loss: -0.0646\n",
      "Epoch 11/100, Train Loss: -0.0376, Test Loss: -0.0648\n",
      "Epoch 12/100, Train Loss: -0.0382, Test Loss: -0.0670\n",
      "Epoch 13/100, Train Loss: -0.0384, Test Loss: -0.0674\n",
      "Epoch 14/100, Train Loss: -0.0390, Test Loss: -0.0673\n",
      "Epoch 15/100, Train Loss: -0.0401, Test Loss: -0.0693\n",
      "Epoch 16/100, Train Loss: -0.0409, Test Loss: -0.0698\n",
      "Epoch 17/100, Train Loss: -0.0416, Test Loss: -0.0706\n",
      "Epoch 18/100, Train Loss: -0.0424, Test Loss: -0.0707\n",
      "Epoch 19/100, Train Loss: -0.0424, Test Loss: -0.0723\n",
      "Epoch 20/100, Train Loss: -0.0432, Test Loss: -0.0719\n",
      "Epoch 21/100, Train Loss: -0.0429, Test Loss: -0.0728\n",
      "Epoch 22/100, Train Loss: -0.0435, Test Loss: -0.0696\n",
      "Epoch 23/100, Train Loss: -0.0435, Test Loss: -0.0709\n",
      "Epoch 24/100, Train Loss: -0.0423, Test Loss: -0.0710\n",
      "Epoch 25/100, Train Loss: -0.0435, Test Loss: -0.0639\n",
      "Epoch 26/100, Train Loss: -0.0439, Test Loss: -0.0699\n",
      "Epoch 27/100, Train Loss: -0.0444, Test Loss: -0.0765\n",
      "Epoch 28/100, Train Loss: -0.0464, Test Loss: -0.0751\n",
      "Epoch 29/100, Train Loss: -0.0475, Test Loss: -0.0774\n",
      "Epoch 30/100, Train Loss: -0.0476, Test Loss: -0.0787\n",
      "Epoch 31/100, Train Loss: -0.0472, Test Loss: -0.0792\n",
      "Epoch 32/100, Train Loss: -0.0486, Test Loss: -0.0782\n",
      "Epoch 33/100, Train Loss: -0.0478, Test Loss: -0.0804\n",
      "Epoch 34/100, Train Loss: -0.0483, Test Loss: -0.0787\n",
      "Epoch 35/100, Train Loss: -0.0467, Test Loss: -0.0661\n",
      "Epoch 36/100, Train Loss: -0.0421, Test Loss: -0.0568\n",
      "Epoch 37/100, Train Loss: -0.0417, Test Loss: -0.0719\n",
      "Epoch 38/100, Train Loss: -0.0344, Test Loss: -0.0729\n",
      "Epoch 39/100, Train Loss: -0.0402, Test Loss: -0.0636\n",
      "Epoch 40/100, Train Loss: -0.0444, Test Loss: -0.0685\n",
      "Epoch 41/100, Train Loss: -0.0457, Test Loss: -0.0719\n",
      "Epoch 42/100, Train Loss: -0.0463, Test Loss: -0.0726\n",
      "Epoch 43/100, Train Loss: -0.0465, Test Loss: -0.0740\n",
      "Early stopping due to overfitting.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.16e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 335       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 6         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.12e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 327       |\n",
      "|    iterations             | 2         |\n",
      "|    time_elapsed           | 12        |\n",
      "|    total_timesteps        | 4096      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.443     |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00851   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 97        |\n",
      "|    policy_objective       | 0.0107    |\n",
      "|    std                    | 1.08      |\n",
      "|    value_loss             | 863       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.16e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 321       |\n",
      "|    iterations             | 3         |\n",
      "|    time_elapsed           | 19        |\n",
      "|    total_timesteps        | 6144      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.462     |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00621   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 98        |\n",
      "|    policy_objective       | 0.0065    |\n",
      "|    std                    | 1.1       |\n",
      "|    value_loss             | 1.15e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.12e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 313       |\n",
      "|    iterations             | 4         |\n",
      "|    time_elapsed           | 26        |\n",
      "|    total_timesteps        | 8192      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.356     |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00762   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 99        |\n",
      "|    policy_objective       | 0.011     |\n",
      "|    std                    | 1.08      |\n",
      "|    value_loss             | 955       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.15e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 309       |\n",
      "|    iterations             | 5         |\n",
      "|    time_elapsed           | 33        |\n",
      "|    total_timesteps        | 10240     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.497     |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00818   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 100       |\n",
      "|    policy_objective       | 0.00789   |\n",
      "|    std                    | 1.04      |\n",
      "|    value_loss             | 1.97e+03  |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.14e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 307       |\n",
      "|    iterations             | 6         |\n",
      "|    time_elapsed           | 40        |\n",
      "|    total_timesteps        | 12288     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.467     |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00727   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 101       |\n",
      "|    policy_objective       | 0.00983   |\n",
      "|    std                    | 1.02      |\n",
      "|    value_loss             | 1.19e+03  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.13e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 299       |\n",
      "|    iterations             | 7         |\n",
      "|    time_elapsed           | 47        |\n",
      "|    total_timesteps        | 14336     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.502     |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00721   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 102       |\n",
      "|    policy_objective       | 0.0108    |\n",
      "|    std                    | 1.03      |\n",
      "|    value_loss             | 1.1e+03   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.12e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 299       |\n",
      "|    iterations             | 8         |\n",
      "|    time_elapsed           | 54        |\n",
      "|    total_timesteps        | 16384     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.487     |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00806   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 103       |\n",
      "|    policy_objective       | 0.00981   |\n",
      "|    std                    | 1.01      |\n",
      "|    value_loss             | 967       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward in real env:-1406.4073241241276\n",
      "------------------------------\n",
      "round: 13\n",
      "------------------------------\n",
      "client: 0\n",
      "Epoch 1/100, Train Loss: 0.0472, Test Loss: -0.0435\n",
      "Epoch 2/100, Train Loss: 0.0145, Test Loss: -0.0223\n",
      "Epoch 3/100, Train Loss: -0.0151, Test Loss: -0.0498\n",
      "Epoch 4/100, Train Loss: -0.0218, Test Loss: -0.0381\n",
      "Epoch 5/100, Train Loss: -0.0205, Test Loss: -0.0427\n",
      "Epoch 6/100, Train Loss: -0.0255, Test Loss: -0.0479\n",
      "Epoch 7/100, Train Loss: -0.0278, Test Loss: -0.0479\n",
      "Epoch 8/100, Train Loss: -0.0273, Test Loss: -0.0466\n",
      "Epoch 9/100, Train Loss: -0.0268, Test Loss: -0.0467\n",
      "Epoch 10/100, Train Loss: -0.0274, Test Loss: -0.0478\n",
      "Epoch 11/100, Train Loss: -0.0284, Test Loss: -0.0487\n",
      "Epoch 12/100, Train Loss: -0.0291, Test Loss: -0.0491\n",
      "Epoch 13/100, Train Loss: -0.0293, Test Loss: -0.0494\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 1\n",
      "Epoch 1/100, Train Loss: -0.0058, Test Loss: 0.0204\n",
      "Epoch 2/100, Train Loss: -0.0083, Test Loss: -0.0128\n",
      "Epoch 3/100, Train Loss: -0.0219, Test Loss: -0.0197\n",
      "Epoch 4/100, Train Loss: -0.0252, Test Loss: -0.0241\n",
      "Epoch 5/100, Train Loss: -0.0245, Test Loss: -0.0265\n",
      "Epoch 6/100, Train Loss: -0.0273, Test Loss: -0.0295\n",
      "Epoch 7/100, Train Loss: -0.0290, Test Loss: -0.0289\n",
      "Epoch 8/100, Train Loss: -0.0283, Test Loss: -0.0293\n",
      "Epoch 9/100, Train Loss: -0.0292, Test Loss: -0.0287\n",
      "Epoch 10/100, Train Loss: -0.0299, Test Loss: -0.0275\n",
      "Epoch 11/100, Train Loss: -0.0313, Test Loss: -0.0247\n",
      "Epoch 12/100, Train Loss: -0.0315, Test Loss: -0.0216\n",
      "Epoch 13/100, Train Loss: -0.0324, Test Loss: -0.0160\n",
      "Epoch 14/100, Train Loss: -0.0334, Test Loss: -0.0086\n",
      "Epoch 15/100, Train Loss: -0.0343, Test Loss: 0.0021\n",
      "Epoch 16/100, Train Loss: -0.0351, Test Loss: 0.0152\n",
      "Early stopping due to overfitting.\n",
      "------------------------------\n",
      "client: 2\n",
      "Epoch 1/100, Train Loss: 0.0157, Test Loss: -0.0122\n",
      "Epoch 2/100, Train Loss: 0.0026, Test Loss: -0.0036\n",
      "Epoch 3/100, Train Loss: -0.0245, Test Loss: -0.0545\n",
      "Epoch 4/100, Train Loss: -0.0285, Test Loss: -0.0193\n",
      "Epoch 5/100, Train Loss: -0.0252, Test Loss: -0.0362\n",
      "Epoch 6/100, Train Loss: -0.0313, Test Loss: -0.0587\n",
      "Epoch 7/100, Train Loss: -0.0331, Test Loss: -0.0530\n",
      "Epoch 8/100, Train Loss: -0.0320, Test Loss: -0.0476\n",
      "Epoch 9/100, Train Loss: -0.0325, Test Loss: -0.0554\n",
      "Epoch 10/100, Train Loss: -0.0343, Test Loss: -0.0611\n",
      "Epoch 11/100, Train Loss: -0.0350, Test Loss: -0.0600\n",
      "Epoch 12/100, Train Loss: -0.0352, Test Loss: -0.0611\n",
      "Epoch 13/100, Train Loss: -0.0363, Test Loss: -0.0647\n",
      "Epoch 14/100, Train Loss: -0.0375, Test Loss: -0.0661\n",
      "Epoch 15/100, Train Loss: -0.0381, Test Loss: -0.0673\n",
      "Epoch 16/100, Train Loss: -0.0390, Test Loss: -0.0696\n",
      "Epoch 17/100, Train Loss: -0.0400, Test Loss: -0.0708\n",
      "Epoch 18/100, Train Loss: -0.0407, Test Loss: -0.0722\n",
      "Epoch 19/100, Train Loss: -0.0415, Test Loss: -0.0742\n",
      "Epoch 20/100, Train Loss: -0.0422, Test Loss: -0.0757\n",
      "Epoch 21/100, Train Loss: -0.0428, Test Loss: -0.0767\n",
      "Epoch 22/100, Train Loss: -0.0433, Test Loss: -0.0776\n",
      "Epoch 23/100, Train Loss: -0.0439, Test Loss: -0.0789\n",
      "Epoch 24/100, Train Loss: -0.0445, Test Loss: -0.0798\n",
      "Epoch 25/100, Train Loss: -0.0450, Test Loss: -0.0803\n",
      "Epoch 26/100, Train Loss: -0.0456, Test Loss: -0.0810\n",
      "Epoch 27/100, Train Loss: -0.0462, Test Loss: -0.0816\n",
      "Epoch 28/100, Train Loss: -0.0467, Test Loss: -0.0809\n",
      "Epoch 29/100, Train Loss: -0.0438, Test Loss: -0.0765\n",
      "Epoch 30/100, Train Loss: -0.0458, Test Loss: -0.0807\n",
      "Epoch 31/100, Train Loss: -0.0473, Test Loss: -0.0826\n",
      "Epoch 32/100, Train Loss: -0.0479, Test Loss: -0.0832\n",
      "Epoch 33/100, Train Loss: -0.0481, Test Loss: -0.0826\n",
      "Epoch 34/100, Train Loss: -0.0470, Test Loss: -0.0831\n",
      "Epoch 35/100, Train Loss: -0.0467, Test Loss: -0.0793\n",
      "Epoch 36/100, Train Loss: -0.0461, Test Loss: -0.0689\n",
      "Epoch 37/100, Train Loss: -0.0442, Test Loss: -0.0604\n",
      "Epoch 38/100, Train Loss: -0.0407, Test Loss: -0.0829\n",
      "Epoch 39/100, Train Loss: -0.0453, Test Loss: -0.0682\n",
      "Epoch 40/100, Train Loss: -0.0448, Test Loss: -0.0824\n",
      "Epoch 41/100, Train Loss: -0.0471, Test Loss: -0.0789\n",
      "Epoch 42/100, Train Loss: -0.0473, Test Loss: -0.0777\n",
      "Early stopping due to overfitting.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.17e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 340       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 6         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 200       |\n",
      "|    ep_rew_mean            | -1.16e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 299       |\n",
      "|    iterations             | 2         |\n",
      "|    time_elapsed           | 13        |\n",
      "|    total_timesteps        | 4096      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.41      |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00455   |\n",
      "|    learning_rate          | 0.001     |\n",
      "|    n_updates              | 105       |\n",
      "|    policy_objective       | 0.00647   |\n",
      "|    std                    | 1.05      |\n",
      "|    value_loss             | 855       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 34>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m     Global_RL\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mmodels \u001b[38;5;241m=\u001b[39m env_models\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[43mGlobal_RL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimesteps_fc_per_round\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m#     Server.learn(timesteps_real_per_round = 10000)\u001b[39;00m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;66;03m# test performance\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     mean_reward, std_reward \u001b[38;5;241m=\u001b[39m evaluate_policy(Global_RL, real_envs[\u001b[38;5;241m1\u001b[39m], n_eval_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\sb3_contrib\\trpo\\trpo.py:412\u001b[0m, in \u001b[0;36mTRPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfTRPO,\n\u001b[0;32m    405\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    410\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    411\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfTRPO:\n\u001b[1;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 259\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:169\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 169\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\policies.py:628\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[1;34m(self, obs, deterministic)\u001b[0m\n\u001b[0;32m    626\u001b[0m distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_action_dist_from_latent(latent_pi)\n\u001b[0;32m    627\u001b[0m actions \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mget_actions(deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n\u001b[1;32m--> 628\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m actions, values, log_prob\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\distributions.py:175\u001b[0m, in \u001b[0;36mDiagGaussianDistribution.log_prob\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, actions: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;124;03m    Get the log probabilities of actions according to the distribution.\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m    Note that you must first call the ``proba_distribution()`` method.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sum_independent_dims(log_prob)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\distributions\\normal.py:86\u001b[0m, in \u001b[0;36mNormal.log_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m     81\u001b[0m var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     82\u001b[0m log_scale \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     83\u001b[0m     math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale, Real) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\u001b[38;5;241m.\u001b[39mlog()\n\u001b[0;32m     84\u001b[0m )\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;241m-\u001b[39m(\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m var)\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;241m-\u001b[39m log_scale\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;241m-\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mpi))\n\u001b[0;32m     89\u001b[0m )\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\_tensor.py:40\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize the client and server\n",
    "timesteps_real_per_round = 500\n",
    "timesteps_fc_per_round = timesteps_real_per_round * 30\n",
    "epoch_per_round = 100\n",
    "CLIENTS_NUM = 3\n",
    "rounds_num = 20\n",
    "batch_size_env_model = 128\n",
    "\n",
    "env_models = []\n",
    "MB_env = TimeLimit(MB_PendulumEnv(env_models,device), max_episode_steps = 200)\n",
    "\n",
    "# Global_RL = PPO(\"MlpPolicy\", MB_env, verbose=1)\n",
    "Global_RL = TRPO(\"MlpPolicy\", MB_env, verbose=1)\n",
    "\n",
    "env_theta = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "real_envs = []\n",
    "Clients = []\n",
    "for i in range(CLIENTS_NUM):\n",
    "    real_envs.append( TimeLimit(PendulumEnv(), max_episode_steps=200) )\n",
    "    policy_net = Global_RL\n",
    "    agent = SB3Agent(policy_net)\n",
    "    client = FRLClient(real_envs[i], agent, lr = 3e-4, hidden_size = 256, device = device)\n",
    "    Clients.append(client)\n",
    "    env_model = copy.deepcopy(client.model)\n",
    "    env_models.append(env_model)\n",
    "    \n",
    "\n",
    "Global_RL.env.models = env_models\n",
    "\n",
    "Global_RL.save(f'models/tmp')\n",
    "\n",
    "rewards_log = []\n",
    "\n",
    "env_models = []\n",
    "for round_idx in range(rounds_num):\n",
    "    print('------------------------------')\n",
    "    print(\"round: \" + str(round_idx))\n",
    "    for client_idx in range(len(Clients)):\n",
    "        print('------------------------------')\n",
    "        print(\"client: \" + str(client_idx))\n",
    "        # update policy\n",
    "        Clients[client_idx].agent.policy_net = Global_RL\n",
    "        # train prediction models\n",
    "        Clients[client_idx].learn(timesteps_real_per_round, epoch_per_round, batch_size_env_model)\n",
    "        #\n",
    "        env_model = Clients[client_idx].get_prediction_model()\n",
    "        env_models.append(env_model)\n",
    "    \n",
    "#     Server.update_env_models(env_models)\n",
    "\n",
    "    MB_env = TimeLimit(MB_PendulumEnv(env_models,device), max_episode_steps = 200)\n",
    "    \n",
    "    Global_RL = TRPO.load(\"models/tmp.zip\", env = MB_env)\n",
    "#     Global_RL.env.models = env_models\n",
    "    #\n",
    "    Global_RL.learn(total_timesteps=timesteps_fc_per_round)\n",
    "    \n",
    "    Global_RL.save(f'models/tmp')\n",
    "#     Server.learn(timesteps_real_per_round = 10000)\n",
    "    # test performance\n",
    "    mean_reward, std_reward = evaluate_policy(Global_RL, real_envs[1], n_eval_episodes=10)\n",
    "    rewards_log.append(mean_reward)\n",
    "    print(\"mean_reward in real env:\" + str(mean_reward))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bd73ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " )]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Global_RL.env.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5919ce0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " ),\n",
       " GaussianModel(\n",
       "   (fc_net): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       " )]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Global_RL.env.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9d5679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "866a79b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.25910562, -0.965849  ,  0.49215463], dtype=float32), {})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MB_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "27240e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.25910562, -0.965849  ,  0.49215463], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MB_env.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "86b6906e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mMB_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMB_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mF:\\AI\\RL\\FedMBRL\\MBEnvs\\mb_pendulum2_gaussian.py:147\u001b[0m, in \u001b[0;36mMB_PendulumEnv.step\u001b[1;34m(self, u)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, u):\n\u001b[0;32m    145\u001b[0m     last_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs  \u001b[38;5;66;03m# th := theta\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m     next_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_obs_by_model(last_obs, action\u001b[38;5;241m=\u001b[39mu)\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs \u001b[38;5;241m=\u001b[39m next_obs\n",
      "File \u001b[1;32mF:\\AI\\RL\\FedMBRL\\MBEnvs\\mb_pendulum2_gaussian.py:210\u001b[0m, in \u001b[0;36mMB_PendulumEnv._get_reward\u001b[1;34m(self, obs, action)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_reward\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs, action):\n\u001b[1;32m--> 210\u001b[0m     obs1, obs2, thdot \u001b[38;5;241m=\u001b[39m obs\n\u001b[0;32m    211\u001b[0m     th \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recover_theta(obs1, obs2)\n\u001b[0;32m    212\u001b[0m     costs \u001b[38;5;241m=\u001b[39m angle_normalize(th) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m thdot\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.001\u001b[39m \u001b[38;5;241m*\u001b[39m (action\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "MB_env.step(MB_env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b425f2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2085361, -0.7650793, -1.1017272]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MB_env.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1240dea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f51f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7e7e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f748ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs,_ = Clients[0].env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "200357a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.43307275,  0.90135896, -0.16240236], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46ae387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02079844",
   "metadata": {},
   "outputs": [],
   "source": [
    "action =  Clients[0].agent.act(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8175f7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.33290574], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86eb1c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianModel(\n",
       "  (fc_net): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (mean_logvar): Linear(in_features=256, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Clients[0].model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdc9a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
    "action_tensor = torch.tensor(action, dtype=torch.float32)\n",
    "input_d = torch.cat((obs_tensor, action_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e09bc7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4331,  0.9014, -0.1624, -0.3329])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b90abec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_d = input_d.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1abd23a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, logvar = Clients[0].model.forward(input_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b4e5999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0052, -0.0026,  0.6155], device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([-3.5161, -2.7553, -2.9714], device='cuda:0', grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, logvar[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f4a8690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1724, 0.2522, 0.2263], device='cuda:0', grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std = torch.exp(0.5 * logvar[0])\n",
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cb19391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = torch.randn_like(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7a04f963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7962, 0.2781, 1.4486], device='cuda:0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "87978845",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_state_change = mean + eps * std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "70dd3224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1320, 0.0675, 0.9434], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_state_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "45e782cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_np = pred_state_change.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ef34a4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13200563, 0.06751756, 0.94340324], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "66ce127b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.43307275,  0.90135896, -0.16240236], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a87d7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_new = obs + output_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c3e2b7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5650784 , 0.96887654, 0.78100085], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdfb5ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aeddeb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_clipped = np.clip(obs, Clients[0].env.observation_space.low, Clients[0].env.observation_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da00b3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.43307275,  0.90135896, -0.16240236], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9b01f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aef4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6cfccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22093d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.30735433], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Clients[0].agent.policy_net.predict(obs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4bbe53a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1699efd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: -93.15121025741101 +/- 109.2765889131137\n"
     ]
    }
   ],
   "source": [
    "# mean_reward, std_reward = evaluate_policy(Global_RL, real_envs[1], n_eval_episodes=10)\n",
    "mean_reward, std_reward = evaluate_policy(Global_RL, MB_env, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9285a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import TRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a1ac5563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "TRPO_model = TRPO(\"MlpPolicy\", env=real_envs[0], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "23e35f65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -829     |\n",
      "| time/              |          |\n",
      "|    fps             | 520      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -847     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 478      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 8        |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.715    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00696  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 50       |\n",
      "|    policy_objective       | 0.0147   |\n",
      "|    std                    | 0.809    |\n",
      "|    value_loss             | 433      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -893     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 481      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 12       |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.749    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00777  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 51       |\n",
      "|    policy_objective       | 0.0188   |\n",
      "|    std                    | 0.789    |\n",
      "|    value_loss             | 629      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -890     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 470      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 17       |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.759    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00865  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 52       |\n",
      "|    policy_objective       | 0.0126   |\n",
      "|    std                    | 0.762    |\n",
      "|    value_loss             | 857      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -875     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 470      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 21       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.8      |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00702  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 53       |\n",
      "|    policy_objective       | 0.0123   |\n",
      "|    std                    | 0.769    |\n",
      "|    value_loss             | 679      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -860     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 465      |\n",
      "|    iterations             | 6        |\n",
      "|    time_elapsed           | 26       |\n",
      "|    total_timesteps        | 12288    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.825    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00762  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 54       |\n",
      "|    policy_objective       | 0.0143   |\n",
      "|    std                    | 0.757    |\n",
      "|    value_loss             | 660      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -832     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 467      |\n",
      "|    iterations             | 7        |\n",
      "|    time_elapsed           | 30       |\n",
      "|    total_timesteps        | 14336    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.851    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00773  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 55       |\n",
      "|    policy_objective       | 0.0167   |\n",
      "|    std                    | 0.743    |\n",
      "|    value_loss             | 428      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -808     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 470      |\n",
      "|    iterations             | 8        |\n",
      "|    time_elapsed           | 34       |\n",
      "|    total_timesteps        | 16384    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.881    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00726  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 56       |\n",
      "|    policy_objective       | 0.0213   |\n",
      "|    std                    | 0.72     |\n",
      "|    value_loss             | 291      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -794     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 472      |\n",
      "|    iterations             | 9        |\n",
      "|    time_elapsed           | 39       |\n",
      "|    total_timesteps        | 18432    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.876    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00698  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 57       |\n",
      "|    policy_objective       | 0.0147   |\n",
      "|    std                    | 0.728    |\n",
      "|    value_loss             | 506      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -758     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 471      |\n",
      "|    iterations             | 10       |\n",
      "|    time_elapsed           | 43       |\n",
      "|    total_timesteps        | 20480    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.914    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00761  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 58       |\n",
      "|    policy_objective       | 0.0205   |\n",
      "|    std                    | 0.72     |\n",
      "|    value_loss             | 376      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -734     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 472      |\n",
      "|    iterations             | 11       |\n",
      "|    time_elapsed           | 47       |\n",
      "|    total_timesteps        | 22528    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.883    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00787  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 59       |\n",
      "|    policy_objective       | 0.0231   |\n",
      "|    std                    | 0.698    |\n",
      "|    value_loss             | 250      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -690     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 473      |\n",
      "|    iterations             | 12       |\n",
      "|    time_elapsed           | 51       |\n",
      "|    total_timesteps        | 24576    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.927    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00787  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 60       |\n",
      "|    policy_objective       | 0.0255   |\n",
      "|    std                    | 0.692    |\n",
      "|    value_loss             | 283      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -644     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 473      |\n",
      "|    iterations             | 13       |\n",
      "|    time_elapsed           | 56       |\n",
      "|    total_timesteps        | 26624    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.928    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00833  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 61       |\n",
      "|    policy_objective       | 0.0195   |\n",
      "|    std                    | 0.681    |\n",
      "|    value_loss             | 334      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -604     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 473      |\n",
      "|    iterations             | 14       |\n",
      "|    time_elapsed           | 60       |\n",
      "|    total_timesteps        | 28672    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.884    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00801  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 62       |\n",
      "|    policy_objective       | 0.0296   |\n",
      "|    std                    | 0.677    |\n",
      "|    value_loss             | 282      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -550     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 474      |\n",
      "|    iterations             | 15       |\n",
      "|    time_elapsed           | 64       |\n",
      "|    total_timesteps        | 30720    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.927    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00836  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 63       |\n",
      "|    policy_objective       | 0.0263   |\n",
      "|    std                    | 0.654    |\n",
      "|    value_loss             | 260      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -499     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 474      |\n",
      "|    iterations             | 16       |\n",
      "|    time_elapsed           | 69       |\n",
      "|    total_timesteps        | 32768    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.926    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00848  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 64       |\n",
      "|    policy_objective       | 0.0303   |\n",
      "|    std                    | 0.638    |\n",
      "|    value_loss             | 274      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 200      |\n",
      "|    ep_rew_mean            | -445     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 473      |\n",
      "|    iterations             | 17       |\n",
      "|    time_elapsed           | 73       |\n",
      "|    total_timesteps        | 34816    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.954    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00868  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 65       |\n",
      "|    policy_objective       | 0.0229   |\n",
      "|    std                    | 0.639    |\n",
      "|    value_loss             | 202      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [61]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mTRPO_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\sb3_contrib\\trpo\\trpo.py:412\u001b[0m, in \u001b[0;36mTRPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfTRPO,\n\u001b[0;32m    405\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    410\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    411\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfTRPO:\n\u001b[1;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 259\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:169\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 169\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\policies.py:619\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[1;34m(self, obs, deterministic)\u001b[0m\n\u001b[0;32m    617\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_features(obs)\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[1;32m--> 619\u001b[0m     latent_pi, latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m     pi_features, vf_features \u001b[38;5;241m=\u001b[39m features\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\torch_layers.py:222\u001b[0m, in \u001b[0;36mMlpExtractor.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[th\u001b[38;5;241m.\u001b[39mTensor, th\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03m    :return: latent_policy, latent_value of the specified network.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m        If all layers are shared, then ``latent_policy == latent_value``\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_critic(features)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\stable_baselines3\\common\\torch_layers.py:225\u001b[0m, in \u001b[0;36mMlpExtractor.forward_actor\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_actor\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\activation.py:356\u001b[0m, in \u001b[0;36mTanh.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TRPO_model.learn(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ce23ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5973d385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a388d5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236c5fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e6883d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Input Mean: tensor([ 0.3523, -0.0157, -0.2429,  0.1068], grad_fn=<SqueezeBackward1>)\n",
      "Single Input Logvar: tensor([-0.5165, -0.4999, -0.4875, -0.4478], grad_fn=<SqueezeBackward1>)\n",
      "Batch Input Mean: tensor([[ 0.1343,  0.0868, -0.1983,  0.0582],\n",
      "        [ 0.1066, -0.1898, -0.6315,  0.0736],\n",
      "        [ 0.3875,  0.1861, -0.3479,  0.0122],\n",
      "        [ 0.1715,  0.0231, -0.1943,  0.1135],\n",
      "        [ 0.2438, -0.0612, -0.4051,  0.0425]], grad_fn=<SliceBackward0>)\n",
      "Batch Input Logvar: tensor([[-0.4949, -0.4842, -0.5894, -0.4223],\n",
      "        [-0.4841, -0.3873, -0.6761, -0.3461],\n",
      "        [-0.5958, -0.4917, -0.8158, -0.3497],\n",
      "        [-0.4940, -0.4847, -0.5807, -0.4696],\n",
      "        [-0.4818, -0.4734, -0.7298, -0.3792]], grad_fn=<AddBackward0>)\n",
      "Single Input Loss: 3.6077818870544434\n",
      "Batch Input Loss: 3.898193597793579\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# GaussianModel\n",
    "class GaussianModel(nn.Module):\n",
    "    def __init__(self, obs_size, action_size, hidden_size=256, learn_logvar_bounds=False):\n",
    "        super(GaussianModel, self).__init__()\n",
    "        self.out_size = obs_size\n",
    "        \n",
    "        self.fc_net = nn.Sequential(\n",
    "            nn.Linear(obs_size + action_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.mean_logvar = nn.Linear(hidden_size, obs_size * 2)\n",
    "        \n",
    "        self.min_logvar = nn.Parameter(\n",
    "            -10 * torch.ones(1, obs_size), requires_grad=learn_logvar_bounds\n",
    "        )\n",
    "        self.max_logvar = nn.Parameter(\n",
    "            0.5 * torch.ones(1, obs_size), requires_grad=learn_logvar_bounds\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc_net(x)\n",
    "        mean_and_logvar = self.mean_logvar(x)\n",
    "        mean = mean_and_logvar[..., :self.out_size]\n",
    "        logvar = mean_and_logvar[..., self.out_size:]\n",
    "        \n",
    "        logvar = self.max_logvar - F.softplus(self.max_logvar - logvar)\n",
    "        logvar = self.min_logvar + F.softplus(logvar - self.min_logvar)\n",
    "        \n",
    "        # If the original input was one-dimensional, squeeze the output\n",
    "        if logvar.shape[0] == 1:\n",
    "            mean = mean.squeeze(0)\n",
    "            logvar = logvar.squeeze(0)\n",
    "        \n",
    "        return mean, logvar\n",
    "\n",
    "# \n",
    "def test_gaussian_model():\n",
    "    obs_size = 4\n",
    "    action_size = 2\n",
    "    hidden_size = 256\n",
    "\n",
    "    model = GaussianModel(obs_size, action_size, hidden_size)\n",
    "\n",
    "    # \n",
    "    single_input = torch.randn(obs_size + action_size)\n",
    "    \n",
    "    # \n",
    "    batch_input = torch.randn(5, obs_size + action_size)\n",
    "    \n",
    "    # \n",
    "    single_mean, single_logvar = model(single_input)\n",
    "    batch_mean, batch_logvar = model(batch_input)\n",
    "    \n",
    "    print(\"Single Input Mean:\", single_mean)\n",
    "    print(\"Single Input Logvar:\", single_logvar)\n",
    "    print(\"Batch Input Mean:\", batch_mean)\n",
    "    print(\"Batch Input Logvar:\", batch_logvar)\n",
    "    \n",
    "    # \n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # \n",
    "    target_mean = torch.randn(obs_size)\n",
    "    target_logvar = torch.randn(obs_size)\n",
    "    \n",
    "    # \n",
    "    single_loss = loss_fn(single_mean, target_mean) + loss_fn(single_logvar, target_logvar)\n",
    "    batch_loss = loss_fn(batch_mean, target_mean.unsqueeze(0).expand_as(batch_mean)) + \\\n",
    "                 loss_fn(batch_logvar, target_logvar.unsqueeze(0).expand_as(batch_logvar))\n",
    "    \n",
    "    print(\"Single Input Loss:\", single_loss.item())\n",
    "    print(\"Batch Input Loss:\", batch_loss.item())\n",
    "\n",
    "# \n",
    "test_gaussian_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a08c280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1db1039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
