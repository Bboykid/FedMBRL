# from Client import *
# from Server import PPO_Server
# from MBEnvs.pendulum import MB_PendulumEnv


# CLIENTS_NUM = 5


# def FedMBRL_train(rounds_num = 50):
#     pass
#     # initialize the client and server
#     env_models = []
# 	MB_env = MB_PendulumEnv(env_models)	
 
# 	Global_RL = PPO("MlpPolicy", MB_env)
    
#     env_theta = [0.1, 0.3, 0.5, 0.7, 0.9]
#     real_envs = []
#     Clients = []
#     for i in range(CLIENTS_NUM):
# 		real_envs.append( PENV(env_theta[i]))
# 		policy_net = Global_RL
# 		agent = BaseAgent(policy_net)
# 		client = FRLClient(real_envs[i], agent, lr = 3e-4, hidden_size = 256, device = "auto")
# 		env_model = copy.deepcopy(client.model)
#   		Clients.append(client)
# 		env_models.append(env_model)
	

# 	MB_env = MB_PendulumEnv(env_models)
	
# 	Server = PPO_Server(Global_RL, MB_env)
		
		
    
#     # each round
#     for round_idx in range(rounds_num):
#     	# server send policy params to each selected client
#     	# clients update the policy
#      	for client_idx in range(len(Clients)):
# 			Clients[client_idx].update_policy(Server.get_policy_params())
    
#     	# clients use the gloabl policy to train local environemnt models
# 		for client_idx in range(len(Clients)):
# 			Clients[client_idx].learn(timesteps_real_per_round, epoch_per_round, batch_size_env_model)
#     	# clients send the environment models back to the server
# 		env_models = []
# 		for client_idx in range(len(Clients)):
# 			env_model = Clients[client_idx].get_prediction_model()
# 			env_models.append(env_model)
    	
#      	# server update the environment models
# 		Server.update_env_models(env_models)
#     	# server train the RL Policy by the fictitious data generated by the environment models
# 		Server.learn(timesteps_fictitious_per_round)
# 		# test the performance and log the avg reward
